{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Instructor","text":"<p>Structured outputs powered by llms. Designed for simplicity, transparency, and control.</p> <p> </p> <p>Dive into the world of Python-based structured extraction, by OpenAI's function calling API and Pydantic, the most widely used data validation library for Python. Instructor stands out for its simplicity, transparency, and user-centric design. Whether you're a seasoned developer or just starting out, you'll find Instructor's approach intuitive and steerable.</p> <p>Support in other languages</p> <p>Check out ports to other languages below:</p> <ul> <li>Typescript / Javascript</li> <li>Elixir</li> </ul> <p>If you want to port Instructor to another language, please reach out to us on Twitter we'd love to help you get started!</p>"},{"location":"#usage","title":"Usage","text":"<pre><code>import instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\n# This enables response_model keyword\n# from client.chat.completions.create\nclient = instructor.patch(OpenAI())\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\nuser = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserDetail,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"},\n    ],\n)\n\nassert isinstance(user, UserDetail)\nassert user.name == \"Jason\"\nassert user.age == 25\nprint(user.model_dump_json(indent=2))\n\"\"\"\n{\n  \"name\": \"Jason\",\n  \"age\": 25\n}\n\"\"\"\n</code></pre> <p>Using async clients</p> <p>For async clients you must use <code>apatch</code> vs <code>patch</code> like so:</p> <pre><code>import asyncio\nimport instructor\nfrom openai import AsyncOpenAI\nfrom pydantic import BaseModel\n\naclient = instructor.apatch(AsyncOpenAI())\n\n\nclass UserExtract(BaseModel):\n    name: str\n    age: int\n\n\ntask = aclient.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserExtract,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"},\n    ],\n)\n\nresponse = asyncio.run(task)\nprint(response.model_dump_json(indent=2))\n\"\"\"\n{\n  \"name\": \"Jason\",\n  \"age\": 25\n}\n\"\"\"\n</code></pre> <p>Accessing the original response and usage tokens</p> <p>If you want to access anything like usage or other metadata, the original response is available on the <code>Model._raw_response</code> attribute.</p> <pre><code>import openai\nimport instructor\nfrom pydantic import BaseModel\n\nclient = instructor.patch(openai.OpenAI())\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\nuser = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserDetail,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"},\n    ],\n)\n\nprint(user._raw_response.model_dump_json(indent=2))\n\"\"\"\n{\n  \"id\": \"chatcmpl-8pOAKwq8OXZVvOCMw4dv713oKplLF\",\n  \"choices\": [\n    {\n      \"finish_reason\": \"stop\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"message\": {\n        \"content\": null,\n        \"role\": \"assistant\",\n        \"function_call\": {\n          \"arguments\": \"{\\n  \\\"name\\\": \\\"Jason\\\",\\n  \\\"age\\\": 25\\n}\",\n          \"name\": \"UserDetail\"\n        },\n        \"tool_calls\": null\n      }\n    }\n  ],\n  \"created\": 1707258312,\n  \"model\": \"gpt-3.5-turbo-0613\",\n  \"object\": \"chat.completion\",\n  \"system_fingerprint\": null,\n  \"usage\": {\n    \"completion_tokens\": 16,\n    \"prompt_tokens\": 72,\n    \"total_tokens\": 88\n  }\n}\n\"\"\"\n</code></pre>"},{"location":"#why-use-instructor","title":"Why use Instructor?","text":"<p>The question of using Instructor is fundamentally a question of why to use Pydantic.</p> <ol> <li> <p>Powered by type hints \u2014 Instructor is powered by Pydantic, which is powered by type hints. Schema validation, prompting is controlled by type annotations; less to learn, less code to write, and integrates with your IDE.</p> </li> <li> <p>Powered by OpenAI \u2014 Instructor is powered by OpenAI's function calling API. This means you can use the same API for both prompting and extraction.</p> </li> <li> <p>Customizable \u2014 Pydantic is highly customizable. You can define your own validators, custom error messages, and more.</p> </li> <li> <p>Ecosystem Pydantic is the most widely used data validation library for Python. It's used by FastAPI, Typer, and many other popular libraries.</p> </li> <li> <p>Battle Tested \u2014 Pydantic is downloaded over 100M times per month, and supported by a large community of contributors.</p> </li> <li> <p>Easy Integration with CLI - We offer a variety of CLI tools like <code>instructor jobs</code>, <code>instructor files</code> and <code>instructor usage</code> to track your OpenAI usage, fine-tuning jobs and more, just check out our CLI Documentation to find out more.</p> </li> </ol>"},{"location":"#more-examples","title":"More Examples","text":"<p>If you'd like to see more check out our cookbook.</p> <p>Installing Instructor is a breeze. Just run <code>pip install instructor</code>.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>If you want to help out, checkout some of the issues marked as <code>good-first-issue</code> or <code>help-wanted</code>. Found here. They could be anything from code improvements, a guest blog post, or a new cook book.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the terms of the MIT License.</p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#instructor.patch.apatch","title":"<code>apatch(client, mode=Mode.FUNCTIONS)</code>","text":"<p>No longer necessary, use <code>patch</code> instead.</p> <p>Patch the <code>client.chat.completions.create</code> method</p> <p>Enables the following features:</p> <ul> <li><code>response_model</code> parameter to parse the response from OpenAI's API</li> <li><code>max_retries</code> parameter to retry the function if the response is not valid</li> <li><code>validation_context</code> parameter to validate the response using the pydantic model</li> <li><code>strict</code> parameter to use strict json parsing</li> </ul> Source code in <code>instructor/patch.py</code> <pre><code>def apatch(client: AsyncOpenAI, mode: Mode = Mode.FUNCTIONS):\n    \"\"\"\n    No longer necessary, use `patch` instead.\n\n    Patch the `client.chat.completions.create` method\n\n    Enables the following features:\n\n    - `response_model` parameter to parse the response from OpenAI's API\n    - `max_retries` parameter to retry the function if the response is not valid\n    - `validation_context` parameter to validate the response using the pydantic model\n    - `strict` parameter to use strict json parsing\n    \"\"\"\n    import warnings\n\n    warnings.warn(\n        \"apatch is deprecated, use patch instead\", DeprecationWarning, stacklevel=2\n    )\n    return patch(client, mode=mode)\n</code></pre>"},{"location":"api/#instructor.patch.dump_message","title":"<code>dump_message(message)</code>","text":"<p>Dumps a message to a dict, to be returned to the OpenAI API. Workaround for an issue with the OpenAI API, where the <code>tool_calls</code> field isn't allowed to be present in requests if it isn't used.</p> Source code in <code>instructor/patch.py</code> <pre><code>def dump_message(message: ChatCompletionMessage) -&gt; ChatCompletionMessageParam:\n    \"\"\"Dumps a message to a dict, to be returned to the OpenAI API.\n    Workaround for an issue with the OpenAI API, where the `tool_calls` field isn't allowed to be present in requests\n    if it isn't used.\n    \"\"\"\n    ret: ChatCompletionMessageParam = {\n        \"role\": message.role,\n        \"content\": message.content or \"\",\n    }\n    if hasattr(message, \"tool_calls\") and message.tool_calls is not None:\n        ret[\"tool_calls\"] = message.model_dump()[\"tool_calls\"]\n    if hasattr(message, \"function_call\") and message.function_call is not None:\n        ret[\"content\"] += json.dumps(message.model_dump()[\"function_call\"])\n    return ret\n</code></pre>"},{"location":"api/#instructor.patch.handle_response_model","title":"<code>handle_response_model(response_model, mode=Mode.TOOLS, **kwargs)</code>","text":"<p>Prepare the response model type hint, and returns the response_model along with the new modified kwargs needed to be able to use the response_model parameter with the patch function.</p> <p>Parameters:</p> Name Type Description Default <code>response_model</code> <code>T</code> <p>The response model to use for parsing the response</p> required <code>mode</code> <code>Mode</code> <p>The openai completion mode. Defaults to Mode.TOOLS.</p> <code>TOOLS</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>When using stream=True with a non-iterable response_model</p> <code>ValueError</code> <p>When using an invalid patch mode</p> <p>Returns:</p> Type Description <code>Union[Type[OpenAISchema], dict]</code> <p>Union[Type[OpenAISchema], dict]: The response model to use for parsing the response</p> Source code in <code>instructor/patch.py</code> <pre><code>def handle_response_model(\n    response_model: T, mode: Mode = Mode.TOOLS, **kwargs\n) -&gt; Union[Type[OpenAISchema], dict]:\n    \"\"\"Prepare the response model type hint, and returns the response_model\n    along with the new modified kwargs needed to be able to use the response_model\n    parameter with the patch function.\n\n\n    Args:\n        response_model (T): The response model to use for parsing the response\n        mode (Mode, optional): The openai completion mode. Defaults to Mode.TOOLS.\n\n    Raises:\n        NotImplementedError: When using stream=True with a non-iterable response_model\n        ValueError: When using an invalid patch mode\n\n    Returns:\n        Union[Type[OpenAISchema], dict]: The response model to use for parsing the response\n    \"\"\"\n    new_kwargs = kwargs.copy()\n    if response_model is not None:\n        # This a special case for parallel tools\n        if mode == Mode.PARALLEL_TOOLS:\n            assert (\n                new_kwargs.get(\"stream\", False) is False\n            ), \"stream=True is not supported when using PARALLEL_TOOLS mode\"\n            new_kwargs[\"tools\"] = handle_parallel_model(response_model)\n            new_kwargs[\"tool_choice\"] = \"auto\"\n\n            # This is a special case for parallel models\n            response_model = ParallelModel(typehint=response_model)\n            return response_model, new_kwargs\n\n        # This is for all other single model cases\n        if get_origin(response_model) is Iterable:\n            iterable_element_class = get_args(response_model)[0]\n            response_model = IterableModel(iterable_element_class)\n        if not issubclass(response_model, OpenAISchema):\n            response_model = openai_schema(response_model)  # type: ignore\n\n        if new_kwargs.get(\"stream\", False) and not issubclass(\n            response_model, (IterableBase, PartialBase)\n        ):\n            raise NotImplementedError(\n                \"stream=True is not supported when using response_model parameter for non-iterables\"\n            )\n\n        if mode == Mode.FUNCTIONS:\n            new_kwargs[\"functions\"] = [response_model.openai_schema]  # type: ignore\n            new_kwargs[\"function_call\"] = {\"name\": response_model.openai_schema[\"name\"]}  # type: ignore\n        elif mode == Mode.TOOLS:\n            new_kwargs[\"tools\"] = [\n                {\n                    \"type\": \"function\",\n                    \"function\": response_model.openai_schema,\n                }\n            ]\n            new_kwargs[\"tool_choice\"] = {\n                \"type\": \"function\",\n                \"function\": {\"name\": response_model.openai_schema[\"name\"]},\n            }\n        elif mode in {Mode.JSON, Mode.MD_JSON, Mode.JSON_SCHEMA}:\n            # If its a JSON Mode we need to massage the prompt a bit\n            # in order to get the response we want in a json format\n            message = f\"\"\"\n                As a genius expert, your task is to understand the content and provide\n                the parsed objects in json that match the following json_schema:\\n\n                {response_model.model_json_schema()['properties']}\n                \"\"\"\n            # Check for nested models\n            if \"$defs\" in response_model.model_json_schema():\n                message += f\"\\nHere are some more definitions to adhere too:\\n{response_model.model_json_schema()['$defs']}\"\n\n            if mode == Mode.JSON:\n                new_kwargs[\"response_format\"] = {\"type\": \"json_object\"}\n\n            elif mode == Mode.JSON_SCHEMA:\n                new_kwargs[\"response_format\"] = {\n                    \"type\": \"json_object\",\n                    \"schema\": response_model.model_json_schema(),\n                }\n\n            elif mode == Mode.MD_JSON:\n                new_kwargs[\"messages\"].append(\n                    {\n                        \"role\": \"assistant\",\n                        \"content\": \"Here is the perfectly correctly formatted JSON\\n```json\",\n                    },\n                )\n                new_kwargs[\"stop\"] = \"```\"\n            # check that the first message is a system message\n            # if it is not, add a system message to the beginning\n            if new_kwargs[\"messages\"][0][\"role\"] != \"system\":\n                new_kwargs[\"messages\"].insert(\n                    0,\n                    {\n                        \"role\": \"system\",\n                        \"content\": message,\n                    },\n                )\n\n            # if the first message is a system append the schema to the end\n            if new_kwargs[\"messages\"][0][\"role\"] == \"system\":\n                new_kwargs[\"messages\"][0][\"content\"] += f\"\\n\\n{message}\"\n        else:\n            raise ValueError(f\"Invalid patch mode: {mode}\")\n    return response_model, new_kwargs\n</code></pre>"},{"location":"api/#instructor.patch.is_async","title":"<code>is_async(func)</code>","text":"<p>Returns true if the callable is async, accounting for wrapped callables</p> Source code in <code>instructor/patch.py</code> <pre><code>def is_async(func: Callable) -&gt; bool:\n    \"\"\"Returns true if the callable is async, accounting for wrapped callables\"\"\"\n    return inspect.iscoroutinefunction(func) or (\n        hasattr(func, \"__wrapped__\") and inspect.iscoroutinefunction(func.__wrapped__)\n    )\n</code></pre>"},{"location":"api/#instructor.patch.patch","title":"<code>patch(client=None, create=None, mode=Mode.FUNCTIONS)</code>","text":"<p>Patch the <code>client.chat.completions.create</code> method</p> <p>Enables the following features:</p> <ul> <li><code>response_model</code> parameter to parse the response from OpenAI's API</li> <li><code>max_retries</code> parameter to retry the function if the response is not valid</li> <li><code>validation_context</code> parameter to validate the response using the pydantic model</li> <li><code>strict</code> parameter to use strict json parsing</li> </ul> Source code in <code>instructor/patch.py</code> <pre><code>def patch(\n    client: Union[OpenAI, AsyncOpenAI] = None,\n    create: Callable[T_ParamSpec, T_Retval] = None,\n    mode: Mode = Mode.FUNCTIONS,\n) -&gt; Union[OpenAI, AsyncOpenAI]:\n    \"\"\"\n    Patch the `client.chat.completions.create` method\n\n    Enables the following features:\n\n    - `response_model` parameter to parse the response from OpenAI's API\n    - `max_retries` parameter to retry the function if the response is not valid\n    - `validation_context` parameter to validate the response using the pydantic model\n    - `strict` parameter to use strict json parsing\n    \"\"\"\n\n    logger.debug(f\"Patching `client.chat.completions.create` with {mode=}\")\n\n    if create is not None:\n        func = create\n    elif client is not None:\n        func = client.chat.completions.create\n    else:\n        raise ValueError(\"Either client or create must be provided\")\n\n    func_is_async = is_async(func)\n\n    @wraps(func)\n    async def new_create_async(\n        response_model: Type[T_Model] = None,\n        validation_context: dict = None,\n        max_retries: int = 1,\n        *args: T_ParamSpec.args,\n        **kwargs: T_ParamSpec.kwargs,\n    ) -&gt; T_Model:\n        response_model, new_kwargs = handle_response_model(\n            response_model=response_model, mode=mode, **kwargs\n        )\n        response = await retry_async(\n            func=func,\n            response_model=response_model,\n            validation_context=validation_context,\n            max_retries=max_retries,\n            args=args,\n            kwargs=new_kwargs,\n            mode=mode,\n        )  # type: ignore\n        return response\n\n    @wraps(func)\n    def new_create_sync(\n        response_model: Type[T_Model] = None,\n        validation_context: dict = None,\n        max_retries: int = 1,\n        *args: T_ParamSpec.args,\n        **kwargs: T_ParamSpec.kwargs,\n    ) -&gt; T_Model:\n        response_model, new_kwargs = handle_response_model(\n            response_model=response_model, mode=mode, **kwargs\n        )\n        response = retry_sync(\n            func=func,\n            response_model=response_model,\n            validation_context=validation_context,\n            max_retries=max_retries,\n            args=args,\n            kwargs=new_kwargs,\n            mode=mode,\n        )\n        return response\n\n    new_create = new_create_async if func_is_async else new_create_sync\n    new_create.__doc__ = OVERRIDE_DOCS\n\n    if client is not None:\n        client.chat.completions.create = new_create\n        return client\n    else:\n        return new_create\n</code></pre>"},{"location":"api/#instructor.patch.process_response","title":"<code>process_response(response, *, response_model, stream, validation_context=None, strict=None, mode=Mode.FUNCTIONS)</code>","text":"<p>Processes a OpenAI response with the response model, if available.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>T</code> <p>The response from OpenAI's API</p> required <code>response_model</code> <code>Type[T_Model]</code> <p>The response model to use for parsing the response</p> required <code>stream</code> <code>bool</code> <p>Whether the response is a stream</p> required <code>validation_context</code> <code>dict</code> <p>The validation context to use for validating the response. Defaults to None.</p> <code>None</code> <code>strict</code> <code>_type_</code> <p>Whether to use strict json parsing. Defaults to None.</p> <code>None</code> <code>mode</code> <code>Mode</code> <p>The openai completion mode. Defaults to Mode.FUNCTIONS.</p> <code>FUNCTIONS</code> <p>Returns:</p> Type Description <code>Union[T_Model, T]</code> <p>Union[T_Model, T]: The parsed response, if a response model is available, otherwise the response as is from the SDK</p> Source code in <code>instructor/patch.py</code> <pre><code>def process_response(\n    response: T,\n    *,\n    response_model: Type[T_Model],\n    stream: bool,\n    validation_context: dict = None,\n    strict=None,\n    mode: Mode = Mode.FUNCTIONS,\n) -&gt; Union[T_Model, T]:\n    \"\"\"Processes a OpenAI response with the response model, if available.\n\n    Args:\n        response (T): The response from OpenAI's API\n        response_model (Type[T_Model]): The response model to use for parsing the response\n        stream (bool): Whether the response is a stream\n        validation_context (dict, optional): The validation context to use for validating the response. Defaults to None.\n        strict (_type_, optional): Whether to use strict json parsing. Defaults to None.\n        mode (Mode, optional): The openai completion mode. Defaults to Mode.FUNCTIONS.\n\n    Returns:\n        Union[T_Model, T]: The parsed response, if a response model is available, otherwise the response as is from the SDK\n    \"\"\"\n    if response_model is None:\n        return response\n\n    if (\n        inspect.isclass(response_model)\n        and issubclass(response_model, (IterableBase, PartialBase))\n        and stream\n    ):\n        model = response_model.from_streaming_response(\n            response,\n            mode=mode,\n        )\n        return model\n\n    model = response_model.from_response(\n        response,\n        validation_context=validation_context,\n        strict=strict,\n        mode=mode,\n    )\n\n    # ? This really hints at the fact that we need a better way of\n    # ? attaching usage data and the raw response to the model we return.\n    if isinstance(model, IterableBase):\n        return [task for task in model.tasks]\n\n    if isinstance(response_model, ParallelBase):\n        return model\n\n    model._raw_response = response\n    return model\n</code></pre>"},{"location":"api/#instructor.patch.process_response_async","title":"<code>process_response_async(response, *, response_model, stream=False, validation_context=None, strict=None, mode=Mode.FUNCTIONS)</code>  <code>async</code>","text":"<p>Processes a OpenAI response with the response model, if available. It can use <code>validation_context</code> and <code>strict</code> to validate the response via the pydantic model</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>ChatCompletion</code> <p>The response from OpenAI's API</p> required <code>response_model</code> <code>BaseModel</code> <p>The response model to use for parsing the response</p> required <code>stream</code> <code>bool</code> <p>Whether the response is a stream</p> <code>False</code> <code>validation_context</code> <code>dict</code> <p>The validation context to use for validating the response. Defaults to None.</p> <code>None</code> <code>strict</code> <code>bool</code> <p>Whether to use strict json parsing. Defaults to None.</p> <code>None</code> Source code in <code>instructor/patch.py</code> <pre><code>async def process_response_async(\n    response: ChatCompletion,\n    *,\n    response_model: Type[T_Model],\n    stream: bool = False,\n    validation_context: dict = None,\n    strict: Optional[bool] = None,\n    mode: Mode = Mode.FUNCTIONS,\n) -&gt; T:\n    \"\"\"Processes a OpenAI response with the response model, if available.\n    It can use `validation_context` and `strict` to validate the response\n    via the pydantic model\n\n    Args:\n        response (ChatCompletion): The response from OpenAI's API\n        response_model (BaseModel): The response model to use for parsing the response\n        stream (bool): Whether the response is a stream\n        validation_context (dict, optional): The validation context to use for validating the response. Defaults to None.\n        strict (bool, optional): Whether to use strict json parsing. Defaults to None.\n    \"\"\"\n    if response_model is None:\n        return response\n\n    if (\n        inspect.isclass(response_model)\n        and issubclass(response_model, (IterableBase, PartialBase))\n        and stream\n    ):\n        model = await response_model.from_streaming_response_async(\n            response,\n            mode=mode,\n        )\n        return model\n\n    model = response_model.from_response(\n        response,\n        validation_context=validation_context,\n        strict=strict,\n        mode=mode,\n    )\n\n    # ? This really hints at the fact that we need a better way of\n    # ? attaching usage data and the raw response to the model we return.\n    if isinstance(model, IterableBase):\n        #! If the response model is a multitask, return the tasks\n        return [task for task in model.tasks]\n\n    if isinstance(response_model, ParallelBase):\n        return model\n\n    model._raw_response = response\n    return model\n</code></pre>"},{"location":"api/#instructor.dsl.validators.Validator","title":"<code>Validator</code>","text":"<p>             Bases: <code>OpenAISchema</code></p> <p>Validate if an attribute is correct and if not, return a new value with an error message</p> Source code in <code>instructor/dsl/validators.py</code> <pre><code>class Validator(OpenAISchema):\n    \"\"\"\n    Validate if an attribute is correct and if not,\n    return a new value with an error message\n    \"\"\"\n\n    is_valid: bool = Field(\n        default=True,\n        description=\"Whether the attribute is valid based on the requirements\",\n    )\n    reason: Optional[str] = Field(\n        default=None,\n        description=\"The error message if the attribute is not valid, otherwise None\",\n    )\n    fixed_value: Optional[str] = Field(\n        default=None,\n        description=\"If the attribute is not valid, suggest a new value for the attribute\",\n    )\n</code></pre>"},{"location":"api/#instructor.dsl.validators.llm_validator","title":"<code>llm_validator(statement, allow_override=False, model='gpt-3.5-turbo', temperature=0, openai_client=None)</code>","text":"<p>Create a validator that uses the LLM to validate an attribute</p>"},{"location":"api/#instructor.dsl.validators.llm_validator--usage","title":"Usage","text":"<pre><code>from instructor import llm_validator\nfrom pydantic import BaseModel, Field, field_validator\n\nclass User(BaseModel):\n    name: str = Annotated[str, llm_validator(\"The name must be a full name all lowercase\")\n    age: int = Field(description=\"The age of the person\")\n\ntry:\n    user = User(name=\"Jason Liu\", age=20)\nexcept ValidationError as e:\n    print(e)\n</code></pre> <pre><code>1 validation error for User\nname\n  The name is valid but not all lowercase (type=value_error.llm_validator)\n</code></pre> <p>Note that there, the error message is written by the LLM, and the error type is <code>value_error.llm_validator</code>.</p> <p>Parameters:</p> Name Type Description Default <code>statement</code> <code>str</code> <p>The statement to validate</p> required <code>model</code> <code>str</code> <p>The LLM to use for validation (default: \"gpt-3.5-turbo-0613\")</p> <code>'gpt-3.5-turbo'</code> <code>temperature</code> <code>float</code> <p>The temperature to use for the LLM (default: 0)</p> <code>0</code> <code>openai_client</code> <code>OpenAI</code> <p>The OpenAI client to use (default: None)</p> <code>None</code> Source code in <code>instructor/dsl/validators.py</code> <pre><code>def llm_validator(\n    statement: str,\n    allow_override: bool = False,\n    model: str = \"gpt-3.5-turbo\",\n    temperature: float = 0,\n    openai_client: OpenAI = None,\n):\n    \"\"\"\n    Create a validator that uses the LLM to validate an attribute\n\n    ## Usage\n\n    ```python\n    from instructor import llm_validator\n    from pydantic import BaseModel, Field, field_validator\n\n    class User(BaseModel):\n        name: str = Annotated[str, llm_validator(\"The name must be a full name all lowercase\")\n        age: int = Field(description=\"The age of the person\")\n\n    try:\n        user = User(name=\"Jason Liu\", age=20)\n    except ValidationError as e:\n        print(e)\n    ```\n\n    ```\n    1 validation error for User\n    name\n      The name is valid but not all lowercase (type=value_error.llm_validator)\n    ```\n\n    Note that there, the error message is written by the LLM, and the error type is `value_error.llm_validator`.\n\n    Parameters:\n        statement (str): The statement to validate\n        model (str): The LLM to use for validation (default: \"gpt-3.5-turbo-0613\")\n        temperature (float): The temperature to use for the LLM (default: 0)\n        openai_client (OpenAI): The OpenAI client to use (default: None)\n    \"\"\"\n\n    openai_client = openai_client if openai_client else patch(OpenAI())\n\n    def llm(v):\n        resp = openai_client.chat.completions.create(\n            response_model=Validator,\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": \"You are a world class validation model. Capable to determine if the following value is valid for the statement, if it is not, explain why and suggest a new value.\",\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"Does `{v}` follow the rules: {statement}\",\n                },\n            ],\n            model=model,\n            temperature=temperature,\n        )  # type: ignore\n\n        # If the response is  not valid, return the reason, this could be used in\n        # the future to generate a better response, via reasking mechanism.\n        assert resp.is_valid, resp.reason\n\n        if allow_override and not resp.is_valid and resp.fixed_value is not None:\n            # If the value is not valid, but we allow override, return the fixed value\n            return resp.fixed_value\n        return v\n\n    return llm\n</code></pre>"},{"location":"api/#instructor.dsl.validators.openai_moderation","title":"<code>openai_moderation(client=None)</code>","text":"<p>Validates a message using OpenAI moderation model.</p> <p>Should only be used for monitoring inputs and outputs of OpenAI APIs Other use cases are disallowed as per: https://platform.openai.com/docs/guides/moderation/overview</p> <p>Example: <pre><code>from instructor import OpenAIModeration\n\nclass Response(BaseModel):\n    message: Annotated[str, AfterValidator(OpenAIModeration(openai_client=client))]\n\nResponse(message=\"I hate you\")\n</code></pre></p> <pre><code> ValidationError: 1 validation error for Response\n message\nValue error, `I hate you.` was flagged for ['harassment'] [type=value_error, input_value='I hate you.', input_type=str]\n</code></pre> <p>client (OpenAI): The OpenAI client to use, must be sync (default: None)</p> Source code in <code>instructor/dsl/validators.py</code> <pre><code>def openai_moderation(client: OpenAI = None):\n    \"\"\"\n    Validates a message using OpenAI moderation model.\n\n    Should only be used for monitoring inputs and outputs of OpenAI APIs\n    Other use cases are disallowed as per:\n    https://platform.openai.com/docs/guides/moderation/overview\n\n    Example:\n    ```python\n    from instructor import OpenAIModeration\n\n    class Response(BaseModel):\n        message: Annotated[str, AfterValidator(OpenAIModeration(openai_client=client))]\n\n    Response(message=\"I hate you\")\n    ```\n\n    ```\n     ValidationError: 1 validation error for Response\n     message\n    Value error, `I hate you.` was flagged for ['harassment'] [type=value_error, input_value='I hate you.', input_type=str]\n    ```\n\n    client (OpenAI): The OpenAI client to use, must be sync (default: None)\n    \"\"\"\n\n    client = client or OpenAI()\n\n    def validate_message_with_openai_mod(v: str) -&gt; str:\n        response = client.moderations.create(input=v)\n        out = response.results[0]\n        cats = out.categories.model_dump()\n        if out.flagged:\n            raise ValueError(\n                f\"`{v}` was flagged for {', '.join(cat for cat in cats if cats[cat])}\"\n            )\n\n        return v\n\n    return validate_message_with_openai_mod\n</code></pre>"},{"location":"api/#instructor.dsl.iterable.IterableModel","title":"<code>IterableModel(subtask_class, name=None, description=None)</code>","text":"<p>Dynamically create a IterableModel OpenAISchema that can be used to segment multiple tasks given a base class. This creates class that can be used to create a toolkit for a specific task, names and descriptions are automatically generated. However they can be overridden.</p>"},{"location":"api/#instructor.dsl.iterable.IterableModel--usage","title":"Usage","text":"<pre><code>from pydantic import BaseModel, Field\nfrom instructor import IterableModel\n\nclass User(BaseModel):\n    name: str = Field(description=\"The name of the person\")\n    age: int = Field(description=\"The age of the person\")\n    role: str = Field(description=\"The role of the person\")\n\nMultiUser = IterableModel(User)\n</code></pre>"},{"location":"api/#instructor.dsl.iterable.IterableModel--result","title":"Result","text":"<pre><code>class MultiUser(OpenAISchema, MultiTaskBase):\n    tasks: List[User] = Field(\n        default_factory=list,\n        repr=False,\n        description=\"Correctly segmented list of `User` tasks\",\n    )\n\n    @classmethod\n    def from_streaming_response(cls, completion) -&gt; Generator[User]:\n        '''\n        Parse the streaming response from OpenAI and yield a `User` object\n        for each task in the response\n        '''\n        json_chunks = cls.extract_json(completion)\n        yield from cls.tasks_from_chunks(json_chunks)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>subtask_class</code> <code>Type[OpenAISchema]</code> <p>The base class to use for the MultiTask</p> required <code>name</code> <code>Optional[str]</code> <p>The name of the MultiTask class, if None then the name of the subtask class is used as <code>Multi{subtask_class.__name__}</code></p> <code>None</code> <code>description</code> <code>Optional[str]</code> <p>The description of the MultiTask class, if None then the description is set to <code>Correct segmentation of</code>{subtask_class.name}<code>tasks</code></p> <code>None</code> <p>Returns:</p> Name Type Description <code>schema</code> <code>OpenAISchema</code> <p>A new class that can be used to segment multiple tasks</p> Source code in <code>instructor/dsl/iterable.py</code> <pre><code>def IterableModel(\n    subtask_class: Type[BaseModel],\n    name: Optional[str] = None,\n    description: Optional[str] = None,\n) -&gt; Type[BaseModel]:\n    \"\"\"\n    Dynamically create a IterableModel OpenAISchema that can be used to segment multiple\n    tasks given a base class. This creates class that can be used to create a toolkit\n    for a specific task, names and descriptions are automatically generated. However\n    they can be overridden.\n\n    ## Usage\n\n    ```python\n    from pydantic import BaseModel, Field\n    from instructor import IterableModel\n\n    class User(BaseModel):\n        name: str = Field(description=\"The name of the person\")\n        age: int = Field(description=\"The age of the person\")\n        role: str = Field(description=\"The role of the person\")\n\n    MultiUser = IterableModel(User)\n    ```\n\n    ## Result\n\n    ```python\n    class MultiUser(OpenAISchema, MultiTaskBase):\n        tasks: List[User] = Field(\n            default_factory=list,\n            repr=False,\n            description=\"Correctly segmented list of `User` tasks\",\n        )\n\n        @classmethod\n        def from_streaming_response(cls, completion) -&gt; Generator[User]:\n            '''\n            Parse the streaming response from OpenAI and yield a `User` object\n            for each task in the response\n            '''\n            json_chunks = cls.extract_json(completion)\n            yield from cls.tasks_from_chunks(json_chunks)\n    ```\n\n    Parameters:\n        subtask_class (Type[OpenAISchema]): The base class to use for the MultiTask\n        name (Optional[str]): The name of the MultiTask class, if None then the name\n            of the subtask class is used as `Multi{subtask_class.__name__}`\n        description (Optional[str]): The description of the MultiTask class, if None\n            then the description is set to `Correct segmentation of `{subtask_class.__name__}` tasks`\n\n    Returns:\n        schema (OpenAISchema): A new class that can be used to segment multiple tasks\n    \"\"\"\n    task_name = subtask_class.__name__ if name is None else name\n\n    name = f\"Iterable{task_name}\"\n\n    list_tasks = (\n        List[subtask_class],\n        Field(\n            default_factory=list,\n            repr=False,\n            description=f\"Correctly segmented list of `{task_name}` tasks\",\n        ),\n    )\n\n    new_cls = create_model(\n        name,\n        tasks=list_tasks,\n        __base__=(OpenAISchema, IterableBase),  # type: ignore\n    )\n    # set the class constructor BaseModel\n    new_cls.task_type = subtask_class\n\n    new_cls.__doc__ = (\n        f\"Correct segmentation of `{task_name}` tasks\"\n        if description is None\n        else description\n    )\n    assert issubclass(\n        new_cls, OpenAISchema\n    ), \"The new class should be a subclass of OpenAISchema\"\n    return new_cls\n</code></pre>"},{"location":"api/#instructor.dsl.partial.Partial","title":"<code>Partial</code>","text":"<p>             Bases: <code>Generic[Model]</code></p> <p>Generate a new class with all attributes optionals.</p> Notes <p>This will wrap a class inheriting form BaseModel and will recursively convert all its attributes and its children's attributes to optionals.</p> Example <p>Partial[SomeModel]</p> Source code in <code>instructor/dsl/partial.py</code> <pre><code>class Partial(Generic[Model]):\n    \"\"\"Generate a new class with all attributes optionals.\n\n    Notes:\n        This will wrap a class inheriting form BaseModel and will recursively\n        convert all its attributes and its children's attributes to optionals.\n\n    Example:\n        Partial[SomeModel]\n    \"\"\"\n\n    def __new__(\n        cls,\n        *args: object,  # noqa :ARG003\n        **kwargs: object,  # noqa :ARG003\n    ) -&gt; \"Partial[Model]\":\n        \"\"\"Cannot instantiate.\n\n        Raises:\n            TypeError: Direct instantiation not allowed.\n        \"\"\"\n        raise TypeError(\"Cannot instantiate abstract Partial class.\")\n\n    def __init_subclass__(\n        cls,\n        *args: object,\n        **kwargs: object,\n    ) -&gt; NoReturn:\n        \"\"\"Cannot subclass.\n\n        Raises:\n           TypeError: Subclassing not allowed.\n        \"\"\"\n        raise TypeError(\"Cannot subclass {}.Partial\".format(cls.__module__))\n\n    def __class_getitem__(  # type: ignore[override]\n        cls,\n        wrapped_class: type[Model],\n    ) -&gt; type[Model]:\n        \"\"\"Convert model to a partial model with all fields being optionals.\"\"\"\n\n        def _make_field_optional(\n            field: FieldInfo,\n        ) -&gt; tuple[object, FieldInfo]:\n            tmp_field = deepcopy(field)\n\n            annotation = field.annotation\n\n            # Handle generics (like List, Dict, etc.)\n            if get_origin(annotation) is not None:\n                # Get the generic base (like List, Dict) and its arguments (like User in List[User])\n                generic_base = get_origin(annotation)\n                generic_args = get_args(annotation)\n\n                # Recursively apply Partial to each of the generic arguments\n                modified_args = tuple(\n                    Partial[arg]\n                    if isinstance(arg, type) and issubclass(arg, BaseModel)\n                    else arg\n                    for arg in generic_args\n                )\n\n                # Reconstruct the generic type with modified arguments\n                tmp_field.annotation = Optional[generic_base[modified_args]]\n                tmp_field.default = None\n            # If the field is a BaseModel, then recursively convert it's\n            # attributes to optionals.\n            elif isinstance(annotation, type) and issubclass(annotation, BaseModel):\n                tmp_field.annotation = Optional[Partial[annotation]]  # type: ignore[assignment, valid-type]\n                tmp_field.default = {}\n            else:\n                tmp_field.annotation = Optional[field.annotation]  # type: ignore[assignment]\n                tmp_field.default = None\n            return tmp_field.annotation, tmp_field\n\n        return create_model(  # type: ignore[no-any-return, call-overload]\n            f\"Partial{wrapped_class.__name__}\",\n            __base__=(wrapped_class, PartialBase),\n            __module__=wrapped_class.__module__,\n            **{\n                field_name: _make_field_optional(field_info)\n                for field_name, field_info in wrapped_class.model_fields.items()\n            },\n        )\n</code></pre>"},{"location":"api/#instructor.dsl.partial.Partial.__class_getitem__","title":"<code>__class_getitem__(wrapped_class)</code>","text":"<p>Convert model to a partial model with all fields being optionals.</p> Source code in <code>instructor/dsl/partial.py</code> <pre><code>def __class_getitem__(  # type: ignore[override]\n    cls,\n    wrapped_class: type[Model],\n) -&gt; type[Model]:\n    \"\"\"Convert model to a partial model with all fields being optionals.\"\"\"\n\n    def _make_field_optional(\n        field: FieldInfo,\n    ) -&gt; tuple[object, FieldInfo]:\n        tmp_field = deepcopy(field)\n\n        annotation = field.annotation\n\n        # Handle generics (like List, Dict, etc.)\n        if get_origin(annotation) is not None:\n            # Get the generic base (like List, Dict) and its arguments (like User in List[User])\n            generic_base = get_origin(annotation)\n            generic_args = get_args(annotation)\n\n            # Recursively apply Partial to each of the generic arguments\n            modified_args = tuple(\n                Partial[arg]\n                if isinstance(arg, type) and issubclass(arg, BaseModel)\n                else arg\n                for arg in generic_args\n            )\n\n            # Reconstruct the generic type with modified arguments\n            tmp_field.annotation = Optional[generic_base[modified_args]]\n            tmp_field.default = None\n        # If the field is a BaseModel, then recursively convert it's\n        # attributes to optionals.\n        elif isinstance(annotation, type) and issubclass(annotation, BaseModel):\n            tmp_field.annotation = Optional[Partial[annotation]]  # type: ignore[assignment, valid-type]\n            tmp_field.default = {}\n        else:\n            tmp_field.annotation = Optional[field.annotation]  # type: ignore[assignment]\n            tmp_field.default = None\n        return tmp_field.annotation, tmp_field\n\n    return create_model(  # type: ignore[no-any-return, call-overload]\n        f\"Partial{wrapped_class.__name__}\",\n        __base__=(wrapped_class, PartialBase),\n        __module__=wrapped_class.__module__,\n        **{\n            field_name: _make_field_optional(field_info)\n            for field_name, field_info in wrapped_class.model_fields.items()\n        },\n    )\n</code></pre>"},{"location":"api/#instructor.dsl.partial.Partial.__init_subclass__","title":"<code>__init_subclass__(*args, **kwargs)</code>","text":"<p>Cannot subclass.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>Subclassing not allowed.</p> Source code in <code>instructor/dsl/partial.py</code> <pre><code>def __init_subclass__(\n    cls,\n    *args: object,\n    **kwargs: object,\n) -&gt; NoReturn:\n    \"\"\"Cannot subclass.\n\n    Raises:\n       TypeError: Subclassing not allowed.\n    \"\"\"\n    raise TypeError(\"Cannot subclass {}.Partial\".format(cls.__module__))\n</code></pre>"},{"location":"api/#instructor.dsl.partial.Partial.__new__","title":"<code>__new__(*args, **kwargs)</code>","text":"<p>Cannot instantiate.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>Direct instantiation not allowed.</p> Source code in <code>instructor/dsl/partial.py</code> <pre><code>def __new__(\n    cls,\n    *args: object,  # noqa :ARG003\n    **kwargs: object,  # noqa :ARG003\n) -&gt; \"Partial[Model]\":\n    \"\"\"Cannot instantiate.\n\n    Raises:\n        TypeError: Direct instantiation not allowed.\n    \"\"\"\n    raise TypeError(\"Cannot instantiate abstract Partial class.\")\n</code></pre>"},{"location":"api/#instructor.dsl.maybe.MaybeBase","title":"<code>MaybeBase</code>","text":"<p>             Bases: <code>BaseModel</code>, <code>Generic[T]</code></p> <p>Extract a result from a model, if any, otherwise set the error and message fields.</p> Source code in <code>instructor/dsl/maybe.py</code> <pre><code>class MaybeBase(BaseModel, Generic[T]):\n    \"\"\"\n    Extract a result from a model, if any, otherwise set the error and message fields.\n    \"\"\"\n\n    result: Optional[T]\n    error: bool = Field(default=False)\n    message: Optional[str]\n\n    def __bool__(self):\n        return self.result is not None  # type: ignore\n</code></pre>"},{"location":"api/#instructor.dsl.maybe.Maybe","title":"<code>Maybe(model)</code>","text":"<p>Create a Maybe model for a given Pydantic model. This allows you to return a model that includes fields for <code>result</code>, <code>error</code>, and <code>message</code> for sitatations where the data may not be present in the context.</p>"},{"location":"api/#instructor.dsl.maybe.Maybe--usage","title":"Usage","text":"<pre><code>from pydantic import BaseModel, Field\nfrom instructor import Maybe\n\nclass User(BaseModel):\n    name: str = Field(description=\"The name of the person\")\n    age: int = Field(description=\"The age of the person\")\n    role: str = Field(description=\"The role of the person\")\n\nMaybeUser = Maybe(User)\n</code></pre>"},{"location":"api/#instructor.dsl.maybe.Maybe--result","title":"Result","text":"<pre><code>class MaybeUser(BaseModel):\n    result: Optional[User]\n    error: bool = Field(default=False)\n    message: Optional[str]\n\n    def __bool__(self):\n        return self.result is not None\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Type[BaseModel]</code> <p>The Pydantic model to wrap with Maybe.</p> required <p>Returns:</p> Name Type Description <code>MaybeModel</code> <code>Type[BaseModel]</code> <p>A new Pydantic model that includes fields for <code>result</code>, <code>error</code>, and <code>message</code>.</p> Source code in <code>instructor/dsl/maybe.py</code> <pre><code>def Maybe(model: Type[T]) -&gt; Type[MaybeBase[T]]:\n    \"\"\"\n    Create a Maybe model for a given Pydantic model. This allows you to return a model that includes fields for `result`, `error`, and `message` for sitatations where the data may not be present in the context.\n\n    ## Usage\n\n    ```python\n    from pydantic import BaseModel, Field\n    from instructor import Maybe\n\n    class User(BaseModel):\n        name: str = Field(description=\"The name of the person\")\n        age: int = Field(description=\"The age of the person\")\n        role: str = Field(description=\"The role of the person\")\n\n    MaybeUser = Maybe(User)\n    ```\n\n    ## Result\n\n    ```python\n    class MaybeUser(BaseModel):\n        result: Optional[User]\n        error: bool = Field(default=False)\n        message: Optional[str]\n\n        def __bool__(self):\n            return self.result is not None\n    ```\n\n    Parameters:\n        model (Type[BaseModel]): The Pydantic model to wrap with Maybe.\n\n    Returns:\n        MaybeModel (Type[BaseModel]): A new Pydantic model that includes fields for `result`, `error`, and `message`.\n    \"\"\"\n\n    fields = {\n        \"result\": (\n            Optional[model],\n            Field(\n                default=None,\n                description=\"Correctly extracted result from the model, if any, otherwise None\",\n            ),\n        ),\n        \"error\": (bool, Field(default=False)),\n        \"message\": (\n            Optional[str],\n            Field(\n                default=None,\n                description=\"Error message if no result was found, should be short and concise\",\n            ),\n        ),\n    }\n\n    return create_model(f\"Maybe{model.__name__}\", __base__=MaybeBase, **fields)\n</code></pre>"},{"location":"api/#instructor.function_calls.Mode","title":"<code>Mode</code>","text":"<p>             Bases: <code>Enum</code></p> <p>The mode to use for patching the client</p> Source code in <code>instructor/function_calls.py</code> <pre><code>class Mode(enum.Enum):\n    \"\"\"The mode to use for patching the client\"\"\"\n\n    FUNCTIONS: str = \"function_call\"\n    PARALLEL_TOOLS: str = \"parallel_tool_call\"\n    TOOLS: str = \"tool_call\"\n    JSON: str = \"json_mode\"\n    MD_JSON: str = \"markdown_json_mode\"\n    JSON_SCHEMA: str = \"json_schema_mode\"\n\n    def __new__(cls, value: str) -&gt; \"Mode\":\n        member = object.__new__(cls)\n        member._value_ = value\n\n        # Deprecation warning for FUNCTIONS\n        if value == \"function_call\":\n            warnings.warn(\n                \"FUNCTIONS is deprecated and will be removed in future versions\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n\n        return member\n</code></pre>"},{"location":"api/#instructor.function_calls.OpenAISchema","title":"<code>OpenAISchema</code>","text":"<p>             Bases: <code>BaseModel</code></p> Source code in <code>instructor/function_calls.py</code> <pre><code>class OpenAISchema(BaseModel):  # type: ignore[misc]\n    @classmethod  # type: ignore[misc]\n    @property\n    def openai_schema(cls) -&gt; Dict[str, Any]:\n        \"\"\"\n        Return the schema in the format of OpenAI's schema as jsonschema\n\n        Note:\n            Its important to add a docstring to describe how to best use this class, it will be included in the description attribute and be part of the prompt.\n\n        Returns:\n            model_json_schema (dict): A dictionary in the format of OpenAI's schema as jsonschema\n        \"\"\"\n        schema = cls.model_json_schema()\n        docstring = parse(cls.__doc__ or \"\")\n        parameters = {\n            k: v for k, v in schema.items() if k not in (\"title\", \"description\")\n        }\n        for param in docstring.params:\n            if (name := param.arg_name) in parameters[\"properties\"] and (\n                description := param.description\n            ):\n                if \"description\" not in parameters[\"properties\"][name]:\n                    parameters[\"properties\"][name][\"description\"] = description\n\n        parameters[\"required\"] = sorted(\n            k for k, v in parameters[\"properties\"].items() if \"default\" not in v\n        )\n\n        if \"description\" not in schema:\n            if docstring.short_description:\n                schema[\"description\"] = docstring.short_description\n            else:\n                schema[\"description\"] = (\n                    f\"Correctly extracted `{cls.__name__}` with all \"\n                    f\"the required parameters with correct types\"\n                )\n\n        return {\n            \"name\": schema[\"title\"],\n            \"description\": schema[\"description\"],\n            \"parameters\": parameters,\n        }\n\n    @classmethod\n    def from_response(\n        cls,\n        completion: T,\n        validation_context: Optional[Dict[str, Any]] = None,\n        strict: Optional[bool] = None,\n        mode: Mode = Mode.TOOLS,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Execute the function from the response of an openai chat completion\n\n        Parameters:\n            completion (openai.ChatCompletion): The response from an openai chat completion\n            throw_error (bool): Whether to throw an error if the function call is not detected\n            validation_context (dict): The validation context to use for validating the response\n            strict (bool): Whether to use strict json parsing\n            mode (Mode): The openai completion mode\n\n        Returns:\n            cls (OpenAISchema): An instance of the class\n        \"\"\"\n        assert hasattr(completion, \"choices\")\n\n        if completion.choices[0].finish_reason == \"length\":\n            raise IncompleteOutputException()\n\n        message = completion.choices[0].message\n\n        if mode == Mode.FUNCTIONS:\n            assert (\n                message.function_call.name == cls.openai_schema[\"name\"]  # type: ignore[index]\n            ), \"Function name does not match\"\n            return cls.model_validate_json(\n                message.function_call.arguments,\n                context=validation_context,\n                strict=strict,\n            )\n        elif mode == Mode.TOOLS:\n            assert (\n                len(message.tool_calls) == 1\n            ), \"Instructor does not support multiple tool calls, use List[Model] instead.\"\n            tool_call = message.tool_calls[0]\n            assert (\n                tool_call.function.name == cls.openai_schema[\"name\"]  # type: ignore[index]\n            ), \"Tool name does not match\"\n            return cls.model_validate_json(\n                tool_call.function.arguments,\n                context=validation_context,\n                strict=strict,\n            )\n        elif mode in {Mode.JSON, Mode.JSON_SCHEMA, Mode.MD_JSON}:\n            return cls.model_validate_json(\n                message.content,\n                context=validation_context,\n                strict=strict,\n            )\n        else:\n            raise ValueError(f\"Invalid patch mode: {mode}\")\n</code></pre>"},{"location":"api/#instructor.function_calls.OpenAISchema.openai_schema","title":"<code>openai_schema: Dict[str, Any]</code>  <code>classmethod</code> <code>property</code>","text":"<p>Return the schema in the format of OpenAI's schema as jsonschema</p> Note <p>Its important to add a docstring to describe how to best use this class, it will be included in the description attribute and be part of the prompt.</p> <p>Returns:</p> Name Type Description <code>model_json_schema</code> <code>dict</code> <p>A dictionary in the format of OpenAI's schema as jsonschema</p>"},{"location":"api/#instructor.function_calls.OpenAISchema.from_response","title":"<code>from_response(completion, validation_context=None, strict=None, mode=Mode.TOOLS)</code>  <code>classmethod</code>","text":"<p>Execute the function from the response of an openai chat completion</p> <p>Parameters:</p> Name Type Description Default <code>completion</code> <code>ChatCompletion</code> <p>The response from an openai chat completion</p> required <code>throw_error</code> <code>bool</code> <p>Whether to throw an error if the function call is not detected</p> required <code>validation_context</code> <code>dict</code> <p>The validation context to use for validating the response</p> <code>None</code> <code>strict</code> <code>bool</code> <p>Whether to use strict json parsing</p> <code>None</code> <code>mode</code> <code>Mode</code> <p>The openai completion mode</p> <code>TOOLS</code> <p>Returns:</p> Name Type Description <code>cls</code> <code>OpenAISchema</code> <p>An instance of the class</p> Source code in <code>instructor/function_calls.py</code> <pre><code>@classmethod\ndef from_response(\n    cls,\n    completion: T,\n    validation_context: Optional[Dict[str, Any]] = None,\n    strict: Optional[bool] = None,\n    mode: Mode = Mode.TOOLS,\n) -&gt; Dict[str, Any]:\n    \"\"\"Execute the function from the response of an openai chat completion\n\n    Parameters:\n        completion (openai.ChatCompletion): The response from an openai chat completion\n        throw_error (bool): Whether to throw an error if the function call is not detected\n        validation_context (dict): The validation context to use for validating the response\n        strict (bool): Whether to use strict json parsing\n        mode (Mode): The openai completion mode\n\n    Returns:\n        cls (OpenAISchema): An instance of the class\n    \"\"\"\n    assert hasattr(completion, \"choices\")\n\n    if completion.choices[0].finish_reason == \"length\":\n        raise IncompleteOutputException()\n\n    message = completion.choices[0].message\n\n    if mode == Mode.FUNCTIONS:\n        assert (\n            message.function_call.name == cls.openai_schema[\"name\"]  # type: ignore[index]\n        ), \"Function name does not match\"\n        return cls.model_validate_json(\n            message.function_call.arguments,\n            context=validation_context,\n            strict=strict,\n        )\n    elif mode == Mode.TOOLS:\n        assert (\n            len(message.tool_calls) == 1\n        ), \"Instructor does not support multiple tool calls, use List[Model] instead.\"\n        tool_call = message.tool_calls[0]\n        assert (\n            tool_call.function.name == cls.openai_schema[\"name\"]  # type: ignore[index]\n        ), \"Tool name does not match\"\n        return cls.model_validate_json(\n            tool_call.function.arguments,\n            context=validation_context,\n            strict=strict,\n        )\n    elif mode in {Mode.JSON, Mode.JSON_SCHEMA, Mode.MD_JSON}:\n        return cls.model_validate_json(\n            message.content,\n            context=validation_context,\n            strict=strict,\n        )\n    else:\n        raise ValueError(f\"Invalid patch mode: {mode}\")\n</code></pre>"},{"location":"contributing/","title":"Contributing","text":"<p>We would love for you to contribute to <code>Instructor</code>.</p>"},{"location":"contributing/#evals","title":"Evals","text":"<p>We invite you to contribute evals in pytest as a way to monitor the quality of the openai models and the instructor library. To get started check out the jxnl/instructor/tests/evals and contribute your own evals in the form of pytest tests. These evals will be run once a week and the results will be posted.</p>"},{"location":"contributing/#issues","title":"Issues","text":"<p>If you find a bug, please file an issue on our issue tracker on GitHub.</p> <p>To help us reproduce the bug, please provide a minimal reproducible example, including a code snippet and the full error message.</p> <ol> <li>The <code>response_model</code> you are using.</li> <li>The <code>messages</code> you are using.</li> <li>The <code>model</code> you are using.</li> </ol>"},{"location":"contributing/#pull-requests","title":"Pull Requests","text":"<p>We welcome pull requests! There is plenty to do, and we are happy to discuss any contributions you would like to make.</p> <p>If it is not a small change, please start by filing an issue first.</p> <p>If you need ideas, you can check out the help wanted or good first issue labels.</p> <p>Grit is used to enforce best practices. You can run <code>grit check</code> to check your code before submitting a pull request.</p>"},{"location":"contributing/#contributors","title":"Contributors","text":""},{"location":"contributing/#additional-resources","title":"Additional Resources","text":"<p>To enhance your understanding of the documentation, here are some useful references:</p> <ul> <li> <p>mkdocs serve: The <code>mkdocs serve</code> command is used to preview your documentation locally during the development phase. When you run this command in your terminal, MkDocs starts a development server, allowing you to view and interact with your documentation in a web browser. This is helpful for checking how your changes look before publishing the documentation. Learn more in the mkdocs serve documentation.</p> </li> <li> <p>hl_lines in Code Blocks: The <code>hl_lines</code> feature in code blocks allows you to highlight specific lines within the code block. This is useful for drawing attention to particular lines of code when explaining examples or providing instructions. You can specify the lines to highlight using the <code>hl_lines</code> option in your code block configuration. For more details and examples, you can refer to the hl_lines documentation.</p> </li> <li> <p>Admonitions: Admonitions are a way to visually emphasize or call attention to certain pieces of information in your documentation. They come in various styles, such as notes, warnings, tips, etc. Admonitions provide a structured and consistent way to present important content. For usage examples and details on incorporating admonitions into your documentation, you can refer to the admonitions documentation.</p> </li> </ul> <p>For more details about the documentation structure and features, refer to the MkDocs Material documentation.</p> <p>Thank you for your contributions, and happy coding!</p>"},{"location":"help/","title":"Getting help with Instructor","text":"<p>If you need help getting started with Instructor or with advanced usage, the following sources may be useful.</p>"},{"location":"help/#material-discord-discord","title":":material-discord: Discord","text":"<p>The Discord is a great place to ask questions and get help from the community.</p>"},{"location":"help/#concepts","title":"Concepts","text":"<p>The concepts section explains the core concepts of Instructor and how to prompt with models.</p>"},{"location":"help/#cookbooks","title":"Cookbooks","text":"<p>The cookbooks are a great place to start. They contain a variety of examples that demonstrate how to use Instructor in different scenarios.</p>"},{"location":"help/#blog","title":"Blog","text":"<p>The blog contains articles that explain how to use Instructor in different scenarios.</p>"},{"location":"help/#github-discussions","title":"GitHub Discussions","text":"<p>GitHub discussions are useful for asking questions, your question and the answer will help everyone.</p>"},{"location":"help/#github-issues","title":"GitHub Issues","text":"<p>GitHub issues are useful for reporting bugs or requesting new features.</p>"},{"location":"help/#twitter","title":"Twitter","text":"<p>You can also reach out to me on Twitter if you have any questions or ideas.</p>"},{"location":"installation/","title":"Installation","text":"<p>Installation is as simple as:</p> <pre><code>pip install instructor\n</code></pre> <p>Instructor has a few dependencies:</p> <ul> <li><code>openai</code>: OpenAI's Python client.</li> <li><code>typer</code>: Build great CLIs. Easy to code. Based on Python type hints.</li> <li><code>docstring-parser</code>: A parser for Python docstrings, to improve the experience of working with docstrings in jsonschema.</li> <li><code>pydantic</code>: Data validation and settings management using python type annotations.</li> </ul> <p>If you've got Python 3.9+ and <code>pip</code> installed, you're good to go.</p>"},{"location":"why/","title":"Why use Instructor?","text":"Why use Pydantic? <p>Its hard to answer the question of why use Instructor without first answering why use Pydantic.:</p> <ul> <li> <p>Powered by type hints \u2014 with Pydantic, schema validation and serialization are controlled by type annotations; less to learn, less code to write, and integration with your IDE and static analysis tools.</p> </li> <li> <p>Speed \u2014 Pydantic's core validation logic is written in Rust. As a result, Pydantic is among the fastest data validation libraries for Python.</p> </li> <li> <p>JSON Schema \u2014 Pydantic models can emit JSON Schema, allowing for easy integration with other tools. [Learn more\u2026]</p> </li> <li> <p>Customisation \u2014 Pydantic allows custom validators and serializers to alter how data is processed in many powerful ways.</p> </li> <li> <p>Ecosystem \u2014 around 8,000 packages on PyPI use Pydantic, including massively popular libraries like FastAPI, huggingface, Django Ninja, SQLModel, &amp; LangChain.</p> </li> <li> <p>Battle tested \u2014 Pydantic is downloaded over 70M times/month and is used by all FAANG companies and 20 of the 25 largest companies on NASDAQ. If you're trying to do something with Pydantic, someone else has probably already done it.</p> </li> </ul> <p>Our <code>instructor.patch</code> for the <code>OpenAI</code> class introduces three key enhancements:</p> <ul> <li>Response Mode: Specify a Pydantic model to streamline data extraction.</li> <li>Max Retries: Set your desired number of retry attempts for requests.</li> <li>Validation Context: Provide a context object for enhanced validator access.   A Glimpse into Instructor's Capabilities</li> </ul> <p>Using Validators</p> <p>Learn more about validators checkout our blog post Good llm validation is just good validation</p> <p>With Instructor, your code becomes more efficient and readable. Here\u2019s a quick peek:</p>"},{"location":"why/#understanding-the-patch","title":"Understanding the <code>patch</code>","text":"<p>Lets go over the <code>patch</code> function. And see how we can leverage it to make use of instructor</p>"},{"location":"why/#step-1-patch-the-client","title":"Step 1: Patch the client","text":"<p>First, import the required libraries and apply the <code>patch</code> function to the OpenAI module. This exposes new functionality with the <code>response_model</code> parameter.</p> <pre><code>import instructor\nfrom openai import OpenAI\n\n# This enables response_model keyword\n# from client.chat.completions.create\nclient = instructor.patch(OpenAI())\n</code></pre>"},{"location":"why/#step-2-define-the-pydantic-model","title":"Step 2: Define the Pydantic Model","text":"<p>Create a Pydantic model to define the structure of the data you want to extract. This model will map directly to the information in the prompt.</p> <pre><code>from pydantic import BaseModel\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n</code></pre>"},{"location":"why/#step-3-extract","title":"Step 3: Extract","text":"<p>Use the <code>client.chat.completions.create</code> method to send a prompt and extract the data into the Pydantic object. The <code>response_model</code> parameter specifies the Pydantic model to use for extraction. Its helpful to annotate the variable with the type of the response model, which will help your IDE provide autocomplete and spell check.</p> <pre><code>user: UserDetail = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserDetail,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"},\n    ],\n)\n\nassert user.name == \"Jason\"\nassert user.age == 25\n</code></pre>"},{"location":"why/#understanding-validation","title":"Understanding Validation","text":"<p>Validation can also be plugged into the same Pydantic model. Here, if the answer attribute contains content that violates the rule \"don't say objectionable things,\" Pydantic will raise a validation error.</p> <pre><code>from pydantic import BaseModel, ValidationError, BeforeValidator\nfrom typing_extensions import Annotated\nfrom instructor import llm_validator\n\n\nclass QuestionAnswer(BaseModel):\n    question: str\n    answer: Annotated[\n        str, BeforeValidator(llm_validator(\"don't say objectionable things\"))\n    ]\n\n\ntry:\n    qa = QuestionAnswer(\n        question=\"What is the meaning of life?\",\n        answer=\"The meaning of life is to be evil and steal\",\n    )\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for QuestionAnswer\n    answer\n      Assertion failed, The statement promotes objectionable behavior. [type=assertion_error, input_value='The meaning of life is to be evil and steal', input_type=str]\n        For further information visit https://errors.pydantic.dev/2.6/v/assertion_error\n    \"\"\"\n</code></pre> <p>Its important to note here that the error message is generated by the LLM, not the code, so it'll be helpful for re-asking the model.</p> <pre><code>1 validation error for QuestionAnswer\nanswer\n   Assertion failed, The statement is objectionable. (type=assertion_error)\n</code></pre>"},{"location":"why/#self-correcting-on-validation-error","title":"Self Correcting on Validation Error","text":"<p>Here, the <code>UserDetails</code> model is passed as the <code>response_model</code>, and <code>max_retries</code> is set to 2.</p> <pre><code>import instructor\n\nfrom openai import OpenAI\nfrom pydantic import BaseModel, field_validator\n\n# Apply the patch to the OpenAI client\nclient = instructor.patch(OpenAI())\n\n\nclass UserDetails(BaseModel):\n    name: str\n    age: int\n\n    @field_validator(\"name\")\n    @classmethod\n    def validate_name(cls, v):\n        if v.upper() != v:\n            raise ValueError(\"Name must be in uppercase.\")\n        return v\n\n\nmodel = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserDetails,\n    max_retries=2,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"},\n    ],\n)\n\nassert model.name == \"JASON\"\n</code></pre>"},{"location":"why/#iterables-and-lists","title":"Iterables and Lists","text":"<p>We can also generate tasks as the tokens are streamed in by defining an <code>Iterable[T]</code> type.</p> <p>Lets look at an example in action with the same class</p> <pre><code>from typing import Iterable\n\nUsers = Iterable[User]\n\nusers = client.chat.completions.create(\n    model=\"gpt-4\",\n    temperature=0.1,\n    stream=True,\n    response_model=Users,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a perfect entity extraction system\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": (\n                f\"Consider the data below:\\n{input}\"\n                \"Correctly segment it into entitites\"\n                \"Make sure the JSON is correct\"\n            ),\n        },\n    ],\n    max_tokens=1000,\n)\n\nfor user in users:\n    assert isinstance(user, User)\n    print(user)\n\n#&gt; name=\"Jason\" \"age\"=10\n#&gt; name=\"John\" \"age\"=10\n</code></pre>"},{"location":"why/#partial-extraction","title":"Partial Extraction","text":"<p>We also support partial extraction, which is useful for streaming in data that is incomplete.</p> <pre><code>import instructor\n\nfrom instructor import Partial\nfrom openai import OpenAI\nfrom pydantic import BaseModel\nfrom typing import List\nfrom rich.console import Console\n\nclient = instructor.patch(OpenAI())\n\ntext_block = \"\"\"\nIn our recent online meeting, participants from various backgrounds joined to discuss the upcoming tech conference. The names and contact details of the participants were as follows:\n\n- Name: John Doe, Email: johndoe@email.com, Twitter: @TechGuru44\n- Name: Jane Smith, Email: janesmith@email.com, Twitter: @DigitalDiva88\n- Name: Alex Johnson, Email: alexj@email.com, Twitter: @CodeMaster2023\n\nDuring the meeting, we agreed on several key points. The conference will be held on March 15th, 2024, at the Grand Tech Arena located at 4521 Innovation Drive. Dr. Emily Johnson, a renowned AI researcher, will be our keynote speaker.\n\nThe budget for the event is set at $50,000, covering venue costs, speaker fees, and promotional activities. Each participant is expected to contribute an article to the conference blog by February 20th.\n\nA follow-up meetingis scheduled for January 25th at 3 PM GMT to finalize the agenda and confirm the list of speakers.\n\"\"\"\n\n\nclass User(BaseModel):\n    name: str\n    email: str\n    twitter: str\n\n\nclass MeetingInfo(BaseModel):\n    users: List[User]\n    date: str\n    location: str\n    budget: int\n    deadline: str\n\n\nextraction_stream = client.chat.completions.create(\n    model=\"gpt-4\",\n    response_model=Partial[MeetingInfo],\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": f\"Get the information about the meeting and the users {text_block}\",\n        },\n    ],\n    stream=True,\n)\n\n\nconsole = Console()\n\nfor extraction in extraction_stream:\n    obj = extraction.model_dump()\n    console.clear()\n    console.print(obj)\n</code></pre> <p>This will output the following:</p> <p></p> <p>As you can see, we've baked in a self correcting mechanism into the model. This is a powerful way to make your models more robust and less brittle without including a lot of extra code or prompts.</p>"},{"location":"blog/","title":"Welcome to the Instructor Blog","text":"<p>The goal of the blog is to capture some content that does not neatly fit within documentation or the cookbooks.</p>"},{"location":"blog/#advanced-topics","title":"Advanced Topics","text":"<ol> <li>What is Query Understanding, how does it go beyond embeddings?</li> <li>How can one achieve GPT-4 level summaries using GPT-3.5-turbo?</li> <li>What are the basics of Guardrails and Validation in AI models?</li> <li>How does one validate citations in AI-generated content?</li> <li>What are the methods and benefits of fine-tuning and distillation in AI models?</li> <li>How can I use Anyscale with Instructor?</li> </ol>"},{"location":"blog/#learning-python","title":"Learning Python","text":"<ul> <li>How can I effectively cache my functions in Python?</li> <li>What are the fundamentals of batch processing with async in Python?</li> <li>How can I stream models to improve latency?</li> </ul>"},{"location":"blog/#talks","title":"Talks","text":"<ul> <li>What were the key insights and topics covered at the AI Engineering Summit 2023?</li> </ul>"},{"location":"blog/2023/11/02/ai-engineer-keynote-pydantic-is-all-you-need/","title":"AI Engineer Keynote: Pydantic is all you need","text":"<p>Click here to watch the full talk</p> <p>Last month, I ventured back onto the speaking circuit at the inaugural AI Engineer Summit, sharing insights on leveraging Pydantic for effective prompt engineering. I dove deep into what is covered in our documentation and standard blog posts,</p> <p>I'd genuinely appreciate any feedback on the talk \u2013 every bit helps in refining the art. So, take a moment to check out the full talk here, and let's continue pushing the boundaries of what's possible.</p>","tags":["python","talks","prompt engineering","video"]},{"location":"blog/2023/12/15/patching/","title":"Structured Outputs with Anyscale","text":"<p>Open-source LLMS are gaining popularity, and the release of Anyscale's Mistral model has made it possible to obtain structured outputs using JSON schema at any scale. Instead of relying on a model's default output mode, you can utilize JSON schema to obtain structured outputs. This approach is a time-saving alternative to extensive prompt engineering.</p> <p>By the end of this blog post, you will learn how to effectively utilize the instructor at any scale. But before we proceed, let's first explore the concept of patching.</p>","tags":["patching","open source"]},{"location":"blog/2023/12/15/patching/#patching","title":"Patching","text":"<p>Instructor's patch enhances a openai api it with the following features:</p> <ul> <li><code>response_model</code> in <code>create</code> calls that returns a pydantic model</li> <li><code>max_retries</code> in <code>create</code> calls that retries the call if it fails by using a backoff strategy</li> </ul> <p>Learn More</p> <p>To learn more, please refer to the docs. To understand the benefits of using Pydantic with Instructor, visit the tips and tricks section of the why use Pydantic page.</p>","tags":["patching","open source"]},{"location":"blog/2023/12/15/patching/#anyscale","title":"Anyscale","text":"<p>The good news is that Anyscale employs the same OpenAI client, and its models support some of these output modes too!</p> <p>Getting access</p> <p>If you want to try this out for yourself check out the Anyscale website. You can get started here.</p> <p>Let's explore one of the models available in Anyscale's extensive collection!</p> <pre><code>from openai import OpenAI\nfrom pydantic import BaseModel\n\nimport instructor\n\n\nclass UserDetails(BaseModel):\n    name: str\n    age: int\n\n\n# enables `response_model` in create call\nclient = instructor.patch(\n    OpenAI(\n        base_url=\"https://api.endpoints.anyscale.com/v1\",\n        api_key=\"&lt;YOUR_ANYSCALE_API_KEY&gt;\",\n    ),\n    # This uses Anyscale's json schema output mode\n    mode=instructor.Mode.JSON_SCHEMA,\n)\n\nresp = client.chat.completions.create(\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a world class extractor\"},\n        {\"role\": \"user\", \"content\": 'Extract the following entities: \"Jason is 20\"'},\n    ],\n    response_model=UserDetails,\n)\nprint(resp)\n# # &gt; name='Jason' age=20\n</code></pre> <p>You can find more information about Anyscale's output mode support here.</p>","tags":["patching","open source"]},{"location":"blog/2023/11/26/python-caching/","title":"Introduction to Caching in Python","text":"<p>Instructor makes working with language models easy, but they are still computationally expensive.</p> <p>Today, we're diving into optimizing instructor code while maintaining the excellent DX offered by Pydantic models. We'll tackle the challenges of caching Pydantic models, typically incompatible with <code>pickle</code>, and explore solutions that use <code>decorators</code> like <code>functools.cache</code>. Then, we'll craft custom decorators with <code>diskcache</code> and <code>redis</code> to support persistent caching and distributed systems.</p> <p>Let's first consider our canonical example, using the <code>OpenAI</code> Python client to extract user details.</p> <pre><code>import instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\n# Enables `response_model`\nclient = instructor.patch(OpenAI())\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\ndef extract(data) -&gt; UserDetail:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[\n            {\"role\": \"user\", \"content\": data},\n        ],\n    )\n</code></pre> <p>Now imagine batch processing data, running tests or experiments, or simply calling <code>extract</code> multiple times over a workflow. We'll quickly run into performance issues, as the function may be called repeatedly, and the same data will be processed over and over again, costing us time and money.</p>","tags":["caching","functools","redis","diskcache","python"]},{"location":"blog/2023/11/26/python-caching/#1-functoolscache-for-simple-in-memory-caching","title":"1. <code>functools.cache</code> for Simple In-Memory Caching","text":"<p>When to Use: Ideal for functions with immutable arguments, called repeatedly with the same parameters in small to medium-sized applications. This makes sense when we might be reusing the same data within a single session or in an application where we don't need to persist the cache between sessions.</p> <pre><code>import functools\n\n\n@functools.cache\ndef extract(data):\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[\n            {\"role\": \"user\", \"content\": data},\n        ],\n    )\n</code></pre> <p>Changing the Model does not Invalidate the Cache</p> <p>Note that changing the model does not invalidate the cache. This is because the cache key is based on the function's name and arguments, not the model. This means that if we change the model, the cache will still return the old result.</p> <p>Now we can call <code>extract</code> multiple times with the same argument, and the result will be cached in memory for faster access.</p> <pre><code>import time\n\nstart = time.perf_counter()  # (1)\nmodel = extract(\"Extract jason is 25 years old\")\nprint(f\"Time taken: {time.perf_counter() - start}\")\n\nstart = time.perf_counter()\nmodel = extract(\"Extract jason is 25 years old\")  # (2)\nprint(f\"Time taken: {time.perf_counter() - start}\")\n\n#&gt; Time taken: 0.92\n#&gt; Time taken: 1.20e-06 # (3)\n</code></pre> <ol> <li>Using <code>time.perf_counter()</code> to measure the time taken to run the function is better than using <code>time.time()</code> because it's more accurate and less susceptible to system clock changes.</li> <li>The second time we call <code>extract</code>, the result is returned from the cache, and the function is not called.</li> <li>The second call to <code>extract</code> is much faster because the result is returned from the cache!</li> </ol> <p>Benefits: Easy to implement, provides fast access due to in-memory storage, and requires no additional libraries.</p> What is a decorator? <p>A decorator is a function that takes another function and extends the behavior of the latter function without explicitly modifying it. In Python, decorators are functions that take a function as an argument and return a closure.</p> <pre><code>def decorator(func):\n    def wrapper(*args, **kwargs):\n        print(\"Do something before\")  # (1)\n        result = func(*args, **kwargs)\n        print(\"Do something after\")  # (2)\n        return result\n\n    return wrapper\n\n\n@decorator\ndef say_hello():\n    print(\"Hello!\")\n\n\nsay_hello()\n#&gt; \"Do something before\"\n#&gt; \"Hello!\"\n#&gt; \"Do something after\"\n</code></pre> <ol> <li>The code is executed before the function is called</li> <li>The code is executed after the function is called</li> </ol>","tags":["caching","functools","redis","diskcache","python"]},{"location":"blog/2023/11/26/python-caching/#2-diskcache-for-persistent-large-data-caching","title":"2. <code>diskcache</code> for Persistent, Large Data Caching","text":"Copy Caching Code <p>We'll be using the same <code>instructor_cache</code> decorator for both <code>diskcache</code> and <code>redis</code> caching. You can copy the code below and use it for both examples.</p> <pre><code>import functools\nimport inspect\nimport diskcache\n\ncache = diskcache.Cache('./my_cache_directory')  # (1)\n\n\ndef instructor_cache(func):\n    \"\"\"Cache a function that returns a Pydantic model\"\"\"\n    return_type = inspect.signature(func).return_annotation\n    if not issubclass(return_type, BaseModel):  # (2)\n        raise ValueError(\"The return type must be a Pydantic model\")\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = f\"{func.__name__}-{functools._make_key(args, kwargs, typed=False)}\"\n        # Check if the result is already cached\n        if (cached := cache.get(key)) is not None:\n            # Deserialize from JSON based on the return type\n            return return_type.model_validate_json(cached)\n\n        # Call the function and cache its result\n        result = func(*args, **kwargs)\n        serialized_result = result.model_dump_json()\n        cache.set(key, serialized_result)\n\n        return result\n\n    return wrapper\n</code></pre> <ol> <li>We create a new <code>diskcache.Cache</code> instance to store the cached data. This will create a new directory called <code>my_cache_directory</code> in the current working directory.</li> <li>We only want to cache functions that return a Pydantic model to simplify serialization and deserialization logic in this example code</li> </ol> <p>Remember that you can change this code to support non-Pydantic models, or to use a different caching backend. More over, don't forget that this cache does not invalidate when the model changes, so you might want to encode the <code>Model.model_json_schema()</code> as part of the key.</p> <p>When to Use: Suitable for applications needing cache persistence between sessions or dealing with large datasets. This is useful when we want to reuse the same data across multiple sessions, or when we need to store large amounts of data!</p> <pre><code>import functools\nimport inspect\nimport instructor\nimport diskcache\n\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\nclient = instructor.patch(OpenAI())\ncache = diskcache.Cache('./my_cache_directory')\n\n\ndef instructor_cache(func):\n    \"\"\"Cache a function that returns a Pydantic model\"\"\"\n    return_type = inspect.signature(func).return_annotation  # (4)\n    if not issubclass(return_type, BaseModel):  # (1)\n        raise ValueError(\"The return type must be a Pydantic model\")\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = (\n            f\"{func.__name__}-{functools._make_key(args, kwargs, typed=False)}\"  #  (2)\n        )\n        # Check if the result is already cached\n        if (cached := cache.get(key)) is not None:\n            # Deserialize from JSON based on the return type (3)\n            return return_type.model_validate_json(cached)\n\n        # Call the function and cache its result\n        result = func(*args, **kwargs)\n        serialized_result = result.model_dump_json()\n        cache.set(key, serialized_result)\n\n        return result\n\n    return wrapper\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\n@instructor_cache\ndef extract(data) -&gt; UserDetail:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[\n            {\"role\": \"user\", \"content\": data},\n        ],\n    )\n</code></pre> <ol> <li>We only want to cache functions that return a Pydantic model to simplify serialization and deserialization logic</li> <li>We use functool's <code>_make_key</code> to generate a unique key based on the function's name and arguments. This is important because we want to cache the result of each function call separately.</li> <li>We use Pydantic's <code>model_validate_json</code> to deserialize the cached result into a Pydantic model.</li> <li>We use <code>inspect.signature</code> to get the function's return type annotation, which we use to validate the cached result.</li> </ol> <p>Benefits: Reduces computation time for heavy data processing, provides disk-based caching for persistence.</p>","tags":["caching","functools","redis","diskcache","python"]},{"location":"blog/2023/11/26/python-caching/#2-redis-caching-decorator-for-distributed-systems","title":"2. Redis Caching Decorator for Distributed Systems","text":"Copy Caching Code <p>We'll be using the same <code>instructor_cache</code> decorator for both <code>diskcache</code> and <code>redis</code> caching. You can copy the code below and use it for both examples.</p> <pre><code>import functools\nimport inspect\nimport redis\n\ncache = redis.Redis(\"localhost\")\n\n\ndef instructor_cache(func):\n    \"\"\"Cache a function that returns a Pydantic model\"\"\"\n    return_type = inspect.signature(func).return_annotation\n    if not issubclass(return_type, BaseModel):\n        raise ValueError(\"The return type must be a Pydantic model\")\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = f\"{func.__name__}-{functools._make_key(args, kwargs, typed=False)}\"\n        # Check if the result is already cached\n        if (cached := cache.get(key)) is not None:\n            # Deserialize from JSON based on the return type\n            return return_type.model_validate_json(cached)\n\n        # Call the function and cache its result\n        result = func(*args, **kwargs)\n        serialized_result = result.model_dump_json()\n        cache.set(key, serialized_result)\n\n        return result\n\n    return wrapper\n</code></pre> <p>Remember that you can change this code to support non-Pydantic models, or to use a different caching backend. More over, don't forget that this cache does not invalidate when the model changes, so you might want to encode the <code>Model.model_json_schema()</code> as part of the key.</p> <p>When to Use: Recommended for distributed systems where multiple processes need to access the cached data, or for applications requiring fast read/write access and handling complex data structures.</p> <pre><code>import redis\nimport functools\nimport inspect\nimport instructor\n\nfrom pydantic import BaseModel\nfrom openai import OpenAI\n\nclient = instructor.patch(OpenAI())\ncache = redis.Redis(\"localhost\")\n\n\ndef instructor_cache(func):\n    \"\"\"Cache a function that returns a Pydantic model\"\"\"\n    return_type = inspect.signature(func).return_annotation\n    if not issubclass(return_type, BaseModel):  # (1)\n        raise ValueError(\"The return type must be a Pydantic model\")\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = f\"{func.__name__}-{functools._make_key(args, kwargs, typed=False)}\"  # (2)\n        # Check if the result is already cached\n        if (cached := cache.get(key)) is not None:\n            # Deserialize from JSON based on the return type\n            return return_type.model_validate_json(cached)\n\n        # Call the function and cache its result\n        result = func(*args, **kwargs)\n        serialized_result = result.model_dump_json()\n        cache.set(key, serialized_result)\n\n        return result\n\n    return wrapper\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\n@instructor_cache\ndef extract(data) -&gt; UserDetail:\n    # Assuming client.chat.completions.create returns a UserDetail instance\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[\n            {\"role\": \"user\", \"content\": data},\n        ],\n    )\n</code></pre> <ol> <li>We only want to cache functions that return a Pydantic model to simplify serialization and deserialization logic</li> <li>We use functool's <code>_make_key</code> to generate a unique key based on the function's name and arguments. This is important because we want to cache the result of each function call separately.</li> </ol> <p>Benefits: Scalable for large-scale systems, supports fast in-memory data storage and retrieval, and is versatile for various data types.</p> <p>Looking carefully</p> <p>If you look carefully at the code above you'll notice that we're using the same <code>instructor_cache</code> decorator as before. The implementation is the same, but we're using a different caching backend!</p>","tags":["caching","functools","redis","diskcache","python"]},{"location":"blog/2023/11/26/python-caching/#conclusion","title":"Conclusion","text":"<p>Choosing the right caching strategy depends on your application's specific needs, such as the size and type of data, the need for persistence, and the system's architecture. Whether it's optimizing a function's performance in a small application or managing large datasets in a distributed environment, Python offers robust solutions to improve efficiency and reduce computational overhead.</p> <p>If you'd like to use this code, try to send it over to ChatGPT to understand it more, and to add additional features that might matter for you, for example, the cache isn't invalidated when your BaseModel changes, so you might want to encode the <code>Model.model_json_schema()</code> as part of the key.</p> <p>If you like the content check out our GitHub as give us a star and checkout the library.</p>","tags":["caching","functools","redis","diskcache","python"]},{"location":"blog/2023/11/05/chain-of-density/","title":"Smarter Summaries w/ Finetuning GPT-3.5 and Chain of Density","text":"<p>Discover how to distil an iterative method like Chain Of Density into a single finetuned model using Instructor</p> <p>In this article, we'll guide you through implementing the original Chain of Density method using Instructor, then show how to distile a GPT 3.5 model to match GPT-4's iterative summarization capabilities. Using these methods were able to decrease latency by 20x, reduce costs by 50x and maintain entity density.</p> <p>By the end you'll end up with a GPT 3.5 model, (fine-tuned using Instructor's great tooling), capable of producing summaries that rival the effectiveness of Chain of Density [Adams et al. (2023)]. As always, all code is readily available in our <code>examples/chain-of-density</code> folder in our repo for your reference.</p> Datasets and Colab Notebook <p>We've also uploaded all our generated data to Hugging Face here for you to use if you'd like to try reproducing these experiments. We've also added a Colab Instance for you to check our generated values.</p>","tags":["pydantic","validation","chain of density","finetuneing","gpt-3.5-turbo","distillation"]},{"location":"blog/2023/11/05/chain-of-density/#part-1-chain-of-density","title":"Part 1) Chain of Density","text":"<p>Summarizing extensive texts with AI can be challenging, often relying on inconsistent techniques. Their novel method, Chain Of Density prompting, enhances AI-based text summarization, outperforming human-generated summaries.</p> <p>Initially, an AI produces a summary, then refines it through multiple iterations, adding missing article entities. Each iteration adds new article entities to the summary, keeping length consistent, leading to an entity-dense, informative summary called Chain Of Density.</p> <p>First introduced in the paper - From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting. The team has found that this method is able to consistently beats similar summaries written by human annotators.</p> Implementation Details <p>Note that our implementation uses a validator to ensure that the rewritten summary has a minimum length rather than a prompt. We also perform just 3 and not 5 rounds of rewrites, resulting in a lower final entity density.</p>","tags":["pydantic","validation","chain of density","finetuneing","gpt-3.5-turbo","distillation"]},{"location":"blog/2023/11/05/chain-of-density/#original-prompt","title":"Original Prompt","text":"<p>We can break down the original process into smaller api calls. This allows us to introduce validation at each step to ensure that we're getting the results that we want.</p> Original Chain of Density Prompt <pre><code>Article: {{ARTICLE}}\n\nYou will generate increasingly concise, entity-dense summaries of the\nabove Article.\n\nRepeat the following 2 steps 5 times.\n\nStep 1. Identify 1-3 informative Entities (\";\" delimited) from the\nArticle which are missing from the previously generated summary.\nStep 2. Write a new, denser summary of identical length which covers\nevery entity and detail from the previous summary plus the Missing\nEntities.\n\nA Missing Entity is:\n- Relevant: to the main story.\n- Specific: descriptive yet concise (5 words or fewer).\n- Novel; not in the previous summary.\n- Faithful: present in the Article.\n- Anywhere: located anywhere in the Article.\n\nGuidelines:\n- The first summary should be long (4-5 sentences, -80 words) yet\nhighly non-specific, containing little information beyond the\nentities marked as missing. Use overly verbose language and fillers\n(e.g., \"this article discusses\") to reach -80 words.\n- Make every word count: re-write the previous summary to improve\nflow and make space for additional entities.\n- Make space with fusion, compression, and removal of uninformative\nphrases like \"the article discusses\"\n- The summaries should become highly dense and concise yet\nself-contained, e.g., easily understood without the Article.\n- Missing entities can appear anywhere in the new summary.\n- Never drop entities from the previous summary. If space cannot be\nmade, add fewer new entities.\n\nRemember, use the exact same number of words for each summary.\n\nAnswer in JSON. The JSON should be a list (length 5) of dictionaries\nwhose keys are \"Missing_Entities\" and \"Denser_Summary\"\n</code></pre> <p> </p> Improved process with Instructor","tags":["pydantic","validation","chain of density","finetuneing","gpt-3.5-turbo","distillation"]},{"location":"blog/2023/11/05/chain-of-density/#data-modelling","title":"Data Modelling","text":"<p>Before we begin modelling the data, let's make sure we install all of our dependencies</p> <pre><code>pip install instructor aiohttp rich\n</code></pre>","tags":["pydantic","validation","chain of density","finetuneing","gpt-3.5-turbo","distillation"]},{"location":"blog/2023/11/05/chain-of-density/#initial-summary","title":"Initial Summary","text":"<p>Let's start by walking through some of the data models that we'll be using as the <code>response_model</code> for our open ai function calls</p> <p>Firstly, we'll need a data model for the initial summary that we will be generating. We'll take the description of this class straight from the original prompt. It's important to note that these docstrings serve a purpose, they are directly used by the LLM when generating the outputs.</p> A quick note on Docstrings <p>Under the hood, Instructor parses the <code>response_model</code> that you give us into a function call for OpenAI to execute. This means that the final output will be closely linked to the Pydantic model you specify.</p> <p>For instance, this simple model that we later use in fine-tuning.</p> <pre><code>class GeneratedSummary(BaseModel):\n    \"\"\"\n    This represents a highly concise summary that includes as many entities as possible from the original source article.\n\n    An Entity is a real-world object that's assigned a name - for example, a person, country a product or a book title.\n\n    Guidelines\n    - Make every word count\n    - The new summary should be highly dense and concise yet self-contained, eg., easily understood without the Article.\n    - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\"\n    \"\"\"\n\n    summary: str = Field(\n        ...,\n        description=\"This represents the final summary generated that captures the meaning of the original article which is as concise as possible. \",\n    )\n</code></pre> <p>We eventually transform it into an OpenAI function call as seen below.</p> <pre><code>{\n\"functions\": [\n    {\n    \"name\": \"GeneratedSummary\",\n    \"description\": \"This represents a highly concise summary that includes as many entities as possible from the original source article.\\n\\nAn Entity is a real-world object that's assigned a name - for example, a person, country a product or a book title.\\n\\nGuidelines\\n- Make every word count\\n- The new summary should be highly dense and concise yet self-contained, eg., easily understood without the Article.\\n- Make space with fusion, compression, and removal of uninformative phrases like \\\"the article discusses\\\"\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n        \"summary\": {\n            \"description\": \"This represents the final summary generated that captures the meaning of the original article which is as concise as possible. \",\n            \"title\": \"Summary\",\n            \"type\": \"string\"\n        }\n        },\n        \"required\": [\n        \"summary\"\n        ]\n\n    }\n    }\n]\n}\n}\n</code></pre> <p>Therefore this means that the more elaborate and detailed your descriptions are, the better the outputs you will be able to get back. But we don't just stop there, since it's all Pydantic under the hood, you can validate and parse the resulting output to make sure it is exactly what you specify. It's all python all the way down.</p> <pre><code>class InitialSummary(BaseModel):\n    \"\"\"\n    This is an initial summary which should be long ( 4-5 sentences, ~80 words)\n    yet highly non-specific, containing little information beyond the entities marked as missing.\n    Use overly verbose languages and fillers (Eg. This article discusses) to reach ~80 words.\n    \"\"\"\n\n    summary: str = Field(\n        ...,\n        description=\"This is a summary of the article provided which is overly verbose and uses fillers. It should be roughly 80 words in length\",\n    )\n</code></pre>","tags":["pydantic","validation","chain of density","finetuneing","gpt-3.5-turbo","distillation"]},{"location":"blog/2023/11/05/chain-of-density/#rewritten-summary","title":"Rewritten Summary","text":"<p>We'll also need one additional class to help model the rewritten schema</p> <pre><code>class RewrittenSummary(BaseModel):\n    \"\"\"\n    This is a new, denser summary of identical length which covers every entity\n    and detail from the previous summary plus the Missing Entities.\n\n    Guidelines\n    - Make every word count : Rewrite the previous summary to improve flow and make space for additional entities\n    - Never drop entities from the previous summary. If space cannot be made, add fewer new entities.\n    - The new summary should be highly dense and concise yet self-contained, eg., easily understood without the Article.\n    - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\"\n    - Missing entities can appear anywhere in the new summary\n\n    An Entity is a real-world object that's assigned a name - for example, a person, country a product or a book title.\n    \"\"\"\n\n    summary: str = Field(\n        ...,\n        description=\"This is a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities. It should have the same length ( ~ 80 words ) as the previous summary and should be easily understood without the Article\",\n    )\n    absent: List[str] = Field(\n        ...,\n        default_factory=list,\n        description=\"this is a list of Entities found absent from the new summary that were present in the previous summary\",\n    )\n    missing: List[str] = Field(\n        default_factory=list,\n        description=\"This is a list of 1-3 informative Entities from the Article that are missing from the new summary which should be included in the next generated summary.\",\n    )\n</code></pre> <p>Using Pydantic Validators with Instructor</p> <p>For a more in-depth walkthrough on how to use <code>Pydantic</code> validators with the <code>Instructor</code> library, we recommend checking out our previous article on LLM validation - Good LLM Validation is just Good Validation</p> <p>Ideally, we'd like for <code>Missing</code> to have a length between 1 and 3, <code>Absent</code> to be an empty list and for our rewritten summaries to keep a minimum entity density. With <code>Instructor</code>, we can implement this logic using native <code>Pydantic</code> validators that are simply declared as part of the class itself.</p> <pre><code>import nltk\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\n\n@field_validator(\"summary\")\ndef min_length(cls, v: str):\n    tokens = nltk.word_tokenize(v) #(1)!\n    num_tokens = len(tokens)\n    if num_tokens &lt; 60:\n        raise ValueError(\n            \"The current summary is too short. Please make sure that you generate a new summary that is around 80 words long.\"\n        )\n    return v\n\n@field_validator(\"missing\")\ndef has_missing_entities(cls, missing_entities: List[str]):\n    if len(missing_entities) == 0:\n        raise ValueError(\n            \"You must identify 1-3 informative Entities from the Article which are missing from the previously generated summary to be used in a new summary\"\n        )\n    return missing_entities\n\n@field_validator(\"absent\")\ndef has_no_absent_entities(cls, absent_entities: List[str]):\n    absent_entity_string = \",\".join(absent_entities)\n    if len(absent_entities) &gt; 0:\n        print(f\"Detected absent entities of {absent_entity_string}\")\n        raise ValueError(\n            f\"Do not omit the following Entities {absent_entity_string} from the new summary\"\n        )\n    return absent_entities\n\n@field_validator(\"summary\")\ndef min_entity_density(cls, v: str):\n    tokens = nltk.word_tokenize(v)\n    num_tokens = len(tokens)\n\n    # Extract Entities\n    doc = nlp(v) #(2)!\n    num_entities = len(doc.ents)\n\n    density = num_entities / num_tokens\n    if density &lt; 0.08: #(3)!\n        raise ValueError(\n            f\"The summary of {v} has too few entities. Please regenerate a new summary with more new entities added to it. Remember that new entities can be added at any point of the summary.\"\n        )\n\n    return v\n</code></pre> <ol> <li> <p>Similar to the original paper, we utilize the <code>NLTK</code> word tokenizer to count the number of tokens within our generated sentences.     We aim for at least 60 tokens in our generated summary so that we don't lose information.</p> </li> <li> <p>We also use the spaCy library to calculate the entity density of the generated summary.</p> </li> <li> <p>We also implement a minimum entity density so that we stay within a given range. 0.08 is arbitrarily chosen in this case</p> </li> </ol>","tags":["pydantic","validation","chain of density","finetuneing","gpt-3.5-turbo","distillation"]},{"location":"blog/2023/11/05/chain-of-density/#putting-it-all-together","title":"Putting it all Together","text":"<p>Now that we have our models and the rough flow figured out, let's implement a function to summarize a piece of text using <code>Chain Of Density</code> summarization.</p> <pre><code>from openai import OpenAI\nimport instructor\n\nclient = instructor.patch(OpenAI()) #(1)!\n\ndef summarize_article(article: str, summary_steps: int = 3):\n    summary_chain = []\n    # We first generate an initial summary\n    summary: InitialSummary = client.chat.completions.create(  # (2)!\n        model=\"gpt-4-0613\",\n        response_model=InitialSummary,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"Write a summary about the article that is long (4-5 sentences) yet highly non-specific. Use overly, verbose language and fillers(eg.,'this article discusses') to reach ~80 words\",\n            },\n            {\"role\": \"user\", \"content\": f\"Here is the Article: {article}\"},\n            {\n                \"role\": \"user\",\n                \"content\": \"The generated summary should be about 80 words.\",\n            },\n        ],\n        max_retries=2,\n    )\n    prev_summary = None\n    summary_chain.append(summary.summary)\n    for i in range(summary_steps):\n        missing_entity_message = (\n            []\n            if prev_summary is None\n            else [\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"Please include these Missing Entities: {','.join(prev_summary.missing)}\",\n                },\n            ]\n        )\n        new_summary: RewrittenSummary = client.chat.completions.create( # (3)!\n            model=\"gpt-4-0613\",\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": \"\"\"\n                You are going to generate an increasingly concise,entity-dense summary of the following article.\n\n                Perform the following two tasks\n                - Identify 1-3 informative entities from the following article which is missing from the previous summary\n                - Write a new denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities\n\n                Guidelines\n                - Make every word count: re-write the previous summary to improve flow and make space for additional entities\n                - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\".\n                - The summaries should become highly dense and concise yet self-contained, e.g., easily understood without the Article.\n                - Missing entities can appear anywhere in the new summary\n                - Never drop entities from the previous summary. If space cannot be made, add fewer new entities.\n                \"\"\",\n                },\n                {\"role\": \"user\", \"content\": f\"Here is the Article: {article}\"},\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"Here is the previous summary: {summary_chain[-1]}\",\n                },\n                *missing_entity_message,\n            ],\n            max_retries=3, #(4)!\n            max_tokens=1000,\n            response_model=RewrittenSummary,\n        )\n        summary_chain.append(new_summary.summary)\n        prev_summary = new_summary\n\n    return summary_chain\n</code></pre> <ol> <li> <p>We need to apply a <code>patch</code> function on the <code>OpenAI</code> client for us to get all     of the benefits that <code>Instructor</code> provides. With a simple <code>patch</code>, we can get     automatic type coercion of our outputs and automatic retries for invalid outputs     out of the box!</p> </li> <li> <p>We first generate an initial summary. Note here that we explictly ask for a summary that has     80 words and is lengthy with overly verbose fillers in the system prompt</p> </li> <li> <p>We slightly modify the original system prompt used in the original paper to perform a rewrite of the summary.     Using <code>Instructor</code>, we also get validation of the generated output with our <code>field_validator</code>s that we defined above</p> </li> <li> <p>If you've chosen a value that is larger than 0.08, make sure to increase this value in case you need to do multiple rewrites</p> </li> </ol> <p>This summarization function yields a result which triples the number of entities while maintaining the same number of tokens. We can also see that stylistically, the summary is a lot more natural.</p> <p>First Iteration</p> <p>This article discusses the highly-anticipated boxing match between Manny Pacquiao and Floyd Mayweather. The article revolves around Manny Pacquiao's statements about his upcoming fight and his preparations for the same. A portion of the article provides details about the financial stipulations of the match and its significance in the sporting arena. Quotes from Pacquiao illustrating his determination and his battle strategy are highlighted. The tone of the article is largely centered around creating a build-up to the upcoming mega event.</p> <p>Final Iteration</p> <p>Manny Pacquiao, the Filipino boxer, anticipates the forthcoming May 2 showdown at the MGM Grand as the fight of his life, against the undefeated American Floyd Mayweather, in a $300m bout. Despite being seen as the underdog in this high-stakes Las Vegas match, Pacquiao is confident, promising a warrior's spirit and assuring the fans who have been awaiting this encounter for a decade, that it will indeed be the biggest sporting spectacle in history worthy of their anticipation</p>","tags":["pydantic","validation","chain of density","finetuneing","gpt-3.5-turbo","distillation"]},{"location":"blog/2023/11/05/chain-of-density/#part-2-fine-tuning","title":"Part 2) Fine-Tuning","text":"<p>In this section, we'll look into how to fine-tune a GPT 3.5 model so that it is able to perform at an equivalent level as a GPT-4 model. We'll then compare the performance of our model against that of <code>GPT-4</code> to see how it stacks up.</p>","tags":["pydantic","validation","chain of density","finetuneing","gpt-3.5-turbo","distillation"]},{"location":"blog/2023/11/05/chain-of-density/#creating-a-training-set","title":"Creating a Training Set","text":"<p>In order to prevent any contamination of data during testing, we randomly sampled 120 articles from the <code>griffin/chain-of-density</code> dataset and split these articles into a <code>train.csv</code> and a <code>test.csv</code> file which we uploaded to Hugging Face. Now, we just neeed to import the <code>Instructions</code> module from the <code>Instructor</code> package which allows you to generate a nicely formatted <code>.jsonl</code> file to be used for fine-tuning</p> <pre><code>from typing import List\nfrom chain_of_density import summarize_article #(1)!\nimport csv\nimport logging\nimport instructor\nfrom pydantic import BaseModel\nfrom openai import OpenAI\n\nclient = instructor.patch(OpenAI()) # (2)!\n\nlogging.basicConfig(level=logging.INFO) #(3)!\n\ninstructions = instructor.Instructions( #(4)!\n    name=\"Chain Of Density\",\n    finetune_format=\"messages\",\n    # log handler is used to save the data to a file\n    # you can imagine saving it to a database or other storage\n    # based on your needs!\n    log_handlers=[logging.FileHandler(\"generated.jsonl\")],\n    openai_client=client,\n)\n\nclass GeneratedSummary(BaseModel):\n    \"\"\"\n    This represents a highly concise summary that includes as many entities as possible from the original source article.\n\n    An Entity is a real-world object that's assigned a name - for example, a person, country a product or a book title.\n\n    Guidelines\n    - Make every word count\n    - The new summary should be highly dense and concise yet self-contained, eg., easily understood without the Article.\n    - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\"\n    \"\"\"\n\n    summary: str = Field(\n        ...,\n        description=\"This represents the final summary generated that captures the meaning of the original article which is as concise as possible. \",\n    )\n\n@instructions.distil #(4)!\ndef distil_summarization(text: str) -&gt; GeneratedSummary:\n    summary_chain: List[str] = summarize_article(text)\n    return GeneratedSummary(summary=summary_chain[-1]) #(5)!\n\nwith open(\"train.csv\", \"r\") as file:\n    reader = csv.reader(file)\n    next(reader)  # Skip the header\n    for article, summary in reader:\n        # Run Distillisation to generate the values\n        distil_summarization(article)\n</code></pre> <ol> <li> <p>In this example, we're using the summarize_article that we defined up above. We saved it in a local file called <code>chain_of_density.py</code>,     hence the import</p> </li> <li> <p>We patch the default OpenAI client so that we can use the Instructor library with it</p> </li> <li> <p>We also need to configure logging at the <code>INFO</code> level. This is very important, if this is not configured, your output will not be generated.</p> </li> <li> <p>We instantiate a <code>Instruction</code> object which will help us handle the conversion of our function calls into a valid <code>.jsonl</code> file. We also define     the name of the <code>.jsonl</code> file in the <code>log_handlers</code> parameter</p> </li> <li> <p>We add in an <code>instructions.distil</code> annotation so that we automatically capture the input and output of the function we'd like to     fine-tune our model to output</p> </li> <li> <p>We return a <code>Pydantic</code> object which matches the annotation that we use on our function. Note that we must specify a <code>Pydantic</code> object to     be returned when using the <code>instructions.distil</code> annotation</p> </li> </ol> <p>Rate Limiting</p> <p>We recommend running this script on a small subset of the dataset first to test you've got everything configured nicely. Don't forget to add in rate limiting error handling with <code>tenacity</code> and set the <code>OPENAI_API_KEY</code> shell environment variable before running any subsequent commands</p>","tags":["pydantic","validation","chain of density","finetuneing","gpt-3.5-turbo","distillation"]},{"location":"blog/2023/11/05/chain-of-density/#creating-fine-tuning-jobs","title":"Creating Fine-Tuning Jobs","text":"<p>Once we run this script, we'll have a new file called <code>generated.jsonl</code> in our local repository. Now all that's left is to run the command below to start fine-tuning your first model!</p> <pre><code>instructor jobs create-from-file generated.jsonl\n</code></pre> Finetuning Reference <p>Checking out our Finetuning CLI to learn about other hyperparameters that you can tune to improve your model's performance.</p> <p>Once the job is complete, all we need to do is to then change the annotation in the function call to <code>distil_summarization</code> in our original file above to start using our new model.</p> <pre><code>@instructions.distil(model='gpt-3.5-turbo:finetuned-123', mode=\"dispatch\")  # (1)!\ndef distil_summarization(text: str) -&gt; GeneratedSummary:\n    summary_chain: List[str] = summarize_article(text)\n    return GeneratedSummary(summary=summary_chain[-1])\n</code></pre> <ol> <li>Don't forget to replace this with your new model id. OpenAI identifies fine tuned models with an id of    ft:gpt-3.5-turbo-0613:personal:: under their Fine-tuning tab on their dashboard <p>With that, you've now got your own fine-tuned model ready to go and serve data in production. We've seen how Instructor can make your life easier, from fine-tuning to distillation.</p>","tags":["pydantic","validation","chain of density","finetuneing","gpt-3.5-turbo","distillation"]},{"location":"blog/2023/11/05/chain-of-density/#results-and-benchmarks","title":"Results and Benchmarks","text":"<p>We'll be comparing the following models in 3 ways using 20 articles that were not used for fine-tuning.</p> <ul> <li>Entity Density : This is entities per token, the higher the better for density.</li> <li>Latency : Time to last token generated in seconds</li> <li>Costs : Total cost to generate outputs - we break down the cost into training and inference costs for easy reference</li> </ul> <code>3.5 Finetuned (n)</code> <p>This is a GPT 3.5 model that we fine-tuned on <code>n</code> examples. Each model was finetuned for 4-5 epochs ( This was automatically decided by the OpenAI scheduler )</p> <code>GPT-4 (COD)</code> <p>This is a GPT4 model which we applied 3 rounds of Chain Of Density rewrites to generate a summary with using the methodology above</p> <code>GPT-3.5 (Vanilla)</code> <p>This is a GPT 3.5 model that we asked to generate entity-dense summaries which were concise. Summaries were generated in a single pass targetting about 80-90 tokens.</p> Model Mean Latency (s) Mean Entity Density 3.5 Finetuned (20) 2.1 0.15 3.5 Finetuned (50) 2.1 0.14 3.5 Finetuned (76) 2.1 0.14 GPT-3.5 (Vanilla) 16.8 0.12 GPT-4 (COD) 49.5 0.15 Finetuning Datasets <p>For our finetuned models, we did a few optimisations to raise the performance.</p> <p>We only included summaries that had a minimum density of 0.15 in the dataset, took the summary in the entire chain with the highest density as the final one, forced every regenerated summary to have a minimum density of 0.12 and regenerated summaries up to three times if they didn't meet the summaries. This is a much more expensive strategy and can cost up to 2.5x or more what we do in this tutorial</p> <p>This resulted in the total cost of $63.46 to generate just 75 examples due to the stringent requirements, translating to about $0.85 per generated summary example.</p> <p>Using the OpenAI Usage Dashboard, we can calculate the cost of generating 20 summaries as seen below.</p> Model Training Cost ($) Inference Cost ($) Tokens Used Total Cost ($) GPT-3.5 (Vanilla) - 0.20 51,162 0.2 3.5 Finetuned (20) 0.7 0.20 56,573 0.8 3.5 Finetuned (50) 1.4 0.17 49,057 1.3 3.5 Finetuned (76) 1.8 0.17 51,583 2.5 GPT-4 (COD) - 12.9 409,062 12.9 <p>Here, we can see that <code>GPT-4</code> has an approximate inference cost of <code>0.65</code> per summary while our finetuned models have an inference cost of <code>0.0091</code> per summary which is ~ <code>72x</code> cheaper.</p> <p>Interestingly, the model finetuned with the least examples seems to outperform the others. While the reason for this is unknown, a few potential reasons could be that either we didn't train for sufficient epochs ( We chose the default 5 epochs ) or that the models started learning to imitate other behaviour such as more abstract writing styles from the larger variety of samples, resulting in a decrease in entity density.</p>","tags":["pydantic","validation","chain of density","finetuneing","gpt-3.5-turbo","distillation"]},{"location":"blog/2023/11/05/chain-of-density/#conclusions","title":"Conclusions","text":"<p>Finetuning this iterative method was 20-40x faster while improving overall performance, resulting in massive efficiency gains by finetuning and distilling capabilities into specialized models.</p> <p>We've seen how <code>Instructor</code> can make your life easier, from data modeling to distillation and finetuning. If you enjoy the content or want to try out <code>instructor</code> check out the github and don't forget to give us a star!</p>","tags":["pydantic","validation","chain of density","finetuneing","gpt-3.5-turbo","distillation"]},{"location":"blog/2023/11/18/validate-citations/","title":"Verifying LLM Citations with Pydantic","text":"<p>Ensuring the accuracy of information is crucial. This blog post explores how Pydantic's powerful and flexible validators can enhance data accuracy through citation verification.</p> <p>We'll start with using a simple substring check to verify citations. Then we'll use <code>instructor</code> itself to power an LLM to verify citations and align answers with the given citations. Finally, we'll explore how we can use these techniques to generate a dataset of accurate responses.</p>","tags":["pydantic","validation","finetuneing","citations","hallucination"]},{"location":"blog/2023/11/18/validate-citations/#example-1-simple-substring-check","title":"Example 1: Simple Substring Check","text":"<p>In this example, we use the <code>Statements</code> class to verify if a given substring quote exists within a text chunk. If the substring is not found, an error is raised.</p>","tags":["pydantic","validation","finetuneing","citations","hallucination"]},{"location":"blog/2023/11/18/validate-citations/#code-example","title":"Code Example:","text":"<pre><code>from typing import List\nfrom openai import OpenAI\nfrom pydantic import BaseModel, ValidationInfo, field_validator\nimport instructor\n\nclient = instructor.patch(OpenAI())\n\n\nclass Statements(BaseModel):\n    body: str\n    substring_quote: str\n\n    @field_validator(\"substring_quote\")\n    @classmethod\n    def substring_quote_exists(cls, v: str, info: ValidationInfo):\n        context = info.context.get(\"text_chunks\", None)\n\n        for text_chunk in context.values():\n            if v in text_chunk:  # (1)\n                return v\n        raise ValueError(\"Could not find substring_quote `{v}` in contexts\")\n\n\nclass AnswerWithCitaton(BaseModel):\n    question: str\n    answer: List[Statements]\n</code></pre> <ol> <li>While we use a simple substring check in this example, we can use more complex techniques like regex or Levenshtein distance.</li> </ol> <p>Once the class is defined, we can use it to validate the context and raise an error if the substring is not found.</p> <pre><code>try:\n    AnswerWithCitaton.model_validate(\n        {\n            \"question\": \"What is the capital of France?\",\n            \"answer\": [\n                {\"body\": \"Paris\", \"substring_quote\": \"Paris is the capital of France\"},\n            ],\n        },\n        context={\n            \"text_chunks\": {\n                1: \"Jason is a pirate\",\n                2: \"Paris is not the capital of France\",\n                3: \"Irrelevant data\",\n            }\n        },\n    )\nexcept ValidationError as e:\n    print(e)\n</code></pre>","tags":["pydantic","validation","finetuneing","citations","hallucination"]},{"location":"blog/2023/11/18/validate-citations/#error-message-example","title":"Error Message Example:","text":"<pre><code>answer.0.substring_quote\n  Value error, Could not find substring_quote `Paris is the capital of France` in contexts [type=value_error, input_value='Paris is the capital of France', input_type=str]\n    For further information visit [https://errors.pydantic.dev/2.4/v/value_error](https://errors.pydantic.dev/2.4/v/value_error)\n</code></pre> <p>Pydantic raises a validation error when the <code>substring_quote</code> attribute does not exist in the context. This approach can be used to validate more complex data using techniques like regex or Levenshtein distance.</p>","tags":["pydantic","validation","finetuneing","citations","hallucination"]},{"location":"blog/2023/11/18/validate-citations/#example-2-using-llm-for-verification","title":"Example 2: Using LLM for Verification","text":"<p>This approach leverages OpenAI's LLM to validate citations. If the citation does not exist in the context, the LLM returns an error message.</p>","tags":["pydantic","validation","finetuneing","citations","hallucination"]},{"location":"blog/2023/11/18/validate-citations/#code-example_1","title":"Code Example:","text":"<pre><code>class Validation(BaseModel):\n    is_valid: bool\n    error_messages: Optional[str] = Field(None, description=\"Error messages if any\")\n\n\nclass Statements(BaseModel):\n    body: str\n    substring_quote: str\n\n    @model_validator(mode=\"after\")\n    def substring_quote_exists(self, info: ValidationInfo):\n        context = info.context.get(\"text_chunks\", None)\n\n        resp: Validation = client.chat.completions.create(\n            response_model=Validation,\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"Does the following citation exist in the following context?\\n\\nCitation: {self.substring_quote}\\n\\nContext: {context}\",\n                }\n            ],\n            model=\"gpt-3.5-turbo\",\n        )\n\n        if resp.is_valid:\n            return self\n\n        raise ValueError(resp.error_messages)\n\n\nclass AnswerWithCitaton(BaseModel):\n    question: str\n    answer: List[Statements]\n</code></pre> <p>Now when we use a correct citation, the LLM returns a valid response.</p> <pre><code>resp = AnswerWithCitaton.model_validate(\n    {\n        \"question\": \"What is the capital of France?\",\n        \"answer\": [\n            {\"body\": \"Paris\", \"substring_quote\": \"Paris is the capital of France\"},\n        ],\n    },\n    context={\n        \"text_chunks\": {\n            1: \"Jason is a pirate\",\n            2: \"Paris is the capital of France\",\n            3: \"Irrelevant data\",\n        }\n    },\n)\nprint(resp.model_dump_json(indent=2))\n</code></pre>","tags":["pydantic","validation","finetuneing","citations","hallucination"]},{"location":"blog/2023/11/18/validate-citations/#result","title":"Result:","text":"<pre><code>{\n  \"question\": \"What is the capital of France?\",\n  \"answer\": [\n    {\n      \"body\": \"Paris\",\n      \"substring_quote\": \"Paris is the capital of France\"\n    }\n  ]\n}\n</code></pre> <p>When we have citations that don't exist in the context, the LLM returns an error message.</p> <pre><code>try:\n    AnswerWithCitaton.model_validate(\n        {\n            \"question\": \"What is the capital of France?\",\n            \"answer\": [\n                {\"body\": \"Paris\", \"substring_quote\": \"Paris is the capital of France\"},\n            ],\n        },\n        context={\n            \"text_chunks\": {\n                1: \"Jason is a pirate\",\n                2: \"Paris is not the capital of France\",\n                3: \"Irrelevant data\",\n            }\n        },\n    )\nexcept ValidationError as e:\n    print(e)\n</code></pre>","tags":["pydantic","validation","finetuneing","citations","hallucination"]},{"location":"blog/2023/11/18/validate-citations/#error-message-example_1","title":"Error Message Example:","text":"<pre><code>1 validation error for AnswerWithCitaton\nanswer.0\n  Value error, Citation not found in context [type=value_error, input_value={'body': 'Paris', 'substr... the capital of France'}, input_type=dict]\n    For further information visit [https://errors.pydantic.dev/2.4/v/value_error](https://errors.pydantic.dev/2.4/v/value_error)\n</code></pre>","tags":["pydantic","validation","finetuneing","citations","hallucination"]},{"location":"blog/2023/11/18/validate-citations/#example-3-aligning-citations-and-answers","title":"Example 3: Aligning Citations and Answers","text":"<p>In this example, we ensure that the provided answers are aligned with the given citations and context. The LLM is used to verify the alignment.</p> <p>We use the same <code>Statements</code> model as above, but we add a new model for the answer that also verifies the alignment of citations.</p>","tags":["pydantic","validation","finetuneing","citations","hallucination"]},{"location":"blog/2023/11/18/validate-citations/#code-example_2","title":"Code Example:","text":"<pre><code>class AnswerWithCitaton(BaseModel):\n    question: str\n    answer: List[Statements]\n\n    @model_validator(mode=\"after\")\n    def validate_answer(self, info: ValidationInfo):\n        context = info.context.get(\"text_chunks\", None)\n\n        resp: Validation = client.chat.completions.create(\n            response_model=Validation,\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"Does the following answers match the question and the context?\\n\\nQuestion: {self.question}\\n\\nAnswer: {self.answer}\\n\\nContext: {context}\",\n                }\n            ],\n            model=\"gpt-3.5-turbo\",\n        )\n\n        if resp.is_valid:\n            return self\n\n        raise ValueError(resp.error_messages)\n</code></pre> <p>When we have a mismatch between the answer and the citation, the LLM returns an error message.</p> <pre><code>try:\n    AnswerWithCitaton.model_validate(\n        {\n            \"question\": \"What is the capital of France?\",\n            \"answer\": [\n                {\"body\": \"Texas\", \"substring_quote\": \"Paris is the capital of France\"},\n            ],\n        },\n        context={\n            \"text_chunks\": {\n                1: \"Jason is a pirate\",\n                2: \"Paris is the capital of France\",\n                3: \"Irrelevant data\",\n            }\n        },\n    )\nexcept ValidationError as e:\n    print(e)\n</code></pre>","tags":["pydantic","validation","finetuneing","citations","hallucination"]},{"location":"blog/2023/11/18/validate-citations/#error-message-example_2","title":"Error Message Example:","text":"<pre><code>1 validation error for AnswerWithCitaton\n  Value error, The answer does not match the question and context [type=value_error, input_value={'question': 'What is the...he capital of France'}]}, input_type=dict]\n    For further information visit [https://errors.pydantic.dev/2.4/v/value_error](https://errors.pydantic.dev/2.4/v/value_error)\n</code></pre>","tags":["pydantic","validation","finetuneing","citations","hallucination"]},{"location":"blog/2023/11/18/validate-citations/#conclusion","title":"Conclusion","text":"<p>These examples demonstrate the potential of using Pydantic and OpenAI to enhance data accuracy through citation verification. While the LLM-based approach may not be efficient for runtime operations, it has exciting implications for generating a dataset of accurate responses. By leveraging this method during data generation, we can fine-tune a model that excels in citation accuracy. Similar to our last post on finetuning a better summarizer.</p> <p>If you like the content check out our GitHub as give us a star and checkout the library.</p>","tags":["pydantic","validation","finetuneing","citations","hallucination"]},{"location":"blog/2023/10/17/enhancing-python-functions-with-instructor-a-guide-to-fine-tuning-and-distillation/","title":"Enhancing Python Functions with Instructor: A Guide to Fine-Tuning and Distillation","text":"","tags":["python","distillation","function calling","finetuning"]},{"location":"blog/2023/10/17/enhancing-python-functions-with-instructor-a-guide-to-fine-tuning-and-distillation/#introduction","title":"Introduction","text":"<p>Get ready to dive deep into the world of fine-tuning task specific language models with Python functions. We'll explore how the <code>instructor.instructions</code> streamlines this process, making the task you want to distil more efficient and powerful while preserving its original functionality and backwards compatibility.</p> <p>If you want to see the full example checkout examples/distillation</p>","tags":["python","distillation","function calling","finetuning"]},{"location":"blog/2023/10/17/enhancing-python-functions-with-instructor-a-guide-to-fine-tuning-and-distillation/#why-use-instructor","title":"Why use Instructor?","text":"<p>Imagine you're developing a backend service that uses a mix old and new school ML practises, it may involve pipelines with multiple function calls, validations, and data processing. Sounds cumbersome, right? That's where <code>Instructor</code> comes in. It simplifies complex procedures, making them more efficient and easier to manage by adding a decorator to your function that will automatically generate a dataset for fine-tuning and help you swap out the function implementation.</p>","tags":["python","distillation","function calling","finetuning"]},{"location":"blog/2023/10/17/enhancing-python-functions-with-instructor-a-guide-to-fine-tuning-and-distillation/#quick-start-how-to-use-instructors-distillation-feature","title":"Quick Start: How to Use Instructor's Distillation Feature","text":"<p>Before we dig into the nitty-gritty, let's look at how easy it is to use Instructor's distillation feature to use function calling finetuning to export the data to a JSONL file.</p> <pre><code>import logging\nimport random\nfrom pydantic import BaseModel\nfrom instructor import Instructions  # pip install instructor\n\n# Logging setup\nlogging.basicConfig(level=logging.INFO)\n\ninstructions = Instructions(\n    name=\"three_digit_multiply\",\n    finetune_format=\"messages\",\n    # log handler is used to save the data to a file\n    # you can imagine saving it to a database or other storage\n    # based on your needs!\n    log_handlers=[logging.FileHandler(\"math_finetunes.jsonl\")],\n)\n\n\nclass Multiply(BaseModel):\n    a: int\n    b: int\n    result: int\n\n\n# Define a function with distillation\n# The decorator will automatically generate a dataset for fine-tuning\n# They must return a pydantic model to leverage function calling\n@instructions.distil\ndef fn(a: int, b: int) -&gt; Multiply:\n    resp = a * b\n    return Multiply(a=a, b=b, result=resp)\n\n\n# Generate some data\nfor _ in range(10):\n    a = random.randint(100, 999)\n    b = random.randint(100, 999)\n    print(fn(a, b))\n    #&gt; a=873 b=234 result=204282\n    #&gt; a=902 b=203 result=183106\n    #&gt; a=962 b=284 result=273208\n    #&gt; a=491 b=739 result=362849\n    #&gt; a=193 b=400 result=77200\n    #&gt; a=300 b=448 result=134400\n    #&gt; a=952 b=528 result=502656\n    #&gt; a=574 b=797 result=457478\n    #&gt; a=482 b=204 result=98328\n    #&gt; a=781 b=278 result=217118\n</code></pre>","tags":["python","distillation","function calling","finetuning"]},{"location":"blog/2023/10/17/enhancing-python-functions-with-instructor-a-guide-to-fine-tuning-and-distillation/#the-intricacies-of-fine-tuning-language-models","title":"The Intricacies of Fine-tuning Language Models","text":"<p>Fine-tuning isn't just about writing a function like <code>def f(a, b): return a * b</code>. It requires detailed data preparation and logging. However, Instructor provides a built-in logging feature and structured outputs to simplify this.</p>","tags":["python","distillation","function calling","finetuning"]},{"location":"blog/2023/10/17/enhancing-python-functions-with-instructor-a-guide-to-fine-tuning-and-distillation/#why-instructor-and-distillation-are-game-changers","title":"Why Instructor and Distillation are Game Changers","text":"<p>The library offers two main benefits:</p> <ol> <li>Efficiency: Streamlines functions, distilling requirements into model weights and a few lines of code.</li> <li>Integration: Eases combining classical machine learning and language models by providing a simple interface that wraps existing functions.</li> </ol>","tags":["python","distillation","function calling","finetuning"]},{"location":"blog/2023/10/17/enhancing-python-functions-with-instructor-a-guide-to-fine-tuning-and-distillation/#role-of-instructor-in-simplifying-fine-tuning","title":"Role of Instructor in Simplifying Fine-Tuning","text":"<p>The <code>from instructor import Instructions</code> feature is a time saver. It auto-generates a fine-tuning dataset, making it a breeze to imitate a function's behavior.</p>","tags":["python","distillation","function calling","finetuning"]},{"location":"blog/2023/10/17/enhancing-python-functions-with-instructor-a-guide-to-fine-tuning-and-distillation/#logging-output-and-running-a-finetune","title":"Logging Output and Running a Finetune","text":"<p>Here's how the logging output would look:</p> <pre><code>{\n    \"messages\": [\n        {\"role\": \"system\", \"content\": 'Predict the results of this function: ...'},\n        {\"role\": \"user\", \"content\": 'Return fn(133, b=539)'},\n        {\n            \"role\": \"assistant\",\n            \"function_call\": {\n                \"name\": \"Multiply\",\n                \"arguments\": '{\"a\":133,\"b\":539,\"result\":89509}',\n            },\n        },\n    ],\n    \"functions\": [\n        {\"name\": \"Multiply\", \"description\": \"Correctly extracted `Multiply`...\"}\n    ],\n}\n</code></pre> <p>Run a finetune like this:</p> <p>Don't forget to set your OpenAI Key as an environment variable</p> <p>All of the <code>instructor jobs</code> commands assume you've set an environment variable of <code>OPENAI_API_KEY</code> in your shell. You can set this by running the command <code>export OPENAI_API_KEY=&lt;Insert API Key Here&gt;</code> in your shell</p> <pre><code>instructor jobs create-from-file math_finetunes.jsonl\n</code></pre>","tags":["python","distillation","function calling","finetuning"]},{"location":"blog/2023/10/17/enhancing-python-functions-with-instructor-a-guide-to-fine-tuning-and-distillation/#next-steps-and-future-plans","title":"Next Steps and Future Plans","text":"<p>Here's a sneak peek of what I'm planning:</p> <pre><code>from instructor import Instructions, patch\n\npatch()  # (1)!\n\n\nclass Multiply(BaseModel):\n    a: int\n    b: int\n    result: int\n\n\ninstructions = Instructions(\n    name=\"three_digit_multiply\",\n)\n\n\n@instructions.distil(model='gpt-3.5-turbo:finetuned-123', mode=\"dispatch\")  # (2)!\ndef fn(a: int, b: int) -&gt; Multiply:\n    resp = a + b\n    return Multiply(a=a, b=b, result=resp)\n</code></pre> <ol> <li> <p>Don't forget to run the <code>patch()</code> command that we provide with the <code>Instructor</code> package. This helps     automatically serialize the content back into the `Pydantic`` model that we're looking for.</p> </li> <li> <p>Don't forget to replace this with your new model id. OpenAI identifies fine tuned models with an id     of <code>ft:gpt-3.5-turbo-0613:personal::&lt;id&gt;</code> under their Fine-tuning tab on their dashboard</p> </li> </ol> <p>With this, you can swap the function implementation, making it backward compatible. You can even imagine using the different models for different tasks or validating and runnign evals by using the original function and comparing it to the distillation.</p>","tags":["python","distillation","function calling","finetuning"]},{"location":"blog/2023/10/17/enhancing-python-functions-with-instructor-a-guide-to-fine-tuning-and-distillation/#conclusion","title":"Conclusion","text":"<p>We've seen how <code>Instructor</code> can make your life easier, from fine-tuning to distillation. Now if you're thinking wow, I'd love a backend service to do this for continously, you're in luck! Please check out the survey at useinstructor.com and let us know who you are.</p> <p>If you enjoy the content or want to try out <code>instructor</code> please check out the github and give us a star!</p>","tags":["python","distillation","function calling","finetuning"]},{"location":"blog/2023/11/26/python-generators-and-llm-streaming/","title":"Generators and LLM Streaming","text":"<p>Latency is crucial, especially in eCommerce and newer chat applications like ChatGPT. Streaming is the solution that enables us to enhance the user experience without the need for faster response times.</p> <p>And what makes streaming possible? Generators!</p> <p>In this post, we're going to dive into the cool world of Python generators \u2014 these tools are more than just a coding syntax trick. We'll explore Python generators from the ground up and then delve into LLM streaming using the Instructor library.</p>","tags":["generators","streaming","python"]},{"location":"blog/2023/11/26/python-generators-and-llm-streaming/#python-generators-an-efficient-approach-to-iterables","title":"Python Generators: An Efficient Approach to Iterables","text":"<p>Generators in Python are a game-changer for handling large data sets and stream processing. They allow functions to yield values one at a time, pausing and resuming their state, which is a faster and more memory-efficient approach compared to traditional collections that store all elements in memory.</p>","tags":["generators","streaming","python"]},{"location":"blog/2023/11/26/python-generators-and-llm-streaming/#the-basics-yielding-values","title":"The Basics: Yielding Values","text":"<p>A generator function in Python uses the <code>yield</code> keyword. It yields values one at a time, allowing the function to pause and resume its state.</p> <pre><code>def count_to_3():\n    yield 1\n    yield 2\n    yield 3\n\n\nfor num in count_to_3():\n    print(num)\n    #&gt; 1\n    #&gt; 2\n    #&gt; 3\n</code></pre> <pre><code>1\n2\n3\n</code></pre>","tags":["generators","streaming","python"]},{"location":"blog/2023/11/26/python-generators-and-llm-streaming/#advantages-over-traditional-collections","title":"Advantages Over Traditional Collections","text":"<ul> <li>Lazy Evaluation &amp; reduced latency: The time to get the first element (or time-to-first-token in LLM land) from a generator is significantly lower. Generators only produce one value at a time, whereas accessing the first element of a collection will require that the whole collection be created first.</li> <li>Memory Efficiency: Only one item is in memory at a time.</li> <li>Maintain State: Automatically maintains state between executions.</li> </ul> <p>Let's see how much faster generators are and where they really shine:</p> <pre><code>import time\n\n\ndef expensive_func(x):\n    \"\"\"Simulate an expensive operation.\"\"\"\n    time.sleep(1)\n    return x**2\n\n\ndef calculate_time_for_first_result_with_list(func_input, func):\n    \"\"\"Calculate using a list comprehension and return the first result with its computation time.\"\"\"\n    start_perf = time.perf_counter()\n    result = [func(x) for x in func_input][0]\n    end_perf = time.perf_counter()\n    print(f\"Time for first result (list): {end_perf - start_perf:.2f} seconds\")\n    #&gt; Time for first result (list): 5.02 seconds\n    return result\n\n\ndef calculate_time_for_first_result_with_generator(func_input, func):\n    \"\"\"Calculate using a generator and return the first result with its computation time.\"\"\"\n    start_perf = time.perf_counter()\n    result = next(func(x) for x in func_input)\n    end_perf = time.perf_counter()\n    print(f\"Time for first result (generator): {end_perf - start_perf:.2f} seconds\")\n    #&gt; Time for first result (generator): 1.01 seconds\n    return result\n\n\n# Prepare inputs for the function\nnumbers = [1, 2, 3, 4, 5]\n\n# Benchmarking\nfirst_result_list = calculate_time_for_first_result_with_list(numbers, expensive_func)\nfirst_result_gen = calculate_time_for_first_result_with_generator(\n    numbers, expensive_func\n)\n</code></pre> <pre><code>Time for first result (list): 5.02 seconds\nTime for first result (generator): 1.01 seconds\n</code></pre> <p>The generator computes one expensive operation and returns the first result immediately, while the list comprehension computes the expensive operation for all elements in the list before returning the first result.</p>","tags":["generators","streaming","python"]},{"location":"blog/2023/11/26/python-generators-and-llm-streaming/#generator-expressions-a-shortcut","title":"Generator Expressions: A Shortcut","text":"<p>Python also allows creating generators in a single line of code, known as generator expressions. They are syntactically similar to list comprehensions but use parentheses.</p> <pre><code>squares = (x * x for x in range(10))\n</code></pre>","tags":["generators","streaming","python"]},{"location":"blog/2023/11/26/python-generators-and-llm-streaming/#use-cases-in-real-world-applications","title":"Use Cases in Real-World Applications","text":"<p>Generators shine in scenarios like reading large files, data streaming (eg. llm token streaming), and pipeline creation for data processing.</p>","tags":["generators","streaming","python"]},{"location":"blog/2023/11/26/python-generators-and-llm-streaming/#llm-streaming","title":"LLM Streaming","text":"<p>If you've used ChatGPT, you'll see that the tokens are streamed out one by one, instead of the full response being shown at the end (can you imagine waiting for the full response??). This is made possible by generators.</p> <p>Here's how a vanilla openai generator looks:</p> <pre><code>from openai import OpenAI\n\n# Set your OpenAI API key\nclient = OpenAI(\n    api_key=\"My API Key\",\n)\n\nresponse_generator = client.chat.completions.create(\n    model='gpt-3.5-turbo',\n    messages=[{'role': 'user', 'content': \"What are some good reasons to smile?\"}],\n    temperature=0,\n    stream=True,\n)\n\nfor chunk in response_generator:\n    print(chunk.choices[0].delta.content, end=\"\")\n</code></pre> <p>This is great, but what if we want to do some structured extraction on this stream? For instance, we might want to render frontend components based on product rankings that are streamed out by an LLM.</p> <p>Should we wait for the entire stream to finish before extracting &amp; validating the list of components or can we extract &amp; validate the components in real time as they are streamed?</p> <p>In e-commerce, every millisecond matters so the time-to-first-render can differentiate a successful and not-so-successful e commerce store (and i know how a failing e commerce store feels :/ ).</p> <p>Let's see how we can use Instructor to handle extraction from this real time stream!</p>","tags":["generators","streaming","python"]},{"location":"blog/2023/11/26/python-generators-and-llm-streaming/#e-commerce-product-ranking","title":"E-commerce Product Ranking","text":"","tags":["generators","streaming","python"]},{"location":"blog/2023/11/26/python-generators-and-llm-streaming/#scenario","title":"Scenario","text":"<p>Imagine an e-commerce platform where we have:</p> <p>\u2022 a customer profile: this includes a detailed history of purchases, browsing behavior, product ratings, preferences in various categories, search history, and even responses to previous recommendations. This extensive data is crucial for generating highly personalized and relevant product suggestions.</p> <p>\u2022 a list of candidate products: these could be some shortlisted products we think the customer would like.</p> <p>Our goal is to re-rerank these candidate products for the best conversion and we'll use an LLM!</p>","tags":["generators","streaming","python"]},{"location":"blog/2023/11/26/python-generators-and-llm-streaming/#stream-processing","title":"Stream Processing","text":"<p>User Data:</p> <p>Let's assume we have the following user profile:</p> <pre><code>profile_data = \"\"\"\nCustomer ID: 12345\nRecent Purchases: [Laptop, Wireless Headphones, Smart Watch]\nFrequently Browsed Categories: [Electronics, Books, Fitness Equipment]\nProduct Ratings: {Laptop: 5 stars, Wireless Headphones: 4 stars}\nRecent Search History: [best budget laptops 2023, latest sci-fi books, yoga mats]\nPreferred Brands: [Apple, AllBirds, Bench]\nResponses to Previous Recommendations: {Philips: Not Interested, Adidas: Not Interested}\nLoyalty Program Status: Gold Member\nAverage Monthly Spend: $500\nPreferred Shopping Times: Weekend Evenings\n...\n\"\"\"\n</code></pre> <p>We want to rank the following products for this user:</p> <pre><code>products = [\n    {\n        \"product_id\": 1,\n        \"product_name\": \"Apple MacBook Air (2023) - Latest model, high performance, portable\",\n    },\n    {\n        \"product_id\": 2,\n        \"product_name\": \"Sony WH-1000XM4 Wireless Headphones - Noise-canceling, long battery life\",\n    },\n    {\n        \"product_id\": 3,\n        \"product_name\": \"Apple Watch Series 7 - Advanced fitness tracking, seamless integration with Apple ecosystem\",\n    },\n    {\n        \"product_id\": 4,\n        \"product_name\": \"Kindle Oasis - Premium e-reader with adjustable warm light\",\n    },\n    {\n        \"product_id\": 5,\n        \"product_name\": \"AllBirds Wool Runners - Comfortable, eco-friendly sneakers\",\n    },\n    {\n        \"product_id\": 6,\n        \"product_name\": \"Manduka PRO Yoga Mat - High-quality, durable, eco-friendly\",\n    },\n    {\n        \"product_id\": 7,\n        \"product_name\": \"Bench Hooded Jacket - Stylish, durable, suitable for outdoor activities\",\n    },\n    {\n        \"product_id\": 8,\n        \"product_name\": \"GoPro HERO9 Black - 5K video, waterproof, for action photography\",\n    },\n    {\n        \"product_id\": 9,\n        \"product_name\": \"Nespresso Vertuo Next Coffee Machine - Quality coffee, easy to use, compact design\",\n    },\n    {\n        \"product_id\": 10,\n        \"product_name\": \"Project Hail Mary by Andy Weir - Latest sci-fi book from a renowned author\",\n    },\n]\n</code></pre> <p>Let's now define our models for structured extraction. Note: instructor will conveniently let us use <code>Iterable</code> to model an iterable of our class. In this case, once we define our product recommendation model, we can slap on <code>Iterable</code> to define what we ultimately want - a (ranked) list of product recommendations.</p> <pre><code>import instructor\nfrom openai import OpenAI\nfrom typing import Iterable\nfrom pydantic import BaseModel\n\nclient = instructor.patch(OpenAI(), mode=instructor.function_calls.Mode.JSON)\n\n\nclass ProductRecommendation(BaseModel):\n    product_id: str\n    product_name: str\n\n\nRecommendations = Iterable[ProductRecommendation]\n</code></pre> <p>Now let's use our instructor patch. Since we don't want to wait for all the tokens to finish, will set stream to <code>True</code> and process each product recommendation as it comes in:</p> <pre><code>prompt = (\n    f\"Based on the following user profile:\\n{profile_data}\\nRank the following products from most relevant to least relevant:\\n\"\n    + '\\n'.join(\n        f\"{product['product_id']} {product['product_name']}\" for product in products\n    )\n)\n\nstart_perf = time.perf_counter()\nrecommendations_stream = client.chat.completions.create(\n    model=\"gpt-3.5-turbo-1106\",\n    temperature=0.1,\n    response_model=Iterable[ProductRecommendation],\n    stream=True,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"Generate product recommendations based on the customer profile. Return in order of highest recommended first.\",\n        },\n        {\"role\": \"user\", \"content\": prompt},\n    ],\n)\nfor product in recommendations_stream:\n    print(product)\n    end_perf = time.perf_counter()\n    print(f\"Time for first result (generator): {end_perf - start_perf:.2f} seconds\")\n    break\n</code></pre> <pre><code>product_id='1' product_name='Apple MacBook Air (2023)'\nTime for first result (generator): 4.33 seconds\n</code></pre> <p><code>recommendations_stream</code> is a generator! It yields the extracted products as it's processing the stream in real-time. Now let's get the same response without streaming and see how they compare.</p> <pre><code>start_perf = time.perf_counter()\nrecommendations_list = client.chat.completions.create(\n    model=\"gpt-3.5-turbo-1106\",\n    temperature=0.1,\n    response_model=Iterable[ProductRecommendation],\n    stream=False,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"Generate product recommendations based on the customer profile. Return in order of highest recommended first.\",\n        },\n        {\"role\": \"user\", \"content\": prompt},\n    ],\n)\nprint(recommendations_list[0])\nend_perf = time.perf_counter()\nprint(f\"Time for first result (list): {end_perf - start_perf:.2f} seconds\")\n</code></pre> <pre><code>product_id='1' product_name='Apple MacBook Air (2023)'\nTime for first result (list): 8.63 seconds\n</code></pre> <p>Our web application now displays results faster. Even a 100ms improvement can lead to a 1% increase in revenue.</p>","tags":["generators","streaming","python"]},{"location":"blog/2023/11/26/python-generators-and-llm-streaming/#fastapi","title":"FastAPI","text":"<p>We can also take this and set up a streaming LLM API endpoint using FastAPI. Check out our docs on using FastAPI here!</p>","tags":["generators","streaming","python"]},{"location":"blog/2023/11/26/python-generators-and-llm-streaming/#key-takeaways","title":"Key Takeaways","text":"<p>To summarize, we looked at:</p> <p>\u2022 Generators in Python: A powerful feature that allows for efficient data handling with reduced latency</p> <p>\u2022 LLM Streaming: LLMs provide us generators to stream tokens and Instructor can let us validate and extract data from this stream. Real-time data validation ftw!</p> <p>Don't forget to check our GitHub for more resources and give us a star if you find the library helpful!</p> <p>If you have any questions or need further clarifications, feel free to reach out or dive into the Instructor library's documentation for more detailed information. Happy coding!</p>","tags":["generators","streaming","python"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/","title":"Bridging Language Models with Python using Instructor, Pydantic, and OpenAI's Function Calls","text":"<p>Language models have seen significant growth. Using them effectively often requires complex frameworks. This post discusses how Instructor simplifies this process using Pydantic.</p>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#the-problem-with-existing-llm-frameworks","title":"The Problem with Existing LLM Frameworks","text":"<p>Current frameworks for Language Learning Models (LLMs) have complex setups. Developers find it hard to control interactions with language models. Some frameworks require complex JSON Schema setups.</p>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#the-openai-function-calling-game-changer","title":"The OpenAI Function Calling Game-Changer","text":"<p>OpenAI's Function Calling feature provides a constrained interaction model. However, it has its own complexities, mostly around JSON Schema.</p>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#why-pydantic","title":"Why Pydantic?","text":"<p>Instructor uses Pydantic to simplify the interaction between the programmer and the language model.</p> <ul> <li>Widespread Adoption: Pydantic is a popular tool among Python developers.</li> <li>Simplicity: Pydantic allows model definition in Python.</li> <li>Framework Compatibility: Many Python frameworks already use Pydantic.</li> </ul> <pre><code>import pydantic\nimport instructor\nfrom openai import OpenAI\n\n# Enables the response_model\nclient = instructor.patch(OpenAI())\n\n\nclass UserDetail(pydantic.BaseModel):\n    name: str\n    age: int\n\n    def introduce(self):\n        return f\"Hello I'm {self.name} and I'm {self.age} years old\"\n\n\nuser: UserDetail = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserDetail,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"},\n    ],\n)\n</code></pre>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#simplifying-validation-flow-with-pydantic","title":"Simplifying Validation Flow with Pydantic","text":"<p>Pydantic validators simplify features like re-asking or self-critique. This makes these tasks less complex compared to other frameworks.</p> <pre><code>from typing_extensions import Annotated\nfrom pydantic import BaseModel, BeforeValidator\nfrom instructor import llm_validator\n\n\nclass QuestionAnswerNoEvil(BaseModel):\n    question: str\n    answer: Annotated[\n        str,\n        BeforeValidator(llm_validator(\"don't say objectionable things\")),\n    ]\n</code></pre>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#the-modular-approach","title":"The Modular Approach","text":"<p>Pydantic allows for modular output schemas. This leads to more organized code.</p>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#composition-of-schemas","title":"Composition of Schemas","text":"<pre><code>class UserDetails(BaseModel):\n    name: str\n    age: int\n\n\nclass UserWithAddress(UserDetails):\n    address: str\n</code></pre>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#defining-relationships","title":"Defining Relationships","text":"<pre><code>class UserDetail(BaseModel):\n    id: int\n    age: int\n    name: str\n    friends: List[int]\n\n\nclass UserRelationships(BaseModel):\n    users: List[UserDetail]\n</code></pre>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#using-enums","title":"Using Enums","text":"<pre><code>from enum import Enum, auto\n\n\nclass Role(Enum):\n    PRINCIPAL = auto()\n    TEACHER = auto()\n    STUDENT = auto()\n    OTHER = auto()\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Role\n</code></pre>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#flexible-schemas","title":"Flexible Schemas","text":"<pre><code>from typing import List\n\n\nclass Property(BaseModel):\n    key: str\n    value: str\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    properties: List[Property]\n</code></pre>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#chain-of-thought","title":"Chain of Thought","text":"<pre><code>class TimeRange(BaseModel):\n    chain_of_thought: str\n    start_time: int\n    end_time: int\n\n\nclass UserDetail(BaseModel):\n    id: int\n    age: int\n    name: str\n    work_time: TimeRange\n    leisure_time: TimeRange\n</code></pre>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#language-models-as-microservices","title":"Language Models as Microservices","text":"<p>The architecture resembles FastAPI. Most code can be written as Python functions that use Pydantic objects. This eliminates the need for prompt chains.</p>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#fastapi-stub","title":"FastAPI Stub","text":"<pre><code>import fastapi\nfrom pydantic import BaseModel\n\nclass UserDetails(BaseModel):\n    name: str\n    age: int\n\napp = fastapi.FastAPI()\n\n@app.get(\"/user/{user_id}\", response_model=UserDetails)\nasync def get_user(user_id: int) -&gt; UserDetails:\n    return ...\n</code></pre>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#using-instructor-as-a-function","title":"Using Instructor as a Function","text":"<pre><code>def extract_user(str) -&gt; UserDetails:\n    return client.chat.completions(\n           response_model=UserDetails,\n           messages=[]\n    )\n</code></pre>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#response-modeling","title":"Response Modeling","text":"<pre><code>class MaybeUser(BaseModel):\n    result: Optional[UserDetail]\n    error: bool\n    message: Optional[str]\n</code></pre>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#conclusion","title":"Conclusion","text":"<p>Instructor, with Pydantic, simplifies interaction with language models. It is usable for both experienced and new developers.</p> <p>If you enjoy the content or want to try out <code>instructor</code> please check out the github and give us a star!</p>","tags":["Introduction"]},{"location":"blog/2023/11/13/learn-async/","title":"Introduction to Batch Processing using <code>asyncio</code> and <code>Instructor</code>","text":"<p>Today, I will introduce you to various approaches for using asyncio in Python. We will apply this to batch process data using <code>instructor</code> and learn how to use <code>asyncio.gather</code> and <code>asyncio.as_completed</code> for concurrent data processing. Additionally, we will explore how to limit the number of concurrent requests to a server using <code>asyncio.Semaphore</code>.</p> <p>Github Example</p> <p>If you want to run the code examples in this article, you can find them on jxnl/instructor</p> <p>We will start by defining an <code>async</code> function that calls <code>openai</code> to extract data, and then examine four different ways to execute it. We will discuss the pros and cons of each approach and analyze the results of running them on a small batch.</p>","tags":["python","batch","asyncio","async","async/await"]},{"location":"blog/2023/11/13/learn-async/#understanding-asyncio","title":"Understanding <code>asyncio</code>","text":"<p><code>asyncio</code> is a Python library that enables writing concurrent code using the async/await syntax. It is particularly useful for IO-bound and structured network code. If you are familiar with OpenAI's SDK, you might have encountered two classes: <code>OpenAI()</code> and <code>AsyncOpenAI()</code>. Today, we will be using the <code>AsyncOpenAI()</code> class, which processes data asynchronously.</p> <p>By utilizing these tools in web applications or batch processing, we can significantly improve performance by handling multiple requests concurrently instead of sequentially.</p>","tags":["python","batch","asyncio","async","async/await"]},{"location":"blog/2023/11/13/learn-async/#understanding-async-and-await","title":"Understanding <code>async</code> and <code>await</code>","text":"<p>We will be using the <code>async</code> and <code>await</code> keywords to define asynchronous functions. The <code>async</code> keyword is used to define a function that returns a coroutine object. The <code>await</code> keyword is used to wait for the result of a coroutine object.</p> <p>If you want to understand the deeper details of <code>asyncio</code>, I recommend reading this article by Real Python.</p>","tags":["python","batch","asyncio","async","async/await"]},{"location":"blog/2023/11/13/learn-async/#understanding-gather-vs-as_completed","title":"Understanding <code>gather</code> vs <code>as_completed</code>","text":"<p>In this post we'll show two ways to run tasks concurrently: <code>asyncio.gather</code> and <code>asyncio.as_completed</code>. The <code>gather</code> method is used to run multiple tasks concurrently and return the results as a <code>list</code>. The <code>as_completed</code> returns a <code>iterable</code> is used to run multiple tasks concurrently and return the results as they complete. Another great resource on the differences between the two can be found here.</p>","tags":["python","batch","asyncio","async","async/await"]},{"location":"blog/2023/11/13/learn-async/#example-batch-processing","title":"Example: Batch Processing","text":"<p>In this example, we will demonstrate how to use <code>asyncio</code> for batch processing tasks, specifically for extracting and processing data concurrently. The script will extract data from a list of texts and process it concurrently using <code>asyncio</code>.</p> <pre><code>import instructor\nfrom pydantic import BaseModel\nfrom openai import AsyncOpenAI\n\n# Enables `response_model` in `create` method\nclient = instructor.apatch(AsyncOpenAI())  # (1)!\n\n\nclass Person(BaseModel):\n    name: str\n    age: int\n\n\nasync def extract_person(text: str) -&gt; Person:\n    return await client.chat.completions.create(  # (2)!\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\"role\": \"user\", \"content\": text},\n        ],\n        response_model=Person,\n    )\n</code></pre> <ol> <li>We use <code>instructor.apatch</code> to patch the <code>create</code> method of <code>AsyncOpenAI</code> to accept a <code>response_model</code> argument. This is because the <code>create</code> method of <code>AsyncOpenAI</code> does not accept a <code>response_model</code> argument without this patch.</li> <li>We use <code>await</code> here to wait for the response from the server before we return the result. This is because <code>create</code> returns a coroutine object, not the result of the coroutine.</li> </ol> <p>Notice that now there are <code>async</code> and <code>await</code> keywords in the function definition. This is because we're using the <code>asyncio</code> library to run the function concurrently. Now let's define a batch of texts to process.</p> <pre><code>dataset = [\n    \"My name is John and I am 20 years old\",\n    \"My name is Mary and I am 21 years old\",\n    \"My name is Bob and I am 22 years old\",\n    \"My name is Alice and I am 23 years old\",\n    \"My name is Jane and I am 24 years old\",\n    \"My name is Joe and I am 25 years old\",\n    \"My name is Jill and I am 26 years old\",\n]\n</code></pre>","tags":["python","batch","asyncio","async","async/await"]},{"location":"blog/2023/11/13/learn-async/#for-loop-running-tasks-sequentially","title":"<code>for loop</code>: Running tasks sequentially.","text":"<pre><code>persons = []\nfor text in dataset:\n    person = await extract_person(text)\n    persons.append(person)\n</code></pre> <p>Even though there is an <code>await</code> keyword, we still have to wait for each task to finish before starting the next one. This is because we're using a <code>for</code> loop to iterate over the dataset. This method, which uses a <code>for</code> loop, will be the slowest among the four methods discussed today.</p>","tags":["python","batch","asyncio","async","async/await"]},{"location":"blog/2023/11/13/learn-async/#asynciogather-running-tasks-concurrently","title":"<code>asyncio.gather</code>: Running tasks concurrently.","text":"<pre><code>async def gather():\n    tasks_get_persons = [extract_person(text) for text in dataset]\n    all_persons = await asyncio.gather(*tasks_get_persons)  # (1)!\n</code></pre> <ol> <li>We use <code>await</code> here to wait for all the tasks to finish before assigning the result to <code>all_persons</code>. This is because <code>asyncio.gather</code> returns a coroutine object, not the result of the coroutine. Alternatively, we can use <code>asyncio.as_completed</code> to achieve the same result.</li> </ol> <p>Using <code>asyncio.gather</code> allows us to return all the results at once. It is an effective way to speed up our code, but it's not the only way. Particularly, if we have a large dataset, we might not want to wait for everything to finish before starting to process the results. This is where <code>asyncio.as_completed</code> comes into play.</p>","tags":["python","batch","asyncio","async","async/await"]},{"location":"blog/2023/11/13/learn-async/#asyncioas_completed-handling-tasks-as-they-complete","title":"<code>asyncio.as_completed</code>: Handling tasks as they complete.","text":"<pre><code>async def as_completed():\n    all_persons = []\n    tasks_get_persons = [extract_person(text) for text in dataset]\n    for person in asyncio.as_completed(tasks_get_persons):\n        all_persons.append(await person)  # (1)!\n</code></pre> <ol> <li>We use <code>await</code> here to wait for each task to complete before appending it to the list. This is because <code>as_completed</code> returns a coroutine object, not the result of the coroutine. Alternatively, we can use <code>asyncio.gather</code> to achieve the same result.</li> </ol> <p>This method is a great way to handle large datasets. We can start processing the results as they come in, especially if we are streaming data back to a client.</p> <p>However, these methods aim to complete as many tasks as possible as quickly as possible. This can be problematic if we want to be considerate to the server we're making requests to. This is where rate limiting comes into play. While there are libraries available to assist with rate limiting, for our initial defense, we will use a semaphore to limit the number of concurrent requests we make.</p> <p>Ordering of results</p> <p>It is important to note that the order of the results will not be the same as the order of the dataset. This is because the tasks are completed in the order they finish, not the order they were started. If you need to preserve the order of the results, you can use <code>asyncio.gather</code> instead.</p>","tags":["python","batch","asyncio","async","async/await"]},{"location":"blog/2023/11/13/learn-async/#rate-limited-gather-using-semaphores-to-limit-concurrency","title":"Rate-Limited Gather: Using semaphores to limit concurrency.","text":"<pre><code>sem = asyncio.Semaphore(2)\n\n\nasync def rate_limited_extract_person(text: str, sem: Semaphore) -&gt; Person:\n    async with sem:  # (1)!\n        return await extract_person(text)\n\n\nasync def rate_limited_gather(sem: Semaphore):\n    tasks_get_persons = [rate_limited_extract_person(text, sem) for text in dataset]\n    resp = await asyncio.gather(*tasks_get_persons)\n</code></pre> <ol> <li>We use a semaphore to limit the number of concurrent requests to 2. This approach strikes a balance between speed and being considerate to the server we're making requests to.</li> </ol>","tags":["python","batch","asyncio","async","async/await"]},{"location":"blog/2023/11/13/learn-async/#rate-limited-as-completed-using-semaphores-to-limit-concurrency","title":"Rate-Limited As Completed: Using semaphores to limit concurrency.","text":"<pre><code>sem = asyncio.Semaphore(2)\n\n\nasync def rate_limited_extract_person(text: str, sem: Semaphore) -&gt; Person:\n    async with sem:  # (1)!\n        return await extract_person(text)\n\n\nasync def rate_limited_as_completed(sem: Semaphore):\n    all_persons = []\n    tasks_get_persons = [rate_limited_extract_person(text, sem) for text in dataset]\n    for person in asyncio.as_completed(tasks_get_persons):\n        all_persons.append(await person)  # (2)!\n</code></pre> <ol> <li> <p>We use a semaphore to limit the number of concurrent requests to 2. This approach strikes a balance between speed and being considerate to the server we're making requests to.</p> </li> <li> <p>We use <code>await</code> here to wait for each task to complete before appending it to the list. This is because <code>as_completed</code> returns a coroutine object, not the result of the coroutine. Alternatively, we can use <code>asyncio.gather</code> to achieve the same result.</p> </li> </ol> <p>Now that we have seen the code, let's examine the results of processing 7 texts. As the prompts become longer or if we use GPT-4, the differences between these methods will become more pronounced.</p> <p>Other Options</p> <p>It is important to also note that here we are using a <code>semaphore</code> to limit the number of concurrent requests. However, there are other ways to limit concurrency especially since we have rate limit information from the <code>openai</code> request. You can imagine using a library like <code>ratelimit</code> to limit the number of requests per second. OR catching rate limit exceptions and using <code>tenacity</code> to retry the request after a certain amount of time.</p> <ul> <li>tenacity</li> <li>aiolimiter</li> </ul>","tags":["python","batch","asyncio","async","async/await"]},{"location":"blog/2023/11/13/learn-async/#results","title":"Results","text":"<p>As you can see, the <code>for</code> loop is the slowest, while <code>asyncio.as_completed</code> and <code>asyncio.gather</code> are the fastest without any rate limiting.</p> Method Execution Time Rate Limited (Semaphore) For Loop 6.17 seconds Asyncio.gather 0.85 seconds Asyncio.as_completed 0.95 seconds Asyncio.gather 3.04 seconds 2 Asyncio.as_completed 3.26 seconds 2","tags":["python","batch","asyncio","async","async/await"]},{"location":"blog/2023/11/13/learn-async/#practical-implications-of-batch-processing","title":"Practical implications of batch processing","text":"<p>The choice of approach depends on the task's nature and the desired balance between speed and resource utilization.</p> <p>Here are some guidelines to consider:</p> <ul> <li>Use <code>asyncio.gather</code> for handling multiple independent tasks quickly.</li> <li>Apply <code>asyncio.as_completed</code> for large datasets to process tasks as they complete.</li> <li>Implement rate-limiting to avoid overwhelming servers or API endpoints.</li> </ul> <p>If you find the content helpful or want to try out <code>Instructor</code>, please visit our GitHub page and give us a star!</p>","tags":["python","batch","asyncio","async","async/await"]},{"location":"blog/2024/02/08/ollama/","title":"Structured Outputs with Ollama","text":"<p>Open-source LLMS are gaining popularity, and the release of Ollama's OpenAI compatibility later it has made it possible to obtain structured outputs using JSON schema.</p> <p>By the end of this blog post, you will learn how to effectively utilize instructor with ollama. But before we proceed, let's first explore the concept of patching.</p>","tags":["patching","open source"]},{"location":"blog/2024/02/08/ollama/#patching","title":"Patching","text":"<p>Instructor's patch enhances a openai api it with the following features:</p> <ul> <li><code>response_model</code> in <code>create</code> calls that returns a pydantic model</li> <li><code>max_retries</code> in <code>create</code> calls that retries the call if it fails by using a backoff strategy</li> </ul> <p>Learn More</p> <p>To learn more, please refer to the docs. To understand the benefits of using Pydantic with Instructor, visit the tips and tricks section of the why use Pydantic page.</p>","tags":["patching","open source"]},{"location":"blog/2024/02/08/ollama/#ollama","title":"Ollama","text":"<p>Start by downloading Ollama, and then pull a model such as Llama 2 or Mistral.</p> <p>Make sure you update your <code>ollama</code> to the latest version!</p> <pre><code>ollama pull llama2\n</code></pre> <pre><code>from openai import OpenAI\nfrom pydantic import BaseModel, Field\nfrom typing import List\n\nimport instructor\n\n\nclass Character(BaseModel):\n    name: str\n    age: int\n    fact: List[str] = Field(..., description=\"A list of facts about the character\")\n\n\n# enables `response_model` in create call\nclient = instructor.patch(\n    OpenAI(\n        base_url=\"http://localhost:11434/v1\",\n        api_key=\"ollama\",  # required, but unused\n    ),\n    mode=instructor.Mode.JSON,\n)\n\nresp = client.chat.completions.create(\n    model=\"llama2\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Tell me about the Harry Potter\",\n        }\n    ],\n    response_model=Character,\n)\nprint(resp.model_dump_json(indent=2))\n\"\"\"\n{\n  \"name\": \"Harry James Potter\",\n  \"age\": 37,\n  \"fact\": [\n    \"He is the chosen one.\",\n    \"He has a lightning-shaped scar on his forehead.\",\n    \"He is the son of James and Lily Potter.\",\n    \"He attended Hogwarts School of Witchcraft and Wizardry.\",\n    \"He is a skilled wizard and sorcerer.\",\n    \"He fought against Lord Voldemort and his followers.\",\n    \"He has a pet owl named Snowy.\"\n  ]\n}\n\"\"\"\n</code></pre>","tags":["patching","open source"]},{"location":"blog/2023/09/17/rag-is-more-than-just-embedding-search/","title":"RAG is more than just embedding search","text":"<p>With the advent of large language models (LLM), retrieval augmented generation (RAG) has become a hot topic. However throughout the past year of helping startups integrate LLMs into their stack I've noticed that the pattern of taking user queries, embedding them, and directly searching a vector store is effectively demoware.</p> <p>What is RAG?</p> <p>Retrieval augmented generation (RAG) is a technique that uses an LLM to generate responses, but uses a search backend to augment the generation. In the past year using text embeddings with a vector databases has been the most popular approach I've seen being socialized.</p> <p> </p> Simple RAG that embedded the user query and makes a search. <p>So let's kick things off by examining what I like to call the 'Dumb' RAG Model\u2014a basic setup that's more common than you'd think.</p>","tags":["RAG","Embeddings","Query Understanding","Search Systems","Personal Assistant"]},{"location":"blog/2023/09/17/rag-is-more-than-just-embedding-search/#the-dumb-rag-model","title":"The 'Dumb' RAG Model","text":"<p>When you ask a question like, \"what is the capital of France?\" The RAG 'dumb' model embeds the query and searches in some unopinionated search endpoint. Limited to a single method API like <code>search(query: str) -&gt; List[str]</code>. This is fine for simple queries, since you'd expect words like 'paris is the capital of france' to be in the top results of say, your wikipedia embeddings.</p>","tags":["RAG","Embeddings","Query Understanding","Search Systems","Personal Assistant"]},{"location":"blog/2023/09/17/rag-is-more-than-just-embedding-search/#why-is-this-a-problem","title":"Why is this a problem?","text":"<ul> <li> <p>Query-Document Mismatch: This model assumes that query embedding and the content embedding are similar in the embedding space, which is not always true based on the text you're trying to search over. Only using queries that are semantically similar to the content is a huge limitation!</p> </li> <li> <p>Monolithic Search Backend: Assumes a single search backend, which is not always the case. You may have multiple search backends, each with their own API, and you want to route the query to vector stores, search clients, sql databases, and more.</p> </li> <li> <p>Limitation of text search: Restricts complex queries to a single string (<code>{query: str}</code>), sacrificing expressiveness, in using keywords, filters, and other advanced features. For example, asking <code>what problems did we fix last week</code> cannot be answered by a simple text search since documents that contain <code>problem, last week</code> are going to be present at every week.</p> </li> <li> <p>Limited ability to plan: Assumes that the query is the only input to the search backend, but you may want to use other information to improve the search, like the user's location, or the time of day using the context to rewrite the query. For example, if you present the language model of more context it is able to plan a suite of queries to execute to return the best results.</p> </li> </ul> <p>Now let's dive into how we can make it smarter with query understanding. This is where things get interesting.</p>","tags":["RAG","Embeddings","Query Understanding","Search Systems","Personal Assistant"]},{"location":"blog/2023/09/17/rag-is-more-than-just-embedding-search/#improving-the-rag-model-with-query-understanding","title":"Improving the RAG Model with Query Understanding","text":"<p>Shoutouts</p> <p>Much of this work has been inspired by / done in collab with a few of my clients at new.computer, Metaphor Systems, and Naro, go check them out!</p> <p>Ultimately what you want to deploy is a system that understands how to take the query and rewrite it to improve precision and recall.</p> <p> </p> Query Understanding system routes to multiple search backends. <p>Not convinced? Let's move from theory to practice with a real-world example. First up, Metaphor Systems.</p>","tags":["RAG","Embeddings","Query Understanding","Search Systems","Personal Assistant"]},{"location":"blog/2023/09/17/rag-is-more-than-just-embedding-search/#whats-instructor","title":"Whats instructor?","text":"<p>Instructor uses Pydantic to simplify the interaction between the programmer and language models via the function calling API.</p> <ul> <li>Widespread Adoption: Pydantic is a popular tool among Python developers.</li> <li>Simplicity: Pydantic allows model definition in Python.</li> <li>Framework Compatibility: Many Python frameworks already use Pydantic.</li> </ul>","tags":["RAG","Embeddings","Query Understanding","Search Systems","Personal Assistant"]},{"location":"blog/2023/09/17/rag-is-more-than-just-embedding-search/#case-study-1-metaphor-systems","title":"Case Study 1: Metaphor Systems","text":"<p>Take Metaphor Systems, which turns natural language queries into their custom search-optimized query. If you take a look web UI you'll notice that they have an auto-prompt option, which uses function calls to furthur optimize your query using a language model, and turns it into a fully specified metaphor systems query.</p> <p></p> Metaphor Systems UI <p>If we peek under the hood, we can see that the query is actually a complex object, with a date range, and a list of domains to search in. It's actually more complex than this but this is a good start. We can model this structured output in Pydantic using the instructor library</p> <pre><code>class DateRange(BaseModel):\n    start: datetime.date\n    end: datetime.date\n\n\nclass MetaphorQuery(BaseModel):\n    rewritten_query: str\n    published_daterange: DateRange\n    domains_allow_list: List[str]\n\n    async def execute():\n        return await metaphor.search(...)\n</code></pre> <p>Note how we model a rewritten query, range of published dates, and a list of domains to search in. This powerful pattern allows the user query to be restructured for better performance without the user having to know the details of how the search backend works.</p> <pre><code>import instructor\nfrom openai import OpenAI\n\n# Enables response_model in the openai client\nclient = instructor.patch(OpenAI())\n\nquery = client.chat.completions.create(\n    model=\"gpt-4\",\n    response_model=MetaphorQuery,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You're a query understanding system for the Metafor Systems search engine. Here are some tips: ...\",\n        },\n        {\"role\": \"user\", \"content\": \"What are some recent developments in AI?\"},\n    ],\n)\n</code></pre> <p>Example Output</p> <pre><code>{\n  \"rewritten_query\": \"novel developments advancements ai artificial intelligence machine learning\",\n  \"published_daterange\": {\n    \"start\": \"2023-09-17\",\n    \"end\": \"2021-06-17\"\n  },\n  \"domains_allow_list\": [\"arxiv.org\"]\n}\n</code></pre> <p>This isn't just about adding some date ranges. It's about nuanced, tailored searches, that are deeply integrated with the backend. Metaphor Systems has a whole suite of other filters and options that you can use to build a powerful search query. They can even use some chain of thought prompting to improve how they use some of these advanced features.</p> <pre><code>class DateRange(BaseModel):\n    start: datetime.date\n    end: datetime.date\n    chain_of_thought: str = Field(\n        None,\n        description=\"Think step by step to plan what is the best time range to search in\",\n    )\n</code></pre> <p>Now, let's see how this approach can help model an agent like personal assistant.</p>","tags":["RAG","Embeddings","Query Understanding","Search Systems","Personal Assistant"]},{"location":"blog/2023/09/17/rag-is-more-than-just-embedding-search/#case-study-2-personal-assistant","title":"Case Study 2: Personal Assistant","text":"<p>Another great example of this multiple dispatch pattern is a personal assistant. You might ask, \"What do I have today?\", from a vague query you might want events, emails, reminders etc. That data will likely exist in multiple backends, but what you want is one unified summary of results. Here you can't assume that text of those documents are all embedded in a search backend. There might be a calendar client, email client, across personal and profession accounts.</p> <pre><code>class ClientSource(enum.Enum):\n    GMAIL = \"gmail\"\n    CALENDAR = \"calendar\"\n\n\nclass SearchClient(BaseModel):\n    query: str\n    keywords: List[str]\n    email: str\n    source: ClientSource\n    start_date: datetime.date\n    end_date: datetime.date\n\n    async def execute(self) -&gt; str:\n        if self.source == ClientSource.GMAIL:\n            ...\n        elif self.source == ClientSource.CALENDAR:\n            ...\n\n\nclass Retrieval(BaseModel):\n    queries: List[SearchClient]\n\n    async def execute(self) -&gt; str:\n        return await asyncio.gather(*[query.execute() for query in self.queries])\n</code></pre> <p>Now we can call this with a simple query like \"What do I have today?\" and it will try to async dispatch to the correct backend. It's still important to prompt the language model well, but we'll leave that for another day.</p> <pre><code>import instructor\nfrom openai import OpenAI\n\n# Enables response_model in the openai client\nclient = instructor.patch(OpenAI())\n\nretrieval = client.chat.completions.create(\n    model=\"gpt-4\",\n    response_model=Retrieval,\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are Jason's personal assistant.\"},\n        {\"role\": \"user\", \"content\": \"What do I have today?\"},\n    ],\n)\n</code></pre> <p>Example Output</p> <pre><code>{\n    \"queries\": [\n        {\n            \"query\": None,\n            \"keywords\": None,\n            \"email\": \"jason@example.com\",\n            \"source\": \"gmail\",\n            \"start_date\": \"2023-09-17\",\n            \"end_date\": None\n        },\n        {\n            \"query\": None,\n            \"keywords\": [\"meeting\", \"call\", \"zoom\"]]],\n            \"email\": \"jason@example.com\",\n            \"source\": \"calendar\",\n            \"start_date\": \"2023-09-17\",\n            \"end_date\": None\n\n        }\n    ]\n}\n</code></pre> <p>Notice that we have a list of queries that route to different search backends (email and calendar). We can even dispatch them async to be as performance as possible. Not only do we dispatch to different backends (that we have no control over), but you are likely going to render them to the user differently as well. Perhaps you want to summarize the emails in text, but you want to render the calendar events as a list that they can scroll across on a mobile app.</p> <p>Can I used framework X?</p> <p>I get this question a lot, but it's just code. Within these dispatches you can do whatever you want. You can use <code>input()</code> to ask the user for more information, make a post request, call a Langchain agent or LLamaindex query engine to get more information. The sky is the limit.</p> <p>Both of these examples showcase how both search providers and consumers can use <code>instructor</code> to model their systems. This is a powerful pattern that allows you to build a system that can be used by anyone, and can be used to build an LLM layer, from scratch, in front of any arbitrary backend.</p>","tags":["RAG","Embeddings","Query Understanding","Search Systems","Personal Assistant"]},{"location":"blog/2023/09/17/rag-is-more-than-just-embedding-search/#conclusion","title":"Conclusion","text":"<p>This is not about fancy embedding tricks, it's just plain old information retrieval and query understanding. The beauty of instructor is that it simplifies modeling the complex and lets you define the output of the language model, the prompts, and the payload we send to the backend in a single place.</p>","tags":["RAG","Embeddings","Query Understanding","Search Systems","Personal Assistant"]},{"location":"blog/2023/09/17/rag-is-more-than-just-embedding-search/#whats-next","title":"What's Next?","text":"<p>Here I want to show that `instructor`` isn\u2019t just about data extraction. It\u2019s a powerful framework for building a data model and integrating it with your LLM. Structured output is just the beginning \u2014 the untapped goldmine is skilled use of tools and APIs.</p> <p>If you enjoy the content or want to try out <code>instructor</code> please check out the github and give us a star!</p>","tags":["RAG","Embeddings","Query Understanding","Search Systems","Personal Assistant"]},{"location":"blog/2024/01/27/together/","title":"Structured Outputs with Together AI","text":"<p>Open-source LLMS are gaining popularity, and with the release of Together's Function calling models, its been easier than ever to get structured outputs.</p> <p>By the end of this blog post, you will learn how to effectively utilize instructor with Together AI. But before we proceed, let's first explore the concept of patching.</p> <p>Other Languages</p> <p>This blog post is written in Python, but the concepts are applicable to other languages as well, as we currently have support for Javascript and Elixir</p>","tags":["patching","open source"]},{"location":"blog/2024/01/27/together/#patching","title":"Patching","text":"<p>Instructor's patch enhances the openai api it with the following features:</p> <ul> <li><code>response_model</code> in <code>create</code> calls that returns a pydantic model</li> <li><code>max_retries</code> in <code>create</code> calls that retries the call if it fails by using a backoff strategy</li> </ul> <p>Learn More</p> <p>To learn more, please refer to the docs. To understand the benefits of using Pydantic with Instructor, visit the tips and tricks section of the why use Pydantic page.</p>","tags":["patching","open source"]},{"location":"blog/2024/01/27/together/#together-ai","title":"Together AI","text":"<p>The good news is that Together employs the same OpenAI client, and its models support some of these output modes too!</p> <p>Getting access</p> <p>If you want to try this out for yourself check out the Together AI website. You can get started here.</p> <pre><code>import os\nimport openai\nfrom pydantic import BaseModel\nimport instructor\n\nclient = openai.OpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n)\n\n\n# By default, the patch function will patch the ChatCompletion.create and ChatCompletion.create methods to support the response_model parameter\nclient = instructor.patch(client, mode=instructor.Mode.TOOLS)\n\n\n# Now, we can use the response_model parameter using only a base model\n# rather than having to use the OpenAISchema class\nclass UserExtract(BaseModel):\n    name: str\n    age: int\n\n\nuser: UserExtract = client.chat.completions.create(\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n    response_model=UserExtract,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"},\n    ],\n)\n\nassert isinstance(user, UserExtract), \"Should be instance of UserExtract\"\nassert user.name.lower() == \"jason\"\nassert user.age == 25\n\nprint(user.model_dump_json(indent=2))\n\"\"\"\n{\n  \"name\": \"jason\",\n  \"age\": 25\n}\n\"\"\"\n{\n    \"name\": \"Jason\",\n    \"age\": 25,\n}\n</code></pre> <p>You can find more information about Together's function calling support here.</p>","tags":["patching","open source"]},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/","title":"Good LLM Validation is Just Good Validation","text":"<p>What if your validation logic could learn and adapt like a human, but operate at the speed of software? This is the future of validation and it's already here.</p> <p>Validation is the backbone of reliable software. But traditional methods are static, rule-based, and can't adapt to new challenges. This post looks at how to bring dynamic, machine learning-driven validation into your software stack using Python libraries like <code>Pydantic</code> and <code>Instructor</code>. We validate these outputs using a validation function which conforms to the structure seen below.</p> <pre><code>def validation_function(value):\n    if condition(value):\n        raise ValueError(\"Value is not valid\")\n    return mutation(value)\n</code></pre>","tags":["pydantic","validation","guardrails","constitutional ai","chain of thought","citations"]},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#what-is-instructor","title":"What is Instructor?","text":"<p><code>Instructor</code> helps to ensure you get the exact response type you're looking for when using openai's function call api. Once you've defined the <code>Pydantic</code> model for your desired response, <code>Instructor</code> handles all the complicated logic in-between - from the parsing/validation of the response to the automatic retries for invalid responses. This means that we can build in validators 'for free' and have a clear separation of concerns between the prompt and the code that calls openai.</p> <pre><code>from openai import OpenAI\nimport instructor  # pip install instructor\nfrom pydantic import BaseModel\n\n# This enables response_model keyword\n# from client.chat.completions.create\nclient = instructor.patch(OpenAI())  # (1)!\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\nuser: UserDetail = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserDetail,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"},\n    ],\n    max_retries=3,  # (2)!\n)\n\nassert user.name == \"Jason\"  # (3)!\nassert user.age == 25\n</code></pre> <ol> <li> <p>To simplify your work with OpenAI models and streamline the extraction of Pydantic objects from prompts, we     offer a patching mechanism for the <code>ChatCompletion</code> class.</p> </li> <li> <p>Invalid responses that fail to be validated succesfully will trigger up to as many reattempts as you define.</p> </li> <li> <p>As long as you pass in a <code>response_model</code> parameter to the <code>ChatCompletion</code> api call, the returned object will always     be a validated <code>Pydantic</code> object.</p> </li> </ol> <p>In this post, we'll explore how to evolve from static, rule-based validation methods to dynamic, machine learning-driven ones. You'll learn to use <code>Pydantic</code> and <code>Instructor</code> to leverage language models and dive into advanced topics like content moderation, validating chain of thought reasoning, and contextual validation.</p> <p>Let's examine how these approaches with a example. Imagine that you run a software company who wants to ensure you never serve hateful and racist content. This isn't an easy job since the language around these topics change very quickly and frequently.</p>","tags":["pydantic","validation","guardrails","constitutional ai","chain of thought","citations"]},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#software-10-introduction-to-validations-in-pydantic","title":"Software 1.0: Introduction to Validations in Pydantic","text":"<p>A simple method could be to compile a list of different words that are often associated with hate speech. For simplicity, let's assume that we've found that the words <code>Steal</code> and <code>Rob</code> are good predictors of hateful speech from our database. We can modify our validation structure above to accomodate this.</p> <p>This will throw an error if we pass in a string like <code>Let's rob the bank!</code> or <code>We should steal from the supermarkets</code>.</p> <p>Pydantic offers two approaches for this validation: using the <code>field_validator</code> decorator or the <code>Annotated</code> hints.</p>","tags":["pydantic","validation","guardrails","constitutional ai","chain of thought","citations"]},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#using-field_validator-decorator","title":"Using <code>field_validator</code> decorator","text":"<p>We can use the <code>field_validator</code> decorator to define a validator for a field in Pydantic. Here's a quick example of how we might be able to do so.</p> <pre><code>from pydantic import BaseModel, ValidationError, field_validator\n\n\nclass UserMessage(BaseModel):\n    message: str\n\n    @field_validator('message')\n    def message_cannot_have_blacklisted_words(cls, v: str) -&gt; str:\n        for word in v.split():  # (1)!\n            if word.lower() in {'rob', 'steal'}:\n                raise ValueError(f\"`{word}` was found in the message `{v}`\")\n        return v\n\n\ntry:\n    UserMessage(message=\"This is a lovely day\")\n    UserMessage(message=\"We should go and rob a bank\")\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for UserMessage\n    message\n      Value error, `rob` was found in the message `We should go and rob a bank` [type=value_error, input_value='We should go and rob a bank', input_type=str]\n        For further information visit https://errors.pydantic.dev/2.6/v/value_error\n    \"\"\"\n</code></pre> <ol> <li>We split the sentence into its individual words and iterate through each of the words. We then try to see if any of these     words are in our blacklist which in this case is just <code>rob</code> and <code>steal</code></li> </ol> <p>Since the message <code>This is a lovely day</code> does not have any blacklisted words, no errors are thrown. However, in the given example above, the validation fails for the message <code>We should go and rob a bank</code> due to the presence of the word <code>rob</code> and the corresponding error message is displayed.</p> <pre><code>1 validation error for UserMessage\nmessage\n  Value error, `rob` was found in the message `We should go and rob a bank` [type=value_error, input_value='We should go and rob a bank', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.4/v/value_error\n</code></pre>","tags":["pydantic","validation","guardrails","constitutional ai","chain of thought","citations"]},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#using-annotated","title":"Using <code>Annotated</code>","text":"<p>Alternatively, you can use the <code>Annotated</code> function to perform the same validation. Here's an example where we utilise the same function we started with.</p> <pre><code>from pydantic import BaseModel, ValidationError\nfrom typing import Annotated\nfrom pydantic.functional_validators import AfterValidator\n\n\ndef message_cannot_have_blacklisted_words(value: str):\n    for word in value.split():\n        if word.lower() in {'rob', 'steal'}:\n            raise ValueError(f\"`{word}` was found in the message `{value}`\")\n    return value\n\n\nclass UserMessage(BaseModel):\n    message: Annotated[str, AfterValidator(message_cannot_have_blacklisted_words)]\n\n\ntry:\n    UserMessage(message=\"This is a lovely day\")\n    UserMessage(message=\"We should go and rob a bank\")\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for UserMessage\n    message\n      Value error, `rob` was found in the message `We should go and rob a bank` [type=value_error, input_value='We should go and rob a bank', input_type=str]\n        For further information visit https://errors.pydantic.dev/2.6/v/value_error\n    \"\"\"\n</code></pre> <p>This code snippet achieves the same validation result. If the user message contains any of the words in the blacklist, a <code>ValueError</code> is raised and the corresponding error message is displayed.</p> <pre><code>1 validation error for UserMessage\nmessage\n  Value error, `rob` was found in the message `We should go and rob a bank` [type=value_error, input_value='We should go and rob a bank', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.4/v/value_error\n</code></pre> <p>Validation is a fundamental concept in software development and remains the same when applied to AI systems. Existing programming concepts should be leveraged when possible instead of introducing new terms and standards. The underlying principles of validation remain unchanged.</p> <p>Suppose now that we've gotten a new message - <code>Violence is always acceptable, as long as we silence the witness</code>. Our original validator wouldn't throw any errors when passed this new message since it uses neither the words <code>rob</code> or <code>steal</code>. However, it's clear that it is not a message which should be published. How can we ensure that our validation logic can adapt to new challenges?</p>","tags":["pydantic","validation","guardrails","constitutional ai","chain of thought","citations"]},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#software-30-validation-for-llms-or-powered-by-llms","title":"Software 3.0: Validation for LLMs or powered by LLMs","text":"<p>Building upon the understanding of simple field validators, let's delve into probabilistic validation in software 3.0, (prompt engineering). We'll introduce an LLM-powered validator called <code>llm_validator</code> that uses a statement to verify the value.</p> <p>We can get around this by using the inbuilt <code>llm_validator</code> class from <code>Instructor</code>.</p> <pre><code>from instructor import llm_validator\nfrom pydantic import BaseModel, ValidationError\nfrom typing import Annotated\nfrom pydantic.functional_validators import AfterValidator\n\n\nclass UserMessage(BaseModel):\n    message: Annotated[\n        str, AfterValidator(llm_validator(\"don't say objectionable things\"))\n    ]\n\n\ntry:\n    UserMessage(\n        message=\"Violence is always acceptable, as long as we silence the witness\"\n    )\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for UserMessage\n    message\n      Assertion failed, The statement promotes violence, which is objectionable. [type=assertion_error, input_value='Violence is always accep... we silence the witness', input_type=str]\n        For further information visit https://errors.pydantic.dev/2.6/v/assertion_error\n    \"\"\"\n</code></pre> <p>This produces the following error message as seen below</p> <pre><code>1 validation error for UserMessage\nmessage\n  Assertion failed, The statement promotes violence, which is objectionable. [type=assertion_error, input_value='Violence is always accep... we silence the witness', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.4/v/assertion_error\n</code></pre> <p>The error message is generated by the language model (LLM) rather than the code itself, making it helpful for re-asking the model in a later section. To better understand this approach, let's see how to build an <code>llm_validator</code> from scratch.</p>","tags":["pydantic","validation","guardrails","constitutional ai","chain of thought","citations"]},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#creating-your-own-field-level-llm_validator","title":"Creating Your Own Field Level <code>llm_validator</code>","text":"<p>Building your own <code>llm_validator</code> can be a valuable exercise to get started with <code>Instructor</code> and create custom validators.</p> <p>Before we continue, let's review the anatomy of a validator:</p> <pre><code>def validation_function(value):\n    if condition(value):\n        raise ValueError(\"Value is not valid\")\n    return value\n</code></pre> <p>As we can see, a validator is simply a function that takes in a value and returns a value. If the value is not valid, it raises a <code>ValueError</code>. We can represent this using the following structure:</p> <pre><code>class Validation(BaseModel):\n    is_valid: bool = Field(\n        ..., description=\"Whether the value is valid based on the rules\"\n    )\n    error_message: Optional[str] = Field(\n        ...,\n        description=\"The error message if the value is not valid, to be used for re-asking the model\",\n    )\n</code></pre> <p>Using this structure, we can implement the same logic as before and utilize <code>Instructor</code> to generate the validation.</p> <pre><code>import instructor\nfrom openai import OpenAI\n\n# Enables `response_model` and `max_retries` parameters\nclient = instructor.patch(OpenAI())\n\n\ndef validator(v):\n    statement = \"don't say objectionable things\"\n    resp = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a validator. Determine if the value is valid for the statement. If it is not, explain why.\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Does `{v}` follow the rules: {statement}\",\n            },\n        ],\n        # this comes from client = instructor.patch(OpenAI())\n        response_model=Validation,  # (1)!\n    )\n    if not resp.is_valid:\n        raise ValueError(resp.error_message)\n    return v\n</code></pre> <ol> <li>The new parameter of <code>response_model</code> comes from <code>client = instructor.patch(OpenAI())</code> and does not exist in the original OpenAI SDK. This    allows us to pass in the <code>Pydantic</code> model that we want as a response.</li> </ol> <p>Now we can use this validator in the same way we used the <code>llm_validator</code> from <code>Instructor</code>.</p> <pre><code>class UserMessage(BaseModel):\n    message: Annotated[str, AfterValidator(validator)]\n</code></pre>","tags":["pydantic","validation","guardrails","constitutional ai","chain of thought","citations"]},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#writing-more-complex-validations","title":"Writing more complex validations","text":"","tags":["pydantic","validation","guardrails","constitutional ai","chain of thought","citations"]},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#validating-chain-of-thought","title":"Validating Chain of Thought","text":"<p>A popular way of prompting large language models nowadays is known as chain of thought. This involves getting a model to generate reasons and explanations for an answer to a prompt.</p> <p>We can utilise <code>Pydantic</code> and <code>Instructor</code> to perform a validation to check of the reasoning is reasonable, given both the answer and the chain of thought. To do this we can't build a field validator since we need to access multiple fields in the model. Instead we can use a model validator.</p> <pre><code>def validate_chain_of_thought(values):\n    chain_of_thought = values[\"chain_of_thought\"]\n    answer = values[\"answer\"]\n    resp = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a validator. Determine if the value is valid for the statement. If it is not, explain why.\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Verify that `{answer}` follows the chain of thought: {chain_of_thought}\",\n            },\n        ],\n        # this comes from client = instructor.patch(OpenAI())\n        response_model=Validation,\n    )\n    if not resp.is_valid:\n        raise ValueError(resp.error_message)\n    return values\n</code></pre> <p>We can then take advantage of the <code>model_validator</code> decorator to perform a validation on a subset of the model's data.</p> <p>We're defining a model validator here which runs before <code>Pydantic</code> parses the input into its respective fields. That's why we have a before keyword used in the <code>model_validator</code> class.</p> <pre><code>from pydantic import BaseModel, model_validator\n\n\nclass AIResponse(BaseModel):\n    chain_of_thought: str\n    answer: str\n\n    @model_validator(mode='before')\n    @classmethod\n    def chain_of_thought_makes_sense(cls, data: Any) -&gt; Any:\n        # here we assume data is the dict representation of the model\n        # since we use 'before' mode.\n        return validate_chain_of_thought(data)\n</code></pre> <p>Now, when you create a <code>AIResponse</code> instance, the <code>chain_of_thought_makes_sense</code> validator will be invoked. Here's an example:</p> <pre><code>try:\n    resp = AIResponse(chain_of_thought=\"1 + 1 = 2\", answer=\"The meaning of life is 42\")\nexcept ValidationError as e:\n    print(e)\n</code></pre> <p>If we create a <code>AIResponse</code> instance with an answer that does not follow the chain of thought, we will get an error.</p> <pre><code>1 validation error for AIResponse\n    Value error, The statement 'The meaning of life is 42' does not follow the chain of thought: 1 + 1 = 2.\n    [type=value_error, input_value={'chain_of_thought': '1 +... meaning of life is 42'}, input_type=dict]\n</code></pre>","tags":["pydantic","validation","guardrails","constitutional ai","chain of thought","citations"]},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#validating-citations-from-original-text","title":"Validating Citations From Original Text","text":"<p>Let's see a more concrete example. Let's say that we've asked our model a question about some text source and we want to validate that the generated answer is supported by the source. This would allow us to minimize hallucinations and prevent statements that are not backed by the original text. While we could verify this by looking up the original source manually, a more scalable approach is to use a validator to do this automatically.</p> <p>We can pass in additional context to our validation functions using the <code>model_validate</code> function in <code>Pydantic</code> so that our models have more information to work with when performing validation. This context is a normal python dictionary and can be accessed inside the <code>info</code> argument in our validator functions.</p> <pre><code>from pydantic import ValidationInfo, BaseModel, field_validator\n\n\nclass AnswerWithCitation(BaseModel):\n    answer: str\n    citation: str\n\n    @field_validator('citation')\n    @classmethod\n    def citation_exists(cls, v: str, info: ValidationInfo):  # (1)!\n        context = info.context\n        if context:\n            context = context.get('text_chunk')\n            if v not in context:\n                raise ValueError(f\"Citation `{v}` not found in text chunks\")\n        return v\n</code></pre> <ol> <li>This <code>info</code> object corresponds to the value of <code>context</code> that we pass into the <code>model_validate</code> function as seen below.</li> </ol> <p>We can then take our original example and test it against our new model</p> <pre><code>try:\n    AnswerWithCitation.model_validate(\n        {\"answer\": \"Jason is a cool guy\", \"citation\": \"Jason is cool\"},\n        context={\"text_chunk\": \"Jason is just a guy\"},  # (1)!\n    )\nexcept ValidationError as e:\n    print(e)\n</code></pre> <ol> <li>This <code>context</code> object is just a normal python dictionary and can take in and store any arbitrary values</li> </ol> <p>This in turn generates the following error since <code>Jason is cool</code> does not exist in the text <code>Jason is just a guy</code>.</p> <pre><code>1 validation error for AnswerWithCitation\ncitation\nValue error, Citation `Jason is cool` not found in text chunks [type=value_error, input_value='Jason is cool', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.4/v/value_error\n</code></pre>","tags":["pydantic","validation","guardrails","constitutional ai","chain of thought","citations"]},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#putting-it-all-together-with-client-instructorpatchopenai","title":"Putting it all together with <code>client = instructor.patch(OpenAI())</code>","text":"<p>To pass this context from the <code>client.chat.completions.create</code> call, <code>client = instructor.patch(OpenAI())</code> also passes the <code>validation_context</code>, which will be accessible from the <code>info</code> argument in the decorated validator functions.</p> <pre><code>from openai import OpenAI\nimport instructor\n\n# Enables `response_model` and `max_retries` parameters\nclient = instructor.patch(OpenAI())\n\n\ndef answer_question(question: str, text_chunk: str) -&gt; AnswerWithCitation:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"Answer the question: {question} with the text chunk: {text_chunk}\",\n            },\n        ],\n        response_model=AnswerWithCitation,\n        validation_context={\"text_chunk\": text_chunk},\n    )\n</code></pre>","tags":["pydantic","validation","guardrails","constitutional ai","chain of thought","citations"]},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#error-handling-and-re-asking","title":"Error Handling and Re-Asking","text":"<p>Validators can ensure certain properties of the outputs by throwing errors, in an AI system we can use the errors and allow language model to self correct. The by running <code>client = instructor.patch(OpenAI())</code> not only do we add <code>response_model</code> and <code>validation_context</code> it also allows you to use the <code>max_retries</code> parameter to specify the number of times to try and self correct.</p> <p>This approach provides a layer of defense against two types of bad outputs:</p> <ol> <li>Pydantic Validation Errors (code or LLM-based)</li> <li>JSON Decoding Errors (when the model returns an incorrect response)</li> </ol>","tags":["pydantic","validation","guardrails","constitutional ai","chain of thought","citations"]},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#define-the-response-model-with-validators","title":"Define the Response Model with Validators","text":"<p>To keep things simple let's assume we have a model that returns a <code>UserModel</code> object. We can define the response model using Pydantic and add a field validator to ensure that the name is in uppercase.</p> <pre><code>from pydantic import BaseModel, field_validator\n\n\nclass UserModel(BaseModel):\n    name: str\n    age: int\n\n    @field_validator(\"name\")\n    @classmethod\n    def validate_name(cls, v):\n        if v.upper() != v:\n            raise ValueError(\"Name must be in uppercase.\")\n        return v\n</code></pre> <p>This is where the <code>max_retries</code> parameter comes in. It allows the model to self correct and retry the prompt using the error message rather than the prompt.</p> <pre><code>model = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"},\n    ],\n    # Powered by client = instructor.patch(OpenAI())\n    response_model=UserModel,\n    max_retries=2,\n)\n\nassert model.name == \"JASON\"\n</code></pre> <p>In this example, even though there is no code explicitly transforming the name to uppercase, the model is able to correct the output.</p>","tags":["pydantic","validation","guardrails","constitutional ai","chain of thought","citations"]},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#conclusion","title":"Conclusion","text":"<p>From the simplicity of Pydantic and Instructor to the dynamic validation capabilities of LLMs, the landscape of validation is changing but without needing to introduce new contepts. It's clear that the future of validation is not just about preventing bad data but about allowing llms to understand the data and correcting it.</p> <p>If you enjoy the content or want to try out <code>Instructor</code> please check out the github and give us a star!</p>","tags":["pydantic","validation","guardrails","constitutional ai","chain of thought","citations"]},{"location":"cli/","title":"Instructor CLI","text":"<p>Welcome to the Instructor Command-Line Interface (CLI), a tool designed to ease your experience with the OpenAI API. Whether it's tracking your API usage or fine-tuning your models, Instructor CLI is your go-to utility.</p>"},{"location":"cli/#quick-start","title":"Quick Start","text":"<p>First things first: make sure your OpenAI API key is set as an environment variable. The CLI will use this for authenticating your requests to OpenAI's services.</p> <p>You can set the API key in your terminal as follows:</p> <pre><code>export OPENAI_API_KEY=\"your-api-key-here\"\n</code></pre>"},{"location":"cli/#installation-setup","title":"Installation &amp; Setup","text":"<pre><code>pip install instructor\n</code></pre>"},{"location":"cli/#features","title":"Features","text":"<ul> <li>API Usage Monitoring: Keep tabs on your API usage right from the terminal. Track token counts, total requests, and even calculate the costs. To learn more, consult the Usage Guide.</li> <li>Model Fine-Tuning: Optimize your models to meet your specific requirements using our fine-tuning app. For more details, check out the Fine-Tuning Guide.</li> </ul>"},{"location":"cli/#support-contribution","title":"Support &amp; Contribution","text":"<p>Need help or want to contribute? Visit our GitHub Repository</p>"},{"location":"cli/finetune/","title":"Using the Command Line Interface","text":"<p>The instructor CLI provides functionalities for managing fine-tuning jobs on OpenAI.</p> <p>Incomplete API</p> <p>The CLI is still under development and does not yet support all features of the API. If you would like to use a feature that is not yet supported, please consider using the contributing to our library jxnl/instructor instead.</p> <pre><code>!!! note \"Low hanging fruit\"\n\n    If you want to contribute we're looking for a few things:\n\n    1. Adding filenames on upload\n</code></pre>"},{"location":"cli/finetune/#creating-a-fine-tuning-job","title":"Creating a Fine-Tuning Job","text":""},{"location":"cli/finetune/#view-jobs-options","title":"View Jobs Options","text":"<pre><code>$ instructor jobs --help\n\n Usage: instructor jobs [OPTIONS] COMMAND [ARGS]...\n\n Monitor and create fine tuning jobs\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --help                            Display the help message.                             \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 cancel                    Cancel a fine-tuning job.                                                         \u2502\n\u2502 create-from-file          Create a fine-tuning job from a file.                                             \u2502\n\u2502 create-from-id            Create a fine-tuning job from an existing ID.                                     \u2502\n\u2502 list                      Monitor the status of the most recent fine-tuning jobs.                           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"cli/finetune/#create-from-file","title":"Create from File","text":"<p>The create-from-file command uploads and trains a model in a single step.</p> <pre><code>\u276f instructor jobs create-from-file --help\n\nUsage: instructor jobs create-from-file [OPTIONS] FILE\n\n Create a fine-tuning job from a file.\n\n\u256d\u2500 Arguments \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *    file      TEXT  Path to the file for fine-tuning [default: None] [required]                  \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --model                           TEXT     Model to use for fine-tuning [default: gpt-3.5-turbo]  \u2502\n\u2502 --poll                            INTEGER  Polling interval in seconds [default: 2]               \u2502\n\u2502 --n-epochs                        INTEGER  Number of epochs for fine-tuning                       \u2502\n\u2502 --batch-size                      TEXT     Batch size for fine-tuning                             \u2502\n\u2502 --learning-rate-multiplier        TEXT     Learning rate multiplier for fine-tuning               \u2502\n\u2502 --validation-file                 TEXT     Path to the validation file [default: None]            \u2502\n\u2502 --model-suffix                    TEXT     Suffix to identify the model [default: None]           \u2502\n\u2502 --help                                     Show this message and exit.                            \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</code></pre>"},{"location":"cli/finetune/#usage","title":"Usage","text":"<pre><code>$ instructor jobs create-from-file transformed_data.jsonl --validation_file validation_data.jsonl --n_epochs 3 --batch_size 16 --learning_rate_multiplier 0.5\n</code></pre>"},{"location":"cli/finetune/#create-from-id","title":"Create from ID","text":"<p>The create-from-id command uses an uploaded file and trains a model</p> <pre><code>\u276f instructor jobs create-from-id --help\n\n Usage: instructor jobs create-from-id [OPTIONS] ID\n\n Create a fine-tuning job from an existing ID.\n\n\u256d\u2500 Arguments \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *    id      TEXT  ID of the existing fine-tuning job [default: None] [required]      \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --model                           TEXT     Model to use for fine-tuning               \u2502\n\u2502                                            [default: gpt-3.5-turbo]                   \u2502\n\u2502 --n-epochs                        INTEGER  Number of epochs for fine-tuning           \u2502\n\u2502 --batch-size                      TEXT     Batch size for fine-tuning                 \u2502\n\u2502 --learning-rate-multiplier        TEXT     Learning rate multiplier for fine-tuning   \u2502\n\u2502 --validation-file-id              TEXT     ID of the uploaded validation file         \u2502\n\u2502                                            [default: None]                            \u2502\n\u2502 --help                                     Show this message and exit.                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"cli/finetune/#usage_1","title":"Usage","text":"<pre><code>$ instructor files upload transformed_data.jsonl\n$ instructor files upload validation_data.jsonl\n$ instructor files list\n...\n$ instructor jobs create_from_id &lt;file_id&gt; --validation_file &lt;validation_file_id&gt; --n_epochs 3 --batch_size 16 --learning_rate_multiplier 0.5\n</code></pre>"},{"location":"cli/finetune/#viewing-files-and-jobs","title":"Viewing Files and Jobs","text":""},{"location":"cli/finetune/#viewing-jobs","title":"Viewing Jobs","text":"<pre><code>$ instructor jobs list\n\nOpenAI Fine Tuning Job Monitoring\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503                \u2503              \u2503                \u2503     Completion \u2503                 \u2503                \u2503        \u2503                 \u2503\n\u2503 Job ID         \u2503 Status       \u2503  Creation Time \u2503           Time \u2503 Model Name      \u2503 File ID        \u2503 Epochs \u2503 Base Model      \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 ftjob-PWo6uwk\u2026 \u2502 \ud83d\udeab cancelled \u2502     2023-08-23 \u2502            N/A \u2502                 \u2502 file-F7lJg6Z4\u2026 \u2502 3      \u2502 gpt-3.5-turbo-\u2026 \u2502\n\u2502                \u2502              \u2502       23:10:54 \u2502                \u2502                 \u2502                \u2502        \u2502                 \u2502\n\u2502 ftjob-1whjva8\u2026 \u2502 \ud83d\udeab cancelled \u2502     2023-08-23 \u2502            N/A \u2502                 \u2502 file-F7lJg6Z4\u2026 \u2502 3      \u2502 gpt-3.5-turbo-\u2026 \u2502\n\u2502                \u2502              \u2502       22:47:05 \u2502                \u2502                 \u2502                \u2502        \u2502                 \u2502\n\u2502 ftjob-wGoBDld\u2026 \u2502 \ud83d\udeab cancelled \u2502     2023-08-23 \u2502            N/A \u2502                 \u2502 file-F7lJg6Z4\u2026 \u2502 3      \u2502 gpt-3.5-turbo-\u2026 \u2502\n\u2502                \u2502              \u2502       22:44:12 \u2502                \u2502                 \u2502                \u2502        \u2502                 \u2502\n\u2502 ftjob-yd5aRTc\u2026 \u2502 \u2705 succeeded \u2502     2023-08-23 \u2502     2023-08-23 \u2502 ft:gpt-3.5-tur\u2026 \u2502 file-IQxAUDqX\u2026 \u2502 3      \u2502 gpt-3.5-turbo-\u2026 \u2502\n\u2502                \u2502              \u2502       14:26:03 \u2502       15:02:29 \u2502                 \u2502                \u2502        \u2502                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                    Automatically refreshes every 5 seconds, press Ctrl+C to exit\n</code></pre>"},{"location":"cli/finetune/#viewing-files","title":"Viewing Files","text":"<pre><code>$ instructor files list\n\nOpenAI Files\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 File ID                       \u2503 Size (bytes) \u2503 Creation Time       \u2503 Filename \u2503 Purpose   \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 file-0lw2BSNRUlXZXRRu2beCCWjl \u2502       369523 \u2502 2023-08-23 23:31:57 \u2502 file     \u2502 fine-tune \u2502\n\u2502 file-IHaUXcMEykmFUp1kt2puCDEq \u2502       369523 \u2502 2023-08-23 23:09:35 \u2502 file     \u2502 fine-tune \u2502\n\u2502 file-ja9vRBf0FydEOTolaa3BMqES \u2502       369523 \u2502 2023-08-23 22:42:29 \u2502 file     \u2502 fine-tune \u2502\n\u2502 file-F7lJg6Z47CREvmx4kyvyZ6Sn \u2502       369523 \u2502 2023-08-23 22:42:03 \u2502 file     \u2502 fine-tune \u2502\n\u2502 file-YUxqZPyJRl5GJCUTw3cNmA46 \u2502       369523 \u2502 2023-08-23 22:29:10 \u2502 file     \u2502 fine-tune \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"cli/finetune/#contributions","title":"Contributions","text":"<p>We aim to provide a light wrapper around the API rather than offering a complete CLI. Contributions are welcome! Please feel free to make an issue at jxnl/instructor/issues or submit a pull request.</p>"},{"location":"cli/usage/","title":"Using the OpenAI API Usage CLI","text":"<p>The OpenAI API Usage CLI tool provides functionalities for monitoring your OpenAI API usage, breaking it down by model, date, and cost.</p>"},{"location":"cli/usage/#monitoring-api-usage","title":"Monitoring API Usage","text":""},{"location":"cli/usage/#view-usage-options","title":"View Usage Options","text":"<pre><code>$ instructor usage --help\n\n Usage: instructor usage [OPTIONS] COMMAND [ARGS]...\n\n Check OpenAI API usage data\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --help          Show this message and exit.                     \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 list       Displays OpenAI API usage data for the past N days.  \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"cli/usage/#list-usage-for-specific-number-of-days","title":"List Usage for Specific Number of Days","text":"<p>To display API usage for the past 3 days, use the following command:</p> <pre><code>$ instructor usage list -n 3\n</code></pre> <p>This will output a table similar to:</p> <pre><code>                 Usage Summary by Date, Snapshot, and Cost\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Date       \u2503 Snapshot ID               \u2503 Total Requests \u2503 Total Cost ($) \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 2023-09-04 \u2502 gpt-4-0613                \u2502             44 \u2502           0.68 \u2502\n\u2502 2023-09-04 \u2502 gpt-3.5-turbo-16k-0613    \u2502            195 \u2502           0.84 \u2502\n\u2502 2023-09-04 \u2502 text-embedding-ada-002-v2 \u2502            276 \u2502           0.00 \u2502\n\u2502 2023-09-04 \u2502 gpt-4-32k-0613            \u2502            328 \u2502          49.45 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"cli/usage/#list-usage-for-today","title":"List Usage for Today","text":"<p>To display the API usage for today, simply run:</p> <pre><code>$ instructor usage list\n</code></pre>"},{"location":"cli/usage/#contributions","title":"Contributions","text":"<p>We aim to provide a light wrapper around the API rather than offering a complete CLI. Contributions are welcome! Please feel free to make an issue at jxnl/instructor/issues or submit a pull request.</p>"},{"location":"concepts/alias/","title":"Alias","text":"<p>This page is a work in progress</p> <p>This page is a work in progress. Check out Pydantic's documentation</p>"},{"location":"concepts/caching/","title":"Caching","text":"<p>If you want to learn more about concepts in caching and how to use them in your own projects, check out our blog on the topic.</p>"},{"location":"concepts/caching/#1-functoolscache-for-simple-in-memory-caching","title":"1. <code>functools.cache</code> for Simple In-Memory Caching","text":"<p>When to Use: Ideal for functions with immutable arguments, called repeatedly with the same parameters in small to medium-sized applications. This makes sense when we might be reusing the same data within a single session. or in an application where we don't need to persist the cache between sessions.</p> <pre><code>import time\nimport functools\nimport openai\nimport instructor\nfrom pydantic import BaseModel\n\nclient = instructor.patch(openai.OpenAI())\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\n@functools.cache\ndef extract(data) -&gt; UserDetail:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[\n            {\"role\": \"user\", \"content\": data},\n        ],\n    )\n\n\nstart = time.perf_counter()  # (1)\nmodel = extract(\"Extract jason is 25 years old\")\nprint(f\"Time taken: {time.perf_counter() - start}\")\n#&gt; Time taken: 0.41433916706591845\n\nstart = time.perf_counter()\nmodel = extract(\"Extract jason is 25 years old\")  # (2)\nprint(f\"Time taken: {time.perf_counter() - start}\")\n#&gt; Time taken: 1.7080456018447876e-06\n</code></pre> <ol> <li>Using <code>time.perf_counter()</code> to measure the time taken to run the function is better than using <code>time.time()</code> because it's more accurate and less susceptible to system clock changes.</li> <li>The second time we call <code>extract</code>, the result is returned from the cache, and the function is not called.</li> </ol> <p>Changing the Model does not Invalidate the Cache</p> <p>Note that changing the model does not invalidate the cache. This is because the cache key is based on the function's name and arguments, not the model. This means that if we change the model, the cache will still return the old result.</p> <p>Now we can call <code>extract</code> multiple times with the same argument, and the result will be cached in memory for faster access.</p> <p>Benefits: Easy to implement, provides fast access due to in-memory storage, and requires no additional libraries.</p> What is a decorator? <p>A decorator is a function that takes another function and extends the behavior of the latter function without explicitly modifying it. In Python, decorators are functions that take a function as an argument and return a closure.</p> <pre><code>def decorator(func):\n    def wrapper(*args, **kwargs):\n        print(\"Do something before\")  # (1)\n        #&gt; Do something before\n        result = func(*args, **kwargs)\n        print(\"Do something after\")  # (2)\n        #&gt; Do something after\n        return result\n\n    return wrapper\n\n\n@decorator\ndef say_hello():\n    #&gt; Hello!\n    print(\"Hello!\")\n    #&gt; Hello!\n\n\nsay_hello()\n#&gt; \"Do something before\"\n#&gt; \"Hello!\"\n#&gt; \"Do something after\"\n</code></pre> <ol> <li>The code is executed before the function is called</li> <li>The code is executed after the function is called</li> </ol>"},{"location":"concepts/caching/#2-diskcache-for-persistent-large-data-caching","title":"2. <code>diskcache</code> for Persistent, Large Data Caching","text":"Copy Caching Code <p>We'll be using the same <code>instructor_cache</code> decorator for both <code>diskcache</code> and <code>redis</code> caching. You can copy the code below and use it for both examples.</p> <pre><code>import functools\nimport inspect\nimport diskcache\n\ncache = diskcache.Cache('./my_cache_directory')  # (1)\n\n\ndef instructor_cache(func):\n    \"\"\"Cache a function that returns a Pydantic model\"\"\"\n    return_type = inspect.signature(func).return_annotation\n    if not issubclass(return_type, BaseModel):  # (2)\n        raise ValueError(\"The return type must be a Pydantic model\")\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = f\"{func.__name__}-{functools._make_key(args, kwargs, typed=False)}\"\n        # Check if the result is already cached\n        if (cached := cache.get(key)) is not None:\n            # Deserialize from JSON based on the return type\n            return return_type.model_validate_json(cached)\n\n        # Call the function and cache its result\n        result = func(*args, **kwargs)\n        serialized_result = result.model_dump_json()\n        cache.set(key, serialized_result)\n\n        return result\n\n    return wrapper\n</code></pre> <ol> <li>We create a new <code>diskcache.Cache</code> instance to store the cached data. This will create a new directory called <code>my_cache_directory</code> in the current working directory.</li> <li>We only want to cache functions that return a Pydantic model to simplify serialization and deserialization logic in this example code</li> </ol> <p>Remember that you can change this code to support non-Pydantic models, or to use a different caching backend. More over, don't forget that this cache does not invalidate when the model changes, so you might want to encode the <code>Model.model_json_schema()</code> as part of the key.</p> <p>When to Use: Suitable for applications needing cache persistence between sessions or dealing with large datasets. This is useful when we want to reuse the same data across multiple sessions, or when we need to store large amounts of data!</p> <pre><code>import functools\nimport inspect\nimport instructor\nimport diskcache\n\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\nclient = instructor.patch(OpenAI())\ncache = diskcache.Cache('./my_cache_directory')\n\n\ndef instructor_cache(func):\n    \"\"\"Cache a function that returns a Pydantic model\"\"\"\n    return_type = inspect.signature(func).return_annotation  # (4)\n    if not issubclass(return_type, BaseModel):  # (1)\n        raise ValueError(\"The return type must be a Pydantic model\")\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = (\n            f\"{func.__name__}-{functools._make_key(args, kwargs, typed=False)}\"  #  (2)\n        )\n        # Check if the result is already cached\n        if (cached := cache.get(key)) is not None:\n            # Deserialize from JSON based on the return type (3)\n            return return_type.model_validate_json(cached)\n\n        # Call the function and cache its result\n        result = func(*args, **kwargs)\n        serialized_result = result.model_dump_json()\n        cache.set(key, serialized_result)\n\n        return result\n\n    return wrapper\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\n@instructor_cache\ndef extract(data) -&gt; UserDetail:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[\n            {\"role\": \"user\", \"content\": data},\n        ],\n    )\n</code></pre> <ol> <li>We only want to cache functions that return a Pydantic model to simplify serialization and deserialization logic</li> <li>We use functool's <code>_make_key</code> to generate a unique key based on the function's name and arguments. This is important because we want to cache the result of each function call separately.</li> <li>We use Pydantic's <code>model_validate_json</code> to deserialize the cached result into a Pydantic model.</li> <li>We use <code>inspect.signature</code> to get the function's return type annotation, which we use to validate the cached result.</li> </ol> <p>Benefits: Reduces computation time for heavy data processing, provides disk-based caching for persistence.</p>"},{"location":"concepts/caching/#2-redis-caching-decorator-for-distributed-systems","title":"2. Redis Caching Decorator for Distributed Systems","text":"Copy Caching Code <p>We'll be using the same <code>instructor_cache</code> decorator for both <code>diskcache</code> and <code>redis</code> caching. You can copy the code below and use it for both examples.</p> <pre><code>import functools\nimport inspect\nimport redis\n\ncache = redis.Redis(\"localhost\")\n\n\ndef instructor_cache(func):\n    \"\"\"Cache a function that returns a Pydantic model\"\"\"\n    return_type = inspect.signature(func).return_annotation\n    if not issubclass(return_type, BaseModel):\n        raise ValueError(\"The return type must be a Pydantic model\")\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = f\"{func.__name__}-{functools._make_key(args, kwargs, typed=False)}\"\n        # Check if the result is already cached\n        if (cached := cache.get(key)) is not None:\n            # Deserialize from JSON based on the return type\n            return return_type.model_validate_json(cached)\n\n        # Call the function and cache its result\n        result = func(*args, **kwargs)\n        serialized_result = result.model_dump_json()\n        cache.set(key, serialized_result)\n\n        return result\n\n    return wrapper\n</code></pre> <p>Remember that you can change this code to support non-Pydantic models, or to use a different caching backend. More over, don't forget that this cache does not invalidate when the model changes, so you might want to encode the <code>Model.model_json_schema()</code> as part of the key.</p> <p>When to Use: Recommended for distributed systems where multiple processes need to access the cached data, or for applications requiring fast read/write access and handling complex data structures.</p> <pre><code>import redis\nimport functools\nimport inspect\nimport instructor\n\nfrom pydantic import BaseModel\nfrom openai import OpenAI\n\nclient = instructor.patch(OpenAI())\ncache = redis.Redis(\"localhost\")\n\n\ndef instructor_cache(func):\n    \"\"\"Cache a function that returns a Pydantic model\"\"\"\n    return_type = inspect.signature(func).return_annotation\n    if not issubclass(return_type, BaseModel):  # (1)\n        raise ValueError(\"The return type must be a Pydantic model\")\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = f\"{func.__name__}-{functools._make_key(args, kwargs, typed=False)}\"  # (2)\n        # Check if the result is already cached\n        if (cached := cache.get(key)) is not None:\n            # Deserialize from JSON based on the return type\n            return return_type.model_validate_json(cached)\n\n        # Call the function and cache its result\n        result = func(*args, **kwargs)\n        serialized_result = result.model_dump_json()\n        cache.set(key, serialized_result)\n\n        return result\n\n    return wrapper\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\n@instructor_cache\ndef extract(data) -&gt; UserDetail:\n    # Assuming client.chat.completions.create returns a UserDetail instance\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[\n            {\"role\": \"user\", \"content\": data},\n        ],\n    )\n</code></pre> <ol> <li>We only want to cache functions that return a Pydantic model to simplify serialization and deserialization logic</li> <li>We use functool's <code>_make_key</code> to generate a unique key based on the function's name and arguments. This is important because we want to cache the result of each function call separately.</li> </ol> <p>Benefits: Scalable for large-scale systems, supports fast in-memory data storage and retrieval, and is versatile for various data types.</p> <p>Looking carefully</p> <p>If you look carefully at the code above you'll notice that we're using the same <code>instructor_cache</code> decorator as before. The implementation is the same, but we're using a different caching backend!</p>"},{"location":"concepts/distillation/","title":"Distilling python functions into LLM","text":"<p><code>Instructions</code> from the <code>Instructor</code> library offers a seamless way to make language models backward compatible with existing Python functions. By employing Pydantic type hints, it not only ensures compatibility but also facilitates fine-tuning <code>gpt-3.5-turbo</code> to emulate these functions end-to-end.</p> <p>If you want to see the full example checkout examples/distillation</p>"},{"location":"concepts/distillation/#the-challenges-in-function-level-fine-tuning","title":"The Challenges in Function-Level Fine-Tuning","text":"<p>Replicating the behavior of a Python function in a language model involves intricate data preparation. For instance, teaching a model to execute three-digit multiplication is not as trivial as implementing <code>def f(a, b): return a * b</code>. OpenAI's fine-tuning script coupled with their function calling utility provides a structured output, thereby simplifying the data collection process. Additionally, this eliminates the need for passing the schema to the model, thus conserving tokens.</p>"},{"location":"concepts/distillation/#the-role-of-instructions-in-simplifying-the-fine-tuning-process","title":"The Role of <code>Instructions</code> in Simplifying the Fine-Tuning Process","text":"<p>By using <code>Instructions</code>, you can annotate a Python function that returns a Pydantic object, thereby automating the dataset creation for fine-tuning. A handler for logging is all that's needed to build this dataset.</p>"},{"location":"concepts/distillation/#how-to-implement-instructions-in-your-code","title":"How to Implement <code>Instructions</code> in Your Code","text":""},{"location":"concepts/distillation/#quick-start-how-to-use-instructors-distillation-feature","title":"Quick Start: How to Use Instructor's Distillation Feature","text":"<p>Before we dig into the nitty-gritty, let's look at how easy it is to use Instructor's distillation feature to use function calling finetuning to export the data to a JSONL file.</p> <pre><code>import logging\nimport random\nfrom pydantic import BaseModel\nfrom instructor import Instructions  # pip install instructor\n\n# Logging setup\nlogging.basicConfig(level=logging.INFO)\n\ninstructions = Instructions(\n    name=\"three_digit_multiply\",\n    finetune_format=\"messages\",\n    # log handler is used to save the data to a file\n    # you can imagine saving it to a database or other storage\n    # based on your needs!\n    log_handlers=[logging.FileHandler(\"math_finetunes.jsonl\")],\n)\n\n\nclass Multiply(BaseModel):\n    a: int\n    b: int\n    result: int\n\n\n# Define a function with distillation\n# The decorator will automatically generate a dataset for fine-tuning\n# They must return a pydantic model to leverage function calling\n@instructions.distil\ndef fn(a: int, b: int) -&gt; Multiply:\n    resp = a * b\n    return Multiply(a=a, b=b, result=resp)\n\n\n# Generate some data\nfor _ in range(10):\n    random.seed(42)\n    a = random.randint(100, 999)\n    b = random.randint(100, 999)\n    print(fn(a, b))\n    #&gt; a=754 b=214 result=161356\n    #&gt; a=754 b=214 result=161356\n    #&gt; a=754 b=214 result=161356\n    #&gt; a=754 b=214 result=161356\n    #&gt; a=754 b=214 result=161356\n    #&gt; a=754 b=214 result=161356\n    #&gt; a=754 b=214 result=161356\n    #&gt; a=754 b=214 result=161356\n    #&gt; a=754 b=214 result=161356\n    #&gt; a=754 b=214 result=161356\n</code></pre>"},{"location":"concepts/distillation/#the-intricacies-of-fine-tuning-language-models","title":"The Intricacies of Fine-tuning Language Models","text":"<p>Fine-tuning isn't just about writing a function like <code>def f(a, b): return a * b</code>. It requires detailed data preparation and logging. However, Instructor provides a built-in logging feature and structured outputs to simplify this.</p>"},{"location":"concepts/distillation/#why-instructor-and-distillation-are-game-changers","title":"Why Instructor and Distillation are Game Changers","text":"<p>The library offers two main benefits:</p> <ol> <li>Efficiency: Streamlines functions, distilling requirements into model weights and a few lines of code.</li> <li>Integration: Eases combining classical machine learning and language models by providing a simple interface that wraps existing functions.</li> </ol>"},{"location":"concepts/distillation/#role-of-instructor-in-simplifying-fine-tuning","title":"Role of Instructor in Simplifying Fine-Tuning","text":"<p>The <code>from instructor import Instructions</code> feature is a time saver. It auto-generates a fine-tuning dataset, making it a breeze to imitate a function's behavior.</p>"},{"location":"concepts/distillation/#logging-output-and-running-a-finetune","title":"Logging Output and Running a Finetune","text":"<p>Here's how the logging output would look:</p> <pre><code>{\n    \"messages\": [\n        {\"role\": \"system\", \"content\": 'Predict the results of this function: ...'},\n        {\"role\": \"user\", \"content\": 'Return fn(133, b=539)'},\n        {\n            \"role\": \"assistant\",\n            \"function_call\": {\n                \"name\": \"Multiply\",\n                \"arguments\": '{\"a\":133,\"b\":539,\"result\":89509}',\n            },\n        },\n    ],\n    \"functions\": [\n        {\"name\": \"Multiply\", \"description\": \"Correctly extracted `Multiply`...\"}\n    ],\n}\n</code></pre> <p>Run a finetune like this:</p> <pre><code>instructor jobs create-from-file math_finetunes.jsonl\n</code></pre> <p>Once a model is trained you can simply change <code>mode</code> to <code>dispatch</code> and it will use the model to run the function!</p> <pre><code>from instructor import Instructions\nfrom pydantic import BaseModel\n\n\nclass Multiply(BaseModel):\n    a: int\n    b: int\n    result: int\n\n\ninstructions = Instructions(\n    name=\"three_digit_multiply\",\n)\n\n\n@instructions.distil(model='gpt-3.5-turbo:finetuned-123', mode=\"dispatch\")\ndef fn(a: int, b: int) -&gt; Multiply:\n    # now this code will be short circuited and the model will be used instead.\n    resp = a + b\n    return Multiply(a=a, b=b, result=resp)\n</code></pre> <p>With this, you can swap the function implementation, making it backward compatible. You can even imagine using the different models for different tasks or validating and runnign evals by using the original function and comparing it to the distillation.</p>"},{"location":"concepts/enums/","title":"Enums","text":"<p>To prevent data misalignment, we can use Enums for standardized fields. Always include an \"Other\" option as a fallback so the model can signal uncertainty.</p> <pre><code>from pydantic import BaseModel, Field\nfrom enum import Enum\n\n\nclass Role(Enum):\n    PRINCIPAL = \"PRINCIPAL\"\n    TEACHER = \"TEACHER\"\n    STUDENT = \"STUDENT\"\n    OTHER = \"OTHER\"\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Role = Field(\n        description=\"Correctly assign one of the predefined roles to the user.\"\n    )\n</code></pre> <p>If you're having a hard time with <code>Enum</code> and alternative is to use <code>Literal</code> instead.</p> <pre><code>from typing import Literal\nfrom pydantic import BaseModel\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Literal[\"PRINCIPAL\", \"TEACHER\", \"STUDENT\", \"OTHER\"]\n</code></pre>"},{"location":"concepts/fastapi/","title":"Integrating Pydantic Models with FastAPI","text":"<p>FastAPI is an enjoyable tool for building web applications in Python. It is well known for its integration with <code>Pydantic</code> models, which makes defining and validating data structures straightforward and efficient. In this guide, we explore how simple functions that return <code>Pydantic</code> models can seamlessly integrate with <code>FastAPI</code>.</p>"},{"location":"concepts/fastapi/#why-choose-fastapi-and-pydantic","title":"Why Choose FastAPI and Pydantic?","text":"<ul> <li>FastAPI is a modern, high-performance web framework for building APIs with Python.</li> <li>Supports OpenAPI and JSON Schema for automatic documentation and validation.</li> <li>Supports AsyncIO for asynchronous programming leveraging the AsyncOpenAI() client</li> </ul>"},{"location":"concepts/fastapi/#code-example-starting-a-fastapi-app-with-a-post-request","title":"Code Example: Starting a FastAPI App with a POST Request","text":"<p>The following code snippet demonstrates how to start a <code>FastAPI</code> app with a POST endpoint. This endpoint accepts and returns data defined by a <code>Pydantic</code> model.</p> <pre><code>import instructor\n\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\nfrom openai import AsyncOpenAI\n\n# Enables response_model\nclient = instructor.patch(AsyncOpenAI())\napp = FastAPI()\n\n\nclass UserData(BaseModel):\n    # This can be the model for the input data\n    query: str\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\n@app.post(\"/endpoint\", response_model=UserDetail)\nasync def endpoint_function(data: UserData) -&gt; UserDetail:\n    user_detail = await client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[\n            {\"role\": \"user\", \"content\": f\"Extract: `{data.query}`\"},\n        ],\n    )\n    return user_detail\n</code></pre>"},{"location":"concepts/fastapi/#streaming-responses-with-fastapi","title":"Streaming Responses with FastAPI","text":"<p><code>FastAPI</code> supports streaming responses, which is useful for returning large amounts of data. This feature is particularly useful when working with large language models (LLMs) that generate a large amount of data.</p> <pre><code>from fastapi import FastAPI\nfrom fastapi.responses import StreamingResponse\nfrom typing import Iterable\nfrom pydantic import BaseModel\n\napp = FastAPI()\n\n\nclass UserData(BaseModel):\n    query: str\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\n# Route to handle SSE events and return users\n@app.post(\"/extract\", response_class=StreamingResponse)\nasync def extract(data: UserData):\n    users = await client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=Iterable[UserDetail],\n        stream=True,\n        messages=[\n            {\"role\": \"user\", \"content\": data.query},\n        ],\n    )\n\n    async def generate():\n        for user in users:\n            resp_json = user.model_dump_json()\n            yield f\"data: {resp_json}\"\n        yield \"data: [DONE]\"\n\n    return StreamingResponse(generate(), media_type=\"text/event-stream\")\n</code></pre>"},{"location":"concepts/fastapi/#automatic-documentation-with-fastapi","title":"Automatic Documentation with FastAPI","text":"<p>FastAPI leverages the OpenAPI specification to automatically generate a dynamic and interactive documentation page, commonly referred to as the <code>/docs</code> page. This feature is incredibly useful for developers, as it offers a live environment to test API endpoints directly through the browser.</p> <p>To explore the capabilities of your API, follow these steps:</p> <ol> <li>Run the API using the Uvicorn command: <code>uvicorn main:app --reload</code>.</li> <li>Open your web browser and navigate to <code>http://127.0.0.1:8000/docs</code>.</li> <li>You will find an interactive UI where you can send different requests to your API and see the responses in real-time.</li> </ol> <p></p>"},{"location":"concepts/fields/","title":"Fields","text":"<p>The <code>pydantic.Field</code> function is used to customize and add metadata to fields of models. To learn more, check out the Pydantic documentation as this is a near replica of that documentation that is relevant to prompting.</p>"},{"location":"concepts/fields/#default-values","title":"Default values","text":"<p>The <code>default</code> parameter is used to define a default value for a field.</p> <pre><code>from pydantic import BaseModel, Field\n\n\nclass User(BaseModel):\n    name: str = Field(default='John Doe')\n\n\nuser = User()\nprint(user)\n#&gt; name='John Doe'\n</code></pre> <p>You can also use <code>default_factory</code> to define a callable that will be called to generate a default value.</p> <pre><code>from uuid import uuid4\n\nfrom pydantic import BaseModel, Field\n\n\nclass User(BaseModel):\n    id: str = Field(default_factory=lambda: uuid4().hex)\n</code></pre> <p>Info</p> <p>The <code>default</code> and <code>default_factory</code> parameters are mutually exclusive.</p> <p>Note</p> <p>If you use <code>typing.Optional</code>, it doesn't mean that the field has a default value of <code>None</code> you must use <code>default</code> or <code>default_factory</code> to define a default value. Then it will be considered <code>not required</code> when sent to the language model.</p>"},{"location":"concepts/fields/#using-annotated","title":"Using <code>Annotated</code>","text":"<p>The <code>Field</code> function can also be used together with <code>Annotated</code>.</p> <pre><code>from uuid import uuid4\n\nfrom typing_extensions import Annotated\n\nfrom pydantic import BaseModel, Field\n\n\nclass User(BaseModel):\n    id: Annotated[str, Field(default_factory=lambda: uuid4().hex)]\n</code></pre>"},{"location":"concepts/fields/#exclude","title":"Exclude","text":"<p>The <code>exclude</code> parameter can be used to control which fields should be excluded from the model when exporting the model. This is helpful when you want to exclude fields that are not relevant to the model generation like <code>scratch_pad</code> or <code>chain_of_thought</code></p> <p>See the following example:</p> <pre><code>from pydantic import BaseModel, Field\nfrom datetime import date\n\n\nclass DateRange(BaseModel):\n    chain_of_thought: str = Field(\n        description=\"Reasoning behind the date range.\", exclude=True\n    )\n    start_date: date\n    end_date: date\n\n\ndate_range = DateRange(\n    chain_of_thought=\"\"\"\n        I want to find the date range for the last 30 days.\n        Today is 2021-01-30 therefore the start date\n        should be 2021-01-01 and the end date is 2021-01-30\"\"\",\n    start_date=date(2021, 1, 1),\n    end_date=date(2021, 1, 30),\n)\nprint(date_range.model_dump_json())\n#&gt; {\"start_date\":\"2021-01-01\",\"end_date\":\"2021-01-30\"}\n</code></pre>"},{"location":"concepts/fields/#customizing-json-schema","title":"Customizing JSON Schema","text":"<p>There are some fields that are exclusively used to customise the generated JSON Schema:</p> <ul> <li><code>title</code>: The title of the field.</li> <li><code>description</code>: The description of the field.</li> <li><code>examples</code>: The examples of the field.</li> <li><code>json_schema_extra</code>: Extra JSON Schema properties to be added to the field.</li> </ul> <p>These all work as great opportunities to add more information to the JSON schema as part of your prompt engineering.</p> <p>Here's an example:</p> <pre><code>from pydantic import BaseModel, Field, SecretStr\n\n\nclass User(BaseModel):\n    age: int = Field(description='Age of the user')\n    name: str = Field(title='Username')\n    password: SecretStr = Field(\n        json_schema_extra={\n            'title': 'Password',\n            'description': 'Password of the user',\n            'examples': ['123456'],\n        }\n    )\n\n\nprint(User.model_json_schema())\n\"\"\"\n{\n    'properties': {\n        'age': {'description': 'Age of the user', 'title': 'Age', 'type': 'integer'},\n        'name': {'title': 'Username', 'type': 'string'},\n        'password': {\n            'description': 'Password of the user',\n            'examples': ['123456'],\n            'format': 'password',\n            'title': 'Password',\n            'type': 'string',\n            'writeOnly': True,\n        },\n    },\n    'required': ['age', 'name', 'password'],\n    'title': 'User',\n    'type': 'object',\n}\n\"\"\"\n</code></pre>"},{"location":"concepts/fields/#general-notes-on-json-schema-generation","title":"General notes on JSON schema generation","text":"<ul> <li>The JSON schema for Optional fields indicates that the value null is allowed.</li> <li>The Decimal type is exposed in JSON schema (and serialized) as a string.</li> <li>The JSON schema does not preserve namedtuples as namedtuples.</li> <li>When they differ, you can specify whether you want the JSON schema to represent the inputs to validation or the outputs from serialization.</li> <li>Sub-models used are added to the <code>$defs</code> JSON attribute and referenced, as per the spec.</li> <li>Sub-models with modifications (via the Field class) like a custom title, description, or default value, are recursively included instead of referenced.</li> <li>The description for models is taken from either the docstring of the class or the argument description to the Field class.</li> </ul>"},{"location":"concepts/lists/","title":"Multi-task and Streaming","text":"<p>A common use case of structured extraction is defining a single schema class and then making another schema to create a list to do multiple extraction</p> <pre><code>from typing import List\nfrom pydantic import BaseModel\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nclass Users(BaseModel):\n    users: List[User]\n\n\nprint(Users.model_json_schema())\n\"\"\"\n{\n    '$defs': {\n        'User': {\n            'properties': {\n                'name': {'title': 'Name', 'type': 'string'},\n                'age': {'title': 'Age', 'type': 'integer'},\n            },\n            'required': ['name', 'age'],\n            'title': 'User',\n            'type': 'object',\n        }\n    },\n    'properties': {\n        'users': {'items': {'$ref': '#/$defs/User'}, 'title': 'Users', 'type': 'array'}\n    },\n    'required': ['users'],\n    'title': 'Users',\n    'type': 'object',\n}\n\"\"\"\n</code></pre> <p>Defining a task and creating a list of classes is a common enough pattern that we make this convenient by making use of <code>Iterable[T]</code>. This lets us dynamically create a new class that:</p> <ol> <li>Has dynamic docstrings and class name based on the task</li> <li>Support streaming by collecting tokens until a task is received back out.</li> </ol>"},{"location":"concepts/lists/#extracting-tasks-using-iterable","title":"Extracting Tasks using Iterable","text":"<p>By using <code>Iterable</code> you get a very convenient class with prompts and names automatically defined:</p> <pre><code>import instructor\nfrom openai import OpenAI\nfrom typing import Iterable\nfrom pydantic import BaseModel\n\nclient = instructor.patch(OpenAI(), mode=instructor.function_calls.Mode.JSON)\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nusers = client.chat.completions.create(\n    model=\"gpt-3.5-turbo-1106\",\n    temperature=0.1,\n    response_model=Iterable[User],\n    stream=False,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Consider this data: Jason is 10 and John is 30.\\\n                         Correctly segment it into entitites\\\n                        Make sure the JSON is correct\",\n        },\n    ],\n)\nfor user in users:\n    print(user)\n    #&gt; ('tasks', [User(name='Jason', age=10), User(name='John', age=30)])\n\n#&gt; name=\"Jason\" \"age\"=10\n#&gt; name=\"John\" \"age\"=10\n</code></pre>"},{"location":"concepts/lists/#streaming-tasks","title":"Streaming Tasks","text":"<p>We can also generate tasks as the tokens are streamed in by defining an <code>Iterable[T]</code> type.</p> <p>Lets look at an example in action with the same class</p> <pre><code>import instructor\nimport openai\nfrom typing import Iterable\nfrom pydantic import BaseModel\n\nclient = instructor.patch(openai.OpenAI(), mode=instructor.Mode.TOOLS)\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nusers = client.chat.completions.create(\n    model=\"gpt-4\",\n    temperature=0.1,\n    stream=True,\n    response_model=Iterable[User],\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a perfect entity extraction system\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": (f\"Extract `Jason is 10 and John is 10`\"),\n        },\n    ],\n    max_tokens=1000,\n)\n\nfor user in users:\n    print(user)\n    #&gt; name='Jason' age=10\n    #&gt; name='John' age=10\n</code></pre>"},{"location":"concepts/lists/#asynchronous-streaming","title":"Asynchronous Streaming","text":"<p>I also just want to call out in this example that <code>instructor</code> also supports asynchronous streaming. This is useful when you want to stream a response model and process the results as they come in, but you'll need to use the <code>async for</code> syntax to iterate over the results.</p> <pre><code>import instructor\nimport openai\nfrom typing import Iterable\nfrom pydantic import BaseModel\n\nclient = instructor.patch(openai.AsyncOpenAI(), mode=instructor.Mode.TOOLS)\n\n\nclass UserExtract(BaseModel):\n    name: str\n    age: int\n\n\nasync def print_iterable_results():\n    model = await client.chat.completions.create(\n        model=\"gpt-4\",\n        response_model=Iterable[UserExtract],\n        max_retries=2,\n        stream=True,\n        messages=[\n            {\"role\": \"user\", \"content\": \"Make two up people\"},\n        ],\n    )\n    async for m in model:\n        print(m)\n        #&gt; name='John Doe' age=32\n        #&gt; name='Jane Smith' age=28\n\n\nimport asyncio\n\nasyncio.run(print_iterable_results())\n</code></pre>"},{"location":"concepts/maybe/","title":"Handling Missing Data","text":"<p>The <code>Maybe</code> pattern is a concept in functional programming used for error handling. Instead of raising exceptions or returning <code>None</code>, you can use a <code>Maybe</code> type to encapsulate both the result and potential errors.</p> <p>This pattern is particularly useful when making LLM calls, as providing language models with an escape hatch can effectively reduce hallucinations.</p>"},{"location":"concepts/maybe/#defining-the-model","title":"Defining the Model","text":"<p>Using Pydantic, we'll first define the <code>UserDetail</code> and <code>MaybeUser</code> classes.</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import Optional\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Optional[str] = Field(default=None)\n\n\nclass MaybeUser(BaseModel):\n    result: Optional[UserDetail] = Field(default=None)\n    error: bool = Field(default=False)\n    message: Optional[str] = Field(default=None)\n\n    def __bool__(self):\n        return self.result is not None\n</code></pre> <p>Notice that <code>MaybeUser</code> has a <code>result</code> field that is an optional <code>UserDetail</code> instance where the extracted data will be stored. The <code>error</code> field is a boolean that indicates whether an error occurred, and the <code>message</code> field is an optional string that contains the error message.</p>"},{"location":"concepts/maybe/#defining-the-function","title":"Defining the function","text":"<p>Once we have the model defined, we can create a function that uses the <code>Maybe</code> pattern to extract the data.</p> <pre><code>import instructor\nimport openai\nfrom pydantic import BaseModel, Field\nfrom typing import Optional\n\n# This enables the `response_model` keyword\nclient = instructor.patch(openai.OpenAI())\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Optional[str] = Field(default=None)\n\n\nclass MaybeUser(BaseModel):\n    result: Optional[UserDetail] = Field(default=None)\n    error: bool = Field(default=False)\n    message: Optional[str] = Field(default=None)\n\n    def __bool__(self):\n        return self.result is not None\n\n\ndef extract(content: str) -&gt; MaybeUser:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=MaybeUser,\n        messages=[\n            {\"role\": \"user\", \"content\": f\"Extract `{content}`\"},\n        ],\n    )\n\n\nuser1 = extract(\"Jason is a 25-year-old scientist\")\nprint(user1.model_dump_json(indent=2))\n\"\"\"\n{\n  \"result\": {\n    \"age\": 25,\n    \"name\": \"Jason\",\n    \"role\": \"scientist\"\n  },\n  \"error\": false,\n  \"message\": null\n}\n\"\"\"\n\nuser2 = extract(\"Unknown user\")\nprint(user2.model_dump_json(indent=2))\n\"\"\"\n{\n  \"result\": null,\n  \"error\": false,\n  \"message\": \"Unknown user\"\n}\n\"\"\"\n</code></pre> <p>As you can see, when the data is extracted successfully, the <code>result</code> field contains the <code>UserDetail</code> instance. When an error occurs, the <code>error</code> field is set to <code>True</code>, and the <code>message</code> field contains the error message.</p> <p>If you want to learn more about pattern matching, check out Pydantic's docs on Structural Pattern Matching</p>"},{"location":"concepts/models/","title":"Response Model","text":"<p>Defining LLM output schemas in Pydantic is done via <code>pydantic.BaseModel</code>. To learn more about models in Pydantic, check out their documentation.</p> <p>After defining a Pydantic model, we can use it as the <code>response_model</code> in your client <code>create</code> calls to OpenAI or any other supported model. The job of the <code>response_model</code> parameter is to:</p> <ul> <li>Define the schema and prompts for the language model</li> <li>Validate the response from the API</li> <li>Return a Pydantic model instance.</li> </ul>"},{"location":"concepts/models/#prompting","title":"Prompting","text":"<p>When defining a response model, we can use docstrings and field annotations to define the prompt that will be used to generate the response.</p> <pre><code>from pydantic import BaseModel, Field\n\n\nclass User(BaseModel):\n    \"\"\"\n    This is the prompt that will be used to generate the response.\n    Any instructions here will be passed to the language model.\n    \"\"\"\n\n    name: str = Field(description=\"The name of the user.\")\n    age: int = Field(description=\"The age of the user.\")\n</code></pre> <p>Here all docstrings, types, and field annotations will be used to generate the prompt. The prompt will be generated by the <code>create</code> method of the client and will be used to generate the response.</p>"},{"location":"concepts/models/#optional-values","title":"Optional Values","text":"<p>If we use <code>Optional</code> and <code>default</code>, they will be considered not required when sent to the language model</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import Optional\n\n\nclass User(BaseModel):\n    name: str = Field(description=\"The name of the user.\")\n    age: int = Field(description=\"The age of the user.\")\n    email: Optional[str] = Field(description=\"The email of the user.\", default=None)\n</code></pre>"},{"location":"concepts/models/#dynamic-model-creation","title":"Dynamic model creation","text":"<p>There are some occasions where it is desirable to create a model using runtime information to specify the fields. For this, Pydantic provides the create_model function to allow models to be created on the fly:</p> <pre><code>from pydantic import BaseModel, create_model\n\n\nclass FooModel(BaseModel):\n    foo: str\n    bar: int = 123\n\n\nBarModel = create_model(\n    'BarModel',\n    apple=(str, 'russet'),\n    banana=(str, 'yellow'),\n    __base__=FooModel,\n)\nprint(BarModel)\n#&gt; &lt;class '__main__.BarModel'&gt;\nprint(BarModel.model_fields.keys())\n#&gt; dict_keys(['foo', 'bar', 'apple', 'banana'])\n</code></pre> When would I use this? <p>Consider a situation where the model is dynamically defined, based on some configuration or database. For example, we could have a database table that stores the properties of a model for some model name or id. We could then query the database for the properties of the model and use that to create the model.</p> <pre><code>SELECT property_name, property_type, description\nFROM prompt\nWHERE model_name = {model_name}\n</code></pre> <p>We can then use this information to create the model.</p> <pre><code>from pydantic import BaseModel, create_model\nfrom typing import List\n\ntypes = {\n    'string': str,\n    'integer': int,\n    'boolean': bool,\n    'number': float,\n    'List[str]': List[str],\n}\n\n# Mocked cursor.fetchall()\ncursor = [\n    ('name', 'string', 'The name of the user.'),\n    ('age', 'integer', 'The age of the user.'),\n    ('email', 'string', 'The email of the user.'),\n]\n\nBarModel = create_model(\n    'User',\n    **{\n        property_name: (types[property_type], description)\n        for property_name, property_type, description in cursor\n    },\n    __base__=BaseModel,\n)\n\nprint(BarModel.model_json_schema())\n\"\"\"\n{\n    'properties': {\n        'name': {'default': 'The name of the user.', 'title': 'Name', 'type': 'string'},\n        'age': {'default': 'The age of the user.', 'title': 'Age', 'type': 'integer'},\n        'email': {\n            'default': 'The email of the user.',\n            'title': 'Email',\n            'type': 'string',\n        },\n    },\n    'title': 'User',\n    'type': 'object',\n}\n\"\"\"\n</code></pre> <p>This would be useful when different users have different descriptions for the same model. We can use the same model but have different prompts for each user.</p>"},{"location":"concepts/models/#adding-behavior","title":"Adding Behavior","text":"<p>We can add methods to our Pydantic models, just as any plain Python class. We might want to do this to add some custom logic to our models.</p> <pre><code>from pydantic import BaseModel\nfrom typing import Literal\n\nfrom openai import OpenAI\n\nimport instructor\n\nclient = instructor.patch(OpenAI())\n\n\nclass SearchQuery(BaseModel):\n    query: str\n    query_type: Literal[\"web\", \"image\", \"video\"]\n\n    def execute(self):\n        print(f\"Searching for {self.query} of type {self.query_type}\")\n        #&gt; Searching for cat of type image\n        return \"Results for cat\"\n\n\nquery = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[{\"role\": \"user\", \"content\": \"Search for a picture of a cat\"}],\n    response_model=SearchQuery,\n)\n\nresults = query.execute()\nprint(results)\n#&gt; Results for cat\n</code></pre> <p>Now we can call <code>execute</code> on our model instance after extracting it from a language model. If you want to see more examples of this checkout our post on RAG is more than embeddings</p>"},{"location":"concepts/parallel/","title":"Parallel Tools","text":"<p>One of the latest capabilities that OpenAI has recently introduced is parallel function calling. To learn more you can read up on this</p> <p>Experimental Feature</p> <p>This feature is currently in preview and is subject to change. only supported by the <code>gpt-4-turbo-preview</code> model.</p>"},{"location":"concepts/parallel/#understanding-parallel-function-calling","title":"Understanding Parallel Function Calling","text":"<p>By using parallel function callings that allow you to call multiple functions in a single request, you can significantly reduce the latency of your application without having to use tricks with now one builds a schema.</p> <pre><code>import openai\nimport instructor\n\nfrom typing import Iterable, Literal\nfrom pydantic import BaseModel\n\n\nclass Weather(BaseModel):\n    location: str\n    units: Literal[\"imperial\", \"metric\"]\n\n\nclass GoogleSearch(BaseModel):\n    query: str\n\n\nclient = instructor.patch(openai.OpenAI(), mode=instructor.Mode.PARALLEL_TOOLS)  # (1)!\n\nfunction_calls = client.chat.completions.create(\n    model=\"gpt-4-turbo-preview\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You must always use tools\"},\n        {\n            \"role\": \"user\",\n            \"content\": \"What is the weather in toronto and dallas and who won the super bowl?\",\n        },\n    ],\n    response_model=Iterable[Weather | GoogleSearch],  # (2)!\n)\n\nfor fc in function_calls:\n    print(fc)\n    #&gt; location='Toronto' units='metric'\n    #&gt; location='Dallas' units='imperial'\n    #&gt; query='super bowl winner'\n</code></pre> <ol> <li>Set the mode to <code>PARALLEL_TOOLS</code> to enable parallel function calling.</li> <li>Set the response model to <code>Iterable[Weather | GoogleSearch]</code> to indicate that the response will be a list of <code>Weather</code> and <code>GoogleSearch</code> objects. This is necessary because the response will be a list of objects, and we need to specify the types of the objects in the list.</li> </ol> <p>Noticed that the <code>response_model</code> Must be in the form <code>Iterable[Type1 | Type2 | ...]</code> or <code>Iterable[Type1]</code> where <code>Type1</code> and <code>Type2</code> are the types of the objects that will be returned in the response.</p>"},{"location":"concepts/partial/","title":"Streaming Partial Responses","text":"<p>Field level streaming provides incremental snapshots of the current state of the response model that are immediately useable. This approach is particularly relevant in contexts like rendering UI components.</p> <p>Instructor supports this pattern by making use of <code>Partial[T]</code>. This lets us dynamically create a new class that treats all of the original model's fields as <code>Optional</code>.</p>"},{"location":"concepts/partial/#understanding-partial-responses","title":"Understanding Partial Responses","text":"<p>Consider what happens whene we define a response model:</p> <pre><code>from pydantic import BaseModel\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n</code></pre> <p>If we streamed json out from OpenAI, we would only be able to parse when the object is completed returned!</p> <pre><code>{\"name\": \"Jo\n{\"name\": \"John\", \"ag\n{\"name\": \"John\", \"age\":\n{\"name\": \"John\", \"age\": 25} # Completed\n</code></pre> <p>When specifying a <code>Partial[T]</code> and setting <code>stream=True</code>, the response from <code>instructor</code> becomes a <code>Generator[T]</code>. As the generator yields results, you can iterate over these incremental updates. The last value yielded by the generator represents the completed extraction!</p> <pre><code>{\"name\": \"Jo                 =&gt; User(name=\"Jo\", age=None)\n{\"name\": \"John\", \"ag         =&gt; User(name=\"John\", age=None)\n{\"name\": \"John\", \"age:       =&gt; User(name=\"John\", age=None)\n{\"name\": \"John\", \"age\": 25}  =&gt; User(name=\"John\", age=25)\n</code></pre> <p>Limited Validator Support</p> <p>Fewer validators are supported by <code>Partial</code> response models as streamed fields will natural raise validation error, as we do not have a strong opinoin on how to handle them.</p> <p>Let's look at an example of streaming an extraction of conference information, that would be used to stream in an react component.</p> <pre><code>import instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel\nfrom typing import List\nfrom rich.console import Console\n\nclient = instructor.patch(OpenAI())\n\ntext_block = \"\"\"\nIn our recent online meeting, participants from various backgrounds joined to discuss the upcoming tech conference. The names and contact details of the participants were as follows:\n\n- Name: John Doe, Email: johndoe@email.com, Twitter: @TechGuru44\n- Name: Jane Smith, Email: janesmith@email.com, Twitter: @DigitalDiva88\n- Name: Alex Johnson, Email: alexj@email.com, Twitter: @CodeMaster2023\n\nDuring the meeting, we agreed on several key points. The conference will be held on March 15th, 2024, at the Grand Tech Arena located at 4521 Innovation Drive. Dr. Emily Johnson, a renowned AI researcher, will be our keynote speaker.\n\nThe budget for the event is set at $50,000, covering venue costs, speaker fees, and promotional activities. Each participant is expected to contribute an article to the conference blog by February 20th.\n\nA follow-up meetingis scheduled for January 25th at 3 PM GMT to finalize the agenda and confirm the list of speakers.\n\"\"\"\n\n\nclass User(BaseModel):\n    name: str\n    email: str\n    twitter: str\n\n\nclass MeetingInfo(BaseModel):\n    users: List[User]\n    date: str\n    location: str\n    budget: int\n    deadline: str\n\n\nextraction_stream = client.chat.completions.create(\n    model=\"gpt-4\",\n    response_model=instructor.Partial[MeetingInfo],\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": f\"Get the information about the meeting and the users {text_block}\",\n        },\n    ],\n    stream=True,\n)\n\n\nconsole = Console()\n\nfor extraction in extraction_stream:\n    obj = extraction.model_dump()\n    console.clear()\n    console.print(obj)\n\nprint(extraction.model_dump_json())\n\"\"\"\n{\"users\":[{\"name\":\"John Doe\",\"email\":\"johndoe@email.com\",\"twitter\":\"@TechGuru44\"},{\"name\":\"Jane Smith\",\"email\":\"janesmith@email.com\",\"twitter\":\"@DigitalDiva88\"},{\"name\":\"Alex Johnson\",\"email\":\"alexj@email.com\",\"twitter\":\"@CodeMaster2023\"}],\"date\":\"2024-03-15\",\"location\":\"Grand Tech Arena located at 4521 Innovation Drive\",\"budget\":50000,\"deadline\":\"2024-02-20\"}\n\"\"\"\n</code></pre> <p>This will output the following:</p> <p></p>"},{"location":"concepts/partial/#asynchronous-streaming","title":"Asynchronous Streaming","text":"<p>I also just want to call out in this example that <code>instructor</code> also supports asynchronous streaming. This is useful when you want to stream a response model and process the results as they come in, but you'll need to use the <code>async for</code> syntax to iterate over the results.</p> <pre><code>import instructor\nfrom openai import AsyncOpenAI\nfrom pydantic import BaseModel\n\nclient = instructor.apatch(AsyncOpenAI())\n\n\nclass UserExtract(BaseModel):\n    name: str\n    age: int\n\n\nasync def print_partial_results():\n    user = await client.chat.completions.create(\n        model=\"gpt-4-turbo-preview\",\n        response_model=instructor.Partial[UserExtract],\n        max_retries=2,\n        stream=True,\n        messages=[\n            {\"role\": \"user\", \"content\": \"Jason Liu is 12 years old\"},\n        ],\n    )\n    async for m in user:\n        print(m.model_dump_json(indent=2))\n        \"\"\"\n        {\n          \"name\": null,\n          \"age\": null\n        }\n        \"\"\"\n        \"\"\"\n        {\n          \"name\": \"\",\n          \"age\": null\n        }\n        \"\"\"\n        \"\"\"\n        {\n          \"name\": \"Jason\",\n          \"age\": null\n        }\n        \"\"\"\n        \"\"\"\n        {\n          \"name\": \"Jason Liu\",\n          \"age\": null\n        }\n        \"\"\"\n        \"\"\"\n        {\n          \"name\": \"Jason Liu\",\n          \"age\": 12\n        }\n        \"\"\"\n        \"\"\"\n        {\n          \"name\": \"\",\n          \"age\": null\n        }\n        \"\"\"\n        \"\"\"\n        {\n          \"name\": \"Jason\",\n          \"age\": null\n        }\n        \"\"\"\n        \"\"\"\n        {\n          \"name\": \"Jason Liu\",\n          \"age\": null\n        }\n        \"\"\"\n        \"\"\"\n        {\n          \"name\": \"Jason Liu\",\n          \"age\": 12\n        }\n        \"\"\"\n        \"\"\"\n        {\n          \"name\": \"\",\n          \"age\": null\n        }\n        \"\"\"\n        \"\"\"\n        {\n          \"name\": \"Jason\",\n          \"age\": null\n        }\n        \"\"\"\n        \"\"\"\n        {\n          \"name\": \"Jason Liu\",\n          \"age\": null\n        }\n        \"\"\"\n        \"\"\"\n        {\n          \"name\": \"Jason Liu\",\n          \"age\": 12\n        }\n        \"\"\"\n        \"\"\"\n        {\n          \"name\": \"\",\n          \"age\": null\n        }\n        \"\"\"\n        \"\"\"\n        {\n          \"name\": \"Jason\",\n          \"age\": null\n        }\n        \"\"\"\n        \"\"\"\n        {\n          \"name\": \"Jason Liu\",\n          \"age\": null\n        }\n        \"\"\"\n        \"\"\"\n        {\n          \"name\": \"Jason Liu\",\n          \"age\": 12\n        }\n        \"\"\"\n\n\nimport asyncio\n\nasyncio.run(print_partial_results())\n</code></pre>"},{"location":"concepts/patching/","title":"Patching","text":"<p>Instructor enhances client functionality with three new keywords for backwards compatibility. This allows use of the enhanced client as usual, with structured output benefits.</p> <ul> <li><code>response_model</code>: Defines the response type for <code>chat.completions.create</code>.</li> <li><code>max_retries</code>: Determines retry attempts for failed <code>chat.completions.create</code> validations.</li> <li><code>validation_context</code>: Provides extra context to the validation process.</li> </ul> <p>There are three methods for structured output:</p> <ol> <li>Function Calling: The primary method. Use this for stability and testing.</li> <li>Tool Calling: Useful in specific scenarios; lacks the reasking feature of OpenAI's tool calling API.</li> <li>JSON Mode: Offers closer adherence to JSON but with more potential validation errors. Suitable for specific non-function calling clients.</li> </ol>"},{"location":"concepts/patching/#function-calling","title":"Function Calling","text":"<pre><code>import instructor\nfrom openai import OpenAI\n\nclient = instructor.patch(OpenAI(), mode=instructor.Mode.FUNCTIONS)\n</code></pre>"},{"location":"concepts/patching/#tool-calling","title":"Tool Calling","text":"<pre><code>import instructor\nfrom openai import OpenAI\n\nclient = instructor.patch(OpenAI(), mode=instructor.Mode.TOOLS)\n</code></pre>"},{"location":"concepts/patching/#json-mode","title":"JSON Mode","text":"<pre><code>import instructor\nfrom instructor import Mode\nfrom openai import OpenAI\n\nclient = instructor.patch(OpenAI(), mode=Mode.JSON)\n</code></pre>"},{"location":"concepts/patching/#markdown-json-mode","title":"Markdown JSON Mode","text":"<p>Experimental</p> <p>This is not recommended, and may not be supported in the future, this is just left to support vision models.</p> <pre><code>import instructor\nfrom openai import OpenAI\n\nclient = instructor.patch(OpenAI(), mode=instructor.Mode.MD_JSON)\n</code></pre>"},{"location":"concepts/patching/#schema-integration","title":"Schema Integration","text":"<p>In JSON Mode, the schema is part of the system message:</p> <pre><code>import instructor\nfrom openai import OpenAI\n\nclient = instructor.patch(OpenAI())\n\n\nclass UserExtract(instructor.OpenAISchema):\n    name: str\n    age: int\n\n\nresponse = client.chat.completions.create(\n    model=\"gpt-3.5-turbo-1106\",\n    response_format={\"type\": \"json_object\"},\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": f\"Match your response to this json_schema: \\n{UserExtract.model_json_schema()['properties']}\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract jason is 25 years old\",\n        },\n    ],\n)\nuser = UserExtract.from_response(response, mode=instructor.Mode.JSON)\nprint(user)\n#&gt; name='Jason' age=25\n</code></pre>"},{"location":"concepts/philosophy/","title":"Philosophy","text":"<p>The instructor values simplicity and flexibility in leveraging language models (LLMs). It offers a streamlined approach for structured output, avoiding unnecessary dependencies or complex abstractions. Let Pydantic do the heavy lifting.</p> <p>\u201cSimplicity is a great virtue but it requires hard work to achieve it and education to appreciate it. And to make matters worse: complexity sells better.\u201d \u2014 Edsger Dijkstra</p>"},{"location":"concepts/philosophy/#the-bridge-to-object-oriented-programming","title":"The Bridge to Object-Oriented Programming","text":"<p><code>instructor</code> acts as a bridge converting text-based LLM interactions into a familiar object-oriented format. Its integration with Pydantic provides type hints, runtime validation, and robust IDE support; love and supported by many in the Python ecosystem. By treating LLMs as callable functions returning typed objects, instructor makes language models backwards compatible with code, making them practical for everyday use while being complex enough for advanced applications.</p>"},{"location":"concepts/philosophy/#the-zen-of-instructor","title":"The zen of <code>instructor</code>","text":"<p>Maintain the flexibility and power of Python, without unnecessary constraints.</p> <p>Begin with a function and a return type hint \u2013 simplicity is key. With my experience maintaining a large enterprize framework at my previous job over many years I've learned that the goal of a making a useful framework is minimizing regret, both for the author and hopefully for the user.</p> <ol> <li>Define a Schema <code>class StructuredData(BaseModel):</code></li> <li>Define validators and methods on your schema.</li> <li>Encapsulate all your LLM logic into a function <code>def extract(a) -&gt; StructuredData:</code></li> <li>Define typed computations against your data with <code>def compute(data: StructuredData):</code> or call methods on your schema <code>data.compute()</code></li> </ol> <p>It should be that simple.</p>"},{"location":"concepts/philosophy/#my-goals","title":"My Goals","text":"<p>The goal for the library, documentation, and blog, is to help you be a better python programmer and as a result a better AI engineer.</p> <ul> <li>The library is a result of my desire for simplicity.</li> <li>The library should help maintain simplicity in your codebase.</li> <li>I won't try to write prompts for you,</li> <li>I don't try to create indirections or abstractions that make it hard to debug in the future</li> </ul> <p>Please note that the library is designed to be adaptable and open-ended, allowing you to customize and extend its functionality based on your specific requirements. If you have any further questions or ideas hit me up on twitter</p> <p>Cheers!</p>"},{"location":"concepts/prompting/","title":"General Tips for Prompt Engineering","text":"<p>The overarching theme of using Instructor and Pydantic for function calling is to make the models as self-descriptive, modular, and flexible as possible, while maintaining data integrity and ease of use.</p> <ul> <li>Modularity: Design self-contained components for reuse.</li> <li>Self-Description: Use Pydantic's <code>Field</code> for clear field descriptions.</li> <li>Optionality: Use Python's <code>Optional</code> type for nullable fields and set sensible defaults.</li> <li>Standardization: Employ enumerations for fields with a fixed set of values; include a fallback option.</li> <li>Dynamic Data: Use key-value pairs for arbitrary properties and limit list lengths.</li> <li>Entity Relationships: Define explicit identifiers and relationship fields.</li> <li>Contextual Logic: Optionally add a \"chain of thought\" field in reusable components for extra context.</li> </ul>"},{"location":"concepts/prompting/#modular-chain-of-thought","title":"Modular Chain of Thought","text":"<p>This approach to \"chain of thought\" improves data quality but can have modular components rather than global CoT.</p> <pre><code>from pydantic import BaseModel, Field\n\n\nclass Role(BaseModel):\n    chain_of_thought: str = Field(\n        ..., description=\"Think step by step to determine the correct title\"\n    )\n    title: str\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Role\n</code></pre>"},{"location":"concepts/prompting/#utilize-optional-attributes","title":"Utilize Optional Attributes","text":"<p>Use Python's Optional type and set a default value to prevent undesired defaults like empty strings.</p> <pre><code>from typing import Optional\nfrom pydantic import BaseModel, Field\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Optional[str] = Field(default=None)\n</code></pre>"},{"location":"concepts/prompting/#handling-errors-within-function-calls","title":"Handling Errors Within Function Calls","text":"<p>You can create a wrapper class to hold either the result of an operation or an error message. This allows you to remain within a function call even if an error occurs, facilitating better error handling without breaking the code flow.</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import Optional\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Optional[str] = Field(default=None)\n\n\nclass MaybeUser(BaseModel):\n    result: Optional[UserDetail] = Field(default=None)\n    error: bool = Field(default=False)\n    message: Optional[str]\n\n    def __bool__(self):\n        return self.result is not None\n</code></pre> <p>With the <code>MaybeUser</code> class, you can either receive a <code>UserDetail</code> object in result or get an error message in message.</p>"},{"location":"concepts/prompting/#simplification-with-the-maybe-pattern","title":"Simplification with the Maybe Pattern","text":"<p>You can further simplify this using instructor to create the <code>Maybe</code> pattern dynamically from any <code>BaseModel</code>.</p> <pre><code>import instructor\nfrom pydantic import BaseModel\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n\n\nMaybeUser = instructor.Maybe(UserDetail)\n</code></pre> <p>This allows you to quickly create a Maybe type for any class, streamlining the process.</p>"},{"location":"concepts/prompting/#tips-for-enumerations","title":"Tips for Enumerations","text":"<p>To prevent data misalignment, use Enums for standardized fields. Always include an \"Other\" option as a fallback so the model can signal uncertainty.</p> <pre><code>from enum import Enum, auto\nfrom pydantic import BaseModel, Field\n\n\nclass Role(Enum):\n    PRINCIPAL = auto()\n    TEACHER = auto()\n    STUDENT = auto()\n    OTHER = auto()\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Role = Field(\n        description=\"Correctly assign one of the predefined roles to the user.\"\n    )\n</code></pre> <p>If you're having a hard time with <code>Enum</code> and alternative is to use <code>Literal</code></p> <pre><code>from typing import Literal\nfrom pydantic import BaseModel\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Literal[\"PRINCIPAL\", \"TEACHER\", \"STUDENT\", \"OTHER\"]\n</code></pre> <p>If you'd like to improve performance more you can reiterate the requirements in the field descriptions or in the docstrings.</p>"},{"location":"concepts/prompting/#reiterate-long-instructions","title":"Reiterate Long Instructions","text":"<p>For complex attributes, it helps to reiterate the instructions in the field's description.</p> <pre><code>from pydantic import BaseModel, Field\n\n\nclass Role(BaseModel):\n    \"\"\"\n    Extract the role based on the following rules ...\n    \"\"\"\n\n    instructions: str = Field(\n        ...,\n        description=\"Restate the instructions and rules to correctly determine the title.\",\n    )\n    title: str\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Role\n</code></pre>"},{"location":"concepts/prompting/#handle-arbitrary-properties","title":"Handle Arbitrary Properties","text":"<p>When you need to extract undefined attributes, use a list of key-value pairs.</p> <pre><code>from typing import List\nfrom pydantic import BaseModel, Field\n\n\nclass Property(BaseModel):\n    key: str\n    value: str\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    properties: List[Property] = Field(\n        ..., description=\"Extract any other properties that might be relevant.\"\n    )\n</code></pre>"},{"location":"concepts/prompting/#limiting-the-length-of-lists","title":"Limiting the Length of Lists","text":"<p>When dealing with lists of attributes, especially arbitrary properties, it's crucial to manage the length. You can use prompting and enumeration to limit the list length, ensuring a manageable set of properties.</p> <pre><code>from typing import List\nfrom pydantic import BaseModel, Field\n\n\nclass Property(BaseModel):\n    index: str = Field(..., description=\"Monotonically increasing ID\")\n    key: str\n    value: str\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    properties: List[Property] = Field(\n        ...,\n        description=\"Numbered list of arbitrary extracted properties, should be less than 6\",\n    )\n</code></pre> <p>Using Tuples for Simple Types</p> <p>For simple types, tuples can be a more compact alternative to custom classes, especially when the properties don't require additional descriptions.</p> <pre><code>from typing import List, Tuple\nfrom pydantic import BaseModel, Field\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    properties: List[Tuple[int, str]] = Field(\n        ...,\n        description=\"Numbered list of arbitrary extracted properties, should be less than 6\",\n    )\n</code></pre>"},{"location":"concepts/prompting/#advanced-arbitrary-properties","title":"Advanced Arbitrary Properties","text":"<p>For multiple users, aim to use consistent key names when extracting properties.</p> <pre><code>from typing import List\nfrom pydantic import BaseModel\n\n\nclass UserDetail(BaseModel):\n    id: int\n    age: int\n    name: str\n\n\nclass UserDetails(BaseModel):\n    \"\"\"\n    Extract information for multiple users.\n    Use consistent key names for properties across users.\n    \"\"\"\n\n    users: List[UserDetail]\n</code></pre> <p>This refined guide should offer a cleaner and more organized approach to structure engineering in Python.</p>"},{"location":"concepts/prompting/#defining-relationships-between-entities","title":"Defining Relationships Between Entities","text":"<p>In cases where relationships exist between entities, it's vital to define them explicitly in the model. The following example demonstrates how to define relationships between users by incorporating an id and a friends field:</p> <pre><code>from typing import List\nfrom pydantic import BaseModel, Field\n\n\nclass UserDetail(BaseModel):\n    id: int = Field(..., description=\"Unique identifier for each user.\")\n    age: int\n    name: str\n    friends: List[int] = Field(\n        ...,\n        description=\"Correct and complete list of friend IDs, representing relationships between users.\",\n    )\n\n\nclass UserRelationships(BaseModel):\n    users: List[UserDetail] = Field(\n        ...,\n        description=\"Collection of users, correctly capturing the relationships among them.\",\n    )\n</code></pre>"},{"location":"concepts/prompting/#reusing-components-with-different-contexts","title":"Reusing Components with Different Contexts","text":"<p>You can reuse the same component for different contexts within a model. In this example, the TimeRange component is used for both work_time and leisure_time.</p> <pre><code>from pydantic import BaseModel, Field\n\n\nclass TimeRange(BaseModel):\n    start_time: int = Field(..., description=\"The start time in hours.\")\n    end_time: int = Field(..., description=\"The end time in hours.\")\n\n\nclass UserDetail(BaseModel):\n    id: int = Field(..., description=\"Unique identifier for each user.\")\n    age: int\n    name: str\n    work_time: TimeRange = Field(\n        ..., description=\"Time range during which the user is working.\"\n    )\n    leisure_time: TimeRange = Field(\n        ..., description=\"Time range reserved for leisure activities.\"\n    )\n</code></pre> <p>Sometimes, a component like TimeRange may require some context or additional logic to be used effectively. Employing a \"chain of thought\" field within the component can help in understanding or optimizing the time range allocations.</p> <pre><code>from pydantic import BaseModel, Field\n\n\nclass TimeRange(BaseModel):\n    chain_of_thought: str = Field(\n        ..., description=\"Step by step reasoning to get the correct time range\"\n    )\n    start_time: int = Field(..., description=\"The start time in hours.\")\n    end_time: int = Field(..., description=\"The end time in hours.\")\n</code></pre>"},{"location":"concepts/raw_response/","title":"Raw Response","text":"<p>Often times not only do you want the base model but may also want the original response from the API. You can do this by retrieving the <code>raw_response</code>, since the <code>raw_response</code> is also a pydantic model, you can use any of the pydantic model methods on it.</p> <pre><code>import instructor\n\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\nclient = instructor.patch(OpenAI())\n\n\nclass UserExtract(BaseModel):\n    name: str\n    age: int\n\n\nuser: UserExtract = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserExtract,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"},\n    ],\n)\n\nprint(user._raw_response)\n\"\"\"\nChatCompletion(\n    id='chatcmpl-8pOAsSOIHAmngMBBki3BLN3p552L0',\n    choices=[\n        Choice(\n            finish_reason='stop',\n            index=0,\n            logprobs=None,\n            message=ChatCompletionMessage(\n                content=None,\n                role='assistant',\n                function_call=FunctionCall(\n                    arguments='{\\n  \"name\": \"Jason\",\\n  \"age\": 25\\n}',\n                    name='UserExtract',\n                ),\n                tool_calls=None,\n            ),\n        )\n    ],\n    created=1707258346,\n    model='gpt-3.5-turbo-0613',\n    object='chat.completion',\n    system_fingerprint=None,\n    usage=CompletionUsage(completion_tokens=16, prompt_tokens=73, total_tokens=89),\n)\n\"\"\"\n</code></pre> <p>Accessing tokens usage</p> <p>This is the recommended way to access the tokens usage, since it is a pydantic model you can use any of the pydantic model methods on it. For example, you can access the <code>total_tokens</code> by doing <code>user._raw_response.usage.total_tokens</code>. Note that this also includes the tokens used during any previous unsuccessful attempts.</p> <p>In the future, we may add additional hooks to the <code>raw_response</code> to make it easier to access the tokens usage.</p>"},{"location":"concepts/reask_validation/","title":"Validation and Reasking","text":"<p>Instead of framing \"self-critique\" or \"self-reflection\" in AI as new concepts, we can view them as validation errors with clear error messages that the system can use to self-correct.</p>"},{"location":"concepts/reask_validation/#pydantic","title":"Pydantic","text":"<p>Pydantic offers an customizable and expressive validation framework for Python. Instructor leverages Pydantic's validation framework to provide a uniform developer experience for both code-based and LLM-based validation, as well as a reasking mechanism for correcting LLM outputs based on validation errors. To learn more check out the Pydantic docs on validators.</p> <p>Good llm validation is just good validation</p> <p>If you want to see some more examples on validators checkout our blog post Good LLM validation is just good validation</p>"},{"location":"concepts/reask_validation/#code-based-validation-example","title":"Code-based Validation Example","text":"<p>First define a Pydantic model with a validator using the <code>Annotation</code> class from <code>typing_extensions</code>.</p> <p>Enforce a naming rule using Pydantic's built-in validation:</p> <pre><code>from pydantic import BaseModel, ValidationError\nfrom typing_extensions import Annotated\nfrom pydantic import AfterValidator\n\n\ndef name_must_contain_space(v: str) -&gt; str:\n    if \" \" not in v:\n        raise ValueError(\"Name must contain a space.\")\n    return v.lower()\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: Annotated[str, AfterValidator(name_must_contain_space)]\n\n\ntry:\n    person = UserDetail(age=29, name=\"Jason\")\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for UserDetail\n    name\n      Value error, Name must contain a space. [type=value_error, input_value='Jason', input_type=str]\n        For further information visit https://errors.pydantic.dev/2.6/v/value_error\n    \"\"\"\n</code></pre>"},{"location":"concepts/reask_validation/#output-for-code-based-validation","title":"Output for Code-Based Validation","text":"<pre><code>1 validation error for UserDetail\nname\n   Value error, name must contain a space (type=value_error)\n</code></pre> <p>As we can see, Pydantic raises a validation error when the name attribute does not contain a space. This is a simple example, but it demonstrates how Pydantic can be used to validate attributes of a model.</p>"},{"location":"concepts/reask_validation/#llm-based-validation-example","title":"LLM-Based Validation Example","text":"<p>LLM-based validation can also be plugged into the same Pydantic model. Here, if the answer attribute contains content that violates the rule \"don't say objectionable things,\" Pydantic will raise a validation error.</p> <pre><code>import instructor\n\nfrom openai import OpenAI\nfrom instructor import llm_validator\nfrom pydantic import BaseModel, ValidationError, BeforeValidator\nfrom typing_extensions import Annotated\n\n# Apply the patch to the OpenAI client\nclient = instructor.patch(OpenAI())\n\n\nclass QuestionAnswer(BaseModel):\n    question: str\n    answer: Annotated[\n        str,\n        BeforeValidator(\n            llm_validator(\"don't say objectionable things\", openai_client=client)\n        ),\n    ]\n\n\ntry:\n    qa = QuestionAnswer(\n        question=\"What is the meaning of life?\",\n        answer=\"The meaning of life is to be evil and steal\",\n    )\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for QuestionAnswer\n    answer\n      Assertion failed, The statement promotes objectionable behavior. [type=assertion_error, input_value='The meaning of life is to be evil and steal', input_type=str]\n        For further information visit https://errors.pydantic.dev/2.6/v/assertion_error\n    \"\"\"\n</code></pre>"},{"location":"concepts/reask_validation/#output-for-llm-based-validation","title":"Output for LLM-Based Validation","text":"<p>It is important to not here that the error message is generated by the LLM, not the code, so it'll be helpful for re asking the model.</p> <pre><code>1 validation error for QuestionAnswer\nanswer\n   Assertion failed, The statement is objectionable. (type=assertion_error)\n</code></pre>"},{"location":"concepts/reask_validation/#using-reasking-logic-to-correct-outputs","title":"Using Reasking Logic to Correct Outputs","text":"<p>Validators are a great tool for ensuring some property of the outputs. When you use the <code>patch()</code> method with the <code>openai</code> client, you can use the <code>max_retries</code> parameter to set the number of times you can reask the model to correct the output.</p> <p>It is a great layer of defense against bad outputs of two forms:</p> <ol> <li>Pydantic Validation Errors (code or llm based)</li> <li>JSON Decoding Errors (when the model returns a bad response)</li> </ol>"},{"location":"concepts/reask_validation/#step-1-define-the-response-model-with-validators","title":"Step 1: Define the Response Model with Validators","text":"<p>Notice that the field validator wants the name in uppercase, but the user input is lowercase. The validator will raise a <code>ValueError</code> if the name is not in uppercase.</p> <pre><code>import openai\nimport instructor\nfrom pydantic import BaseModel, field_validator\n\n# Apply the patch to the OpenAI client\nclient = instructor.patch(openai.OpenAI())\n\n\nclass UserDetails(BaseModel):\n    name: str\n    age: int\n\n    @field_validator(\"name\")\n    @classmethod\n    def validate_name(cls, v):\n        if v.upper() != v:\n            raise ValueError(\"Name must be in uppercase.\")\n        return v\n</code></pre>"},{"location":"concepts/reask_validation/#step-2-using-the-client-with-retries","title":"Step 2. Using the Client with Retries","text":"<p>Here, the <code>UserDetails</code> model is passed as the <code>response_model</code>, and <code>max_retries</code> is set to 2.</p> <pre><code>import instructor\nimport openai\nfrom pydantic import BaseModel\n\nclient = instructor.patch(openai.OpenAI(), mode=instructor.Mode.TOOLS)\n\n\nclass UserDetails(BaseModel):\n    name: str\n    age: int\n\n\nmodel = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserDetails,\n    max_retries=2,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"},\n    ],\n)\n\nprint(model.model_dump_json(indent=2))\n\"\"\"\n{\n  \"name\": \"Jason\",\n  \"age\": 25\n}\n\"\"\"\n</code></pre>"},{"location":"concepts/reask_validation/#what-happens-behind-the-scenes","title":"What happens behind the scenes?","text":"<p>Behind the scenes, the <code>instructor.patch()</code> method adds a <code>max_retries</code> parameter to the <code>openai.ChatCompletion.create()</code> method. The <code>max_retries</code> parameter will trigger up to 2 reattempts if the <code>name</code> attribute fails the uppercase validation in <code>UserDetails</code>.</p> <pre><code>from pydantic import ValidationError\n\n\ntry:\n    ...\nexcept ValidationError as e:\n    kwargs[\"messages\"].append(response.choices[0].message)\n    kwargs[\"messages\"].append(\n        {\n            \"role\": \"user\",\n            \"content\": f\"Please correct the function call; errors encountered:\\n{e}\",\n        }\n    )\n</code></pre>"},{"location":"concepts/reask_validation/#advanced-validation-techniques","title":"Advanced Validation Techniques","text":"<p>The docs are currently incomplete, but we have a few advanced validation techniques that we're working on documenting better such as model level validation, and using a validation context. Check out our example on verifying citations which covers:</p> <ol> <li>Validate the entire object with all attributes rather than one attribute at a time</li> <li>Using some 'context' to validate the object: In this case, we use the <code>context</code> to check if the citation existed in the original text.</li> </ol>"},{"location":"concepts/reask_validation/#takeaways","title":"Takeaways","text":"<p>By integrating these advanced validation techniques, we not only improve the quality and reliability of LLM-generated content, but also pave the way for more autonomous and effective systems.</p>"},{"location":"concepts/retrying/","title":"Retrying","text":"<p>One of the benefits of having Pydantic is the ease with which we can define validators. We cover this topic in many articles, like Reasking Validation and in our blog post Good LLM validation is just good validation.</p> <p>This post will mostly describe how to use simple and more complex retry and logic.</p>"},{"location":"concepts/retrying/#example-of-a-validator","title":"Example of a Validator","text":"<p>Before we begin, we'll use a simple example of a validator. One that checks that the name is in all caps. While we could obviously prompt that we want the name in all caps, this serves as an example of how we can build in additional logic without changing our prompts.</p> <p>To use simple retry, we just need to set `max_retries`` as an integer. In this example.</p> <pre><code>from typing import Annotated\nfrom pydantic import AfterValidator, BaseModel\n\n\ndef uppercase_validator(v):\n    if v.islower():\n        raise ValueError(\"Name must be ALL CAPS\")\n    return v\n\n\nclass UserDetail(BaseModel):\n    name: Annotated[str, AfterValidator(uppercase_validator)]\n    age: int\n\n\ntry:\n    UserDetail(name=\"jason\", age=12)\nexcept Exception as e:\n    print(e)\n    \"\"\"\n    1 validation error for UserDetail\n    name\n      Value error, Name must be ALL CAPS [type=value_error, input_value='jason', input_type=str]\n        For further information visit https://errors.pydantic.dev/2.6/v/value_error\n    \"\"\"\n</code></pre>"},{"location":"concepts/retrying/#simple-max-retries","title":"Simple: Max Retries","text":"<p>The simplest way of defining a retry is just defining the maximum number of retries.</p> <pre><code>import openai\nimport instructor\nfrom pydantic import BaseModel\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\nclient = instructor.patch(openai.OpenAI(), mode=instructor.Mode.TOOLS)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4-turbo-preview\",\n    response_model=UserDetail,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract `jason is 12`\"},\n    ],\n    max_retries=3,  # (1)!\n)\nprint(response.model_dump_json(indent=2))\n\"\"\"\n{\n  \"name\": \"jason\",\n  \"age\": 12\n}\n\"\"\"\n# (2)!\n</code></pre> <ol> <li>We set the maximum number of retries to 3. This means that if the model returns an error, we'll reask the model up to 3 times.</li> <li>We assert that the name is in all caps.</li> </ol>"},{"location":"concepts/retrying/#advanced-retry-logic","title":"Advanced: Retry Logic","text":"<p>If you want more control over how we define retries such as back-offs and additional retry logic we can use a library called Tenacity. To learn more, check out the documentation on the Tenacity website.</p> <p>Rather than using the decorator <code>@retry</code>, we can use the <code>Retrying</code> and <code>AsyncRetrying</code> classes to define our own retry logic.</p> <pre><code>import openai\nimport instructor\nfrom pydantic import BaseModel\nfrom tenacity import Retrying, stop_after_attempt, wait_fixed\n\nclient = instructor.patch(openai.OpenAI(), mode=instructor.Mode.TOOLS)\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4-turbo-preview\",\n    response_model=UserDetail,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract `jason is 12`\"},\n    ],\n    max_retries=Retrying(\n        stop=stop_after_attempt(2),  # (1)!\n        wait=wait_fixed(1),  # (2)!\n    ),  # (3)!\n)\nprint(response.model_dump_json(indent=2))\n\"\"\"\n{\n  \"name\": \"jason\",\n  \"age\": 12\n}\n\"\"\"\n</code></pre> <ol> <li>We stop after 2 attempts</li> <li>We wait 1 second between each attempt</li> <li>We can now define our own retry logic</li> </ol>"},{"location":"concepts/retrying/#asynchronous-retries","title":"asynchronous retries","text":"<p>If you're using asynchronous code, you can use <code>AsyncRetrying</code> instead.</p> <pre><code>import openai\nimport instructor\nfrom pydantic import BaseModel\nfrom tenacity import AsyncRetrying, stop_after_attempt, wait_fixed\n\nclient = instructor.patch(openai.AsyncOpenAI(), mode=instructor.Mode.TOOLS)\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\ntask = client.chat.completions.create(\n    model=\"gpt-4-turbo-preview\",\n    response_model=UserDetail,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract `jason is 12`\"},\n    ],\n    max_retries=AsyncRetrying(\n        stop=stop_after_attempt(2),\n        wait=wait_fixed(1),\n    ),\n)\n\nimport asyncio\n\nresponse = asyncio.run(task)\nprint(response.model_dump_json(indent=2))\n\"\"\"\n{\n  \"name\": \"jason\",\n  \"age\": 12\n}\n\"\"\"\n</code></pre>"},{"location":"concepts/retrying/#other-features-of-tenacity","title":"Other Features of Tenacity","text":"<p>Tenacity features a huge number of different retrying capabilities. A few of them are listed below.</p> <ul> <li><code>Retrying(stop=stop_after_attempt(2))</code>: Stop after 2 attempts</li> <li><code>Retrying(stop=stop_after_delay(10))</code>: Stop after 10 seconds</li> <li><code>Retrying(wait=wait_fixed(1))</code>: Wait 1 second between each attempt</li> <li><code>Retrying(wait=wait_random(0, 1))</code>: Wait a random amount of time between 0 and 1 seconds</li> <li><code>Retrying(wait=wait_exponential(multiplier=1, min=4, max=10))</code>: Wait an exponential amount of time between 4 and 10 seconds</li> <li><code>Retrying(wait=(stop_after_attempt(2) | stop_after_delay(10)))</code>: Stop after 2 attempts or 10 seconds</li> <li><code>Retrying(wait=(wait_fixed(1) + wait_random(0.2)))</code>: Wait at least 1 second and add up to 0.2 seconds</li> </ul> <p>Remember that for async clients you need to use <code>AsyncRetrying</code> instead of <code>Retrying</code>!</p>"},{"location":"concepts/typeadapter/","title":"Type Adapter","text":"<p>This page is a work in progress</p> <p>This page is a work in progress. Check out Pydantic's documentation</p>"},{"location":"concepts/types/","title":"Types","text":"<p>This page is a work in progress</p> <p>This page is a work in progress. Check out Pydantic's documentation</p>"},{"location":"concepts/union/","title":"Union","text":"<p>Pydantic models also support <code>Union</code> types, which are used to represent a value that can be one of several types.</p> <p>While many libraries support multiple function calls, and tool calls support multiple returns, the goal is to provide only one way to do things.</p>"},{"location":"concepts/union/#unions-for-multiple-types","title":"Unions for Multiple Types","text":"<p>You can use <code>Union</code> types to write agents that can dynamically choose actions - by choosing an output class. For example, in a search and lookup function, the LLM can determine whether to execute another search, lookup or other action.</p> <pre><code>from pydantic import BaseModel\nfrom typing import Union\n\n\nclass Search(BaseModel):\n    query: str\n\n    def execute(self):\n        return ...\n\n\nclass Lookup(BaseModel):\n    key: str\n\n    def execute(self):\n        return ...\n\n\nclass Action(BaseModel):\n    action: Union[Search, Lookup]\n\n    def execute(self):\n        return self.action.execute()\n</code></pre> <p>See 'examples/union/run.py' for a working example.</p>"},{"location":"concepts/usage/","title":"Usage Tokens","text":"<p>The easiest way to get usage for non streaming requests is to access the raw response.</p> <pre><code>import instructor\n\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\nclient = instructor.patch(OpenAI())\n\n\nclass UserExtract(BaseModel):\n    name: str\n    age: int\n\n\nuser: UserExtract = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserExtract,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"},\n    ],\n)\n\nprint(user._raw_response.usage)\n#&gt; CompletionUsage(completion_tokens=16, prompt_tokens=73, total_tokens=89)\n</code></pre>"},{"location":"examples/","title":"Function Calls by Example","text":""},{"location":"examples/#quick-links","title":"Quick Links","text":"<ol> <li>How are single and multi-label classifications done using enums?</li> <li>How is AI self-assessment implemented with <code>llm_validator</code>?</li> <li>How to do classification in batch from user provided classes.</li> <li>How are exact citations retrieved using regular expressions and smart prompting?</li> <li>How are search queries segmented through function calling and multi-task definitions?</li> <li>How are knowledge graphs generated from questions?</li> <li>How are complex queries decomposed into subqueries in a single request?</li> <li>How are entities extracted and resolved from documents?</li> <li>How is Personally Identifiable Information sanitized from documents?</li> <li>How are action items and dependencies generated from transcripts?</li> <li>How to enable OpenAI's moderation</li> <li>How to extract tables using GPT-Vision?</li> <li>How to generate advertising copy from image inputs</li> </ol> <p>Explore more!</p>"},{"location":"examples/action_items/","title":"Example: Extracting Action Items from Meeting Transcripts","text":"<p>In this guide, we'll walk through how to extract action items from meeting transcripts using OpenAI's API and Pydantic. This use case is essential for automating project management tasks, such as task assignment and priority setting.</p> <p>Motivation</p> <p>Significant amount of time is dedicated to meetings, where action items are generated as the actionable outcomes of these discussions. Automating the extraction of action items can save time and guarantee that no critical tasks are overlooked.</p>"},{"location":"examples/action_items/#defining-the-structures","title":"Defining the Structures","text":"<p>We'll model a meeting transcript as a collection of <code>Ticket</code> objects, each representing an action item. Every <code>Ticket</code> can have multiple <code>Subtask</code> objects, representing smaller, manageable pieces of the main task.</p> <pre><code>from enum import Enum\nfrom pydantic import BaseModel\nfrom typing import List, Optional\n\n\nclass PriorityEnum(str, Enum):\n    high = \"High\"\n    medium = \"Medium\"\n    low = \"Low\"\n\n\nclass Subtask(BaseModel):\n    \"\"\"Correctly resolved subtask from the given transcript\"\"\"\n\n    id: int\n    name: str\n\n\nclass Ticket(BaseModel):\n    \"\"\"Correctly resolved ticket from the given transcript\"\"\"\n\n    id: int\n    name: str\n    description: str\n    priority: PriorityEnum\n    assignees: List[str]\n    subtasks: Optional[List[Subtask]]\n    dependencies: Optional[List[int]]\n</code></pre>"},{"location":"examples/action_items/#extracting-action-items","title":"Extracting Action Items","text":"<p>To extract action items from a meeting transcript, we use the <code>generate</code> function. It calls OpenAI's API, processes the text, and returns a set of action items modeled as <code>ActionItems</code>.</p> <pre><code>import instructor\nfrom openai import OpenAI\nfrom typing import Iterable\n\n# Apply the patch to the OpenAI client\n# enables response_model keyword\nclient = instructor.patch(OpenAI())\n\n\ndef generate(data: str) -&gt; Iterable[Ticket]:\n    return client.chat.completions.create(\n        model=\"gpt-4\",\n        response_model=Iterable[Ticket],\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"The following is a transcript of a meeting...\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Create the action items for the following transcript: {data}\",\n            },\n        ],\n    )\n</code></pre>"},{"location":"examples/action_items/#evaluation-and-testing","title":"Evaluation and Testing","text":"<p>To test the <code>generate</code> function, we provide it with a sample transcript, and then print the JSON representation of the extracted action items.</p> <pre><code>prediction = generate(\n    \"\"\"\nAlice: Hey team, we have several critical tasks we need to tackle for the upcoming release. First, we need to work on improving the authentication system. It's a top priority.\n\nBob: Got it, Alice. I can take the lead on the authentication improvements. Are there any specific areas you want me to focus on?\n\nAlice: Good question, Bob. We need both a front-end revamp and back-end optimization. So basically, two sub-tasks.\n\nCarol: I can help with the front-end part of the authentication system.\n\nBob: Great, Carol. I'll handle the back-end optimization then.\n\nAlice: Perfect. Now, after the authentication system is improved, we have to integrate it with our new billing system. That's a medium priority task.\n\nCarol: Is the new billing system already in place?\n\nAlice: No, it's actually another task. So it's a dependency for the integration task. Bob, can you also handle the billing system?\n\nBob: Sure, but I'll need to complete the back-end optimization of the authentication system first, so it's dependent on that.\n\nAlice: Understood. Lastly, we also need to update our user documentation to reflect all these changes. It's a low-priority task but still important.\n\nCarol: I can take that on once the front-end changes for the authentication system are done. So, it would be dependent on that.\n\nAlice: Sounds like a plan. Let's get these tasks modeled out and get started.\"\"\"\n)\n</code></pre>"},{"location":"examples/action_items/#visualizing-the-tasks","title":"Visualizing the tasks","text":"<p>In order to quickly visualize the data we used code interpreter to create a graphviz export of the json version of the ActionItems array.</p> <p></p> <pre><code>[\n  {\n    \"id\": 1,\n    \"name\": \"Improve Authentication System\",\n    \"description\": \"Revamp the front-end and optimize the back-end of the authentication system\",\n    \"priority\": \"High\",\n    \"assignees\": [\"Bob\", \"Carol\"],\n    \"subtasks\": [\n      {\n        \"id\": 2,\n        \"name\": \"Front-end Revamp\"\n      },\n      {\n        \"id\": 3,\n        \"name\": \"Back-end Optimization\"\n      }\n    ],\n    \"dependencies\": []\n  },\n  {\n    \"id\": 4,\n    \"name\": \"Integrate Authentication System with Billing System\",\n    \"description\": \"Integrate the improved authentication system with the new billing system\",\n    \"priority\": \"Medium\",\n    \"assignees\": [\"Bob\"],\n    \"subtasks\": [],\n    \"dependencies\": [1]\n  },\n  {\n    \"id\": 5,\n    \"name\": \"Update User Documentation\",\n    \"description\": \"Update the user documentation to reflect the changes in the authentication system\",\n    \"priority\": \"Low\",\n    \"assignees\": [\"Carol\"],\n    \"subtasks\": [],\n    \"dependencies\": [2]\n  }\n]\n</code></pre> <p>In this example, the <code>generate</code> function successfully identifies and segments the action items, assigning them priorities, assignees, subtasks, and dependencies as discussed in the meeting.</p> <p>By automating this process, you can ensure that important tasks and details are not lost in the sea of meeting minutes, making project management more efficient and effective.</p>"},{"location":"examples/batch_classification/","title":"Bulk Classification from User-Provided Tags.","text":"<p>This tutorial shows how to do classification from user provided tags. This is valuable when you want to provide services that allow users to do some kind of classification.</p> <p>Motivation</p> <p>Imagine allowing the user to upload documents as part of a RAG application. Oftentimes, we might want to allow the user to specify an existing set of tags, give descriptions, and do the classification for them.</p>"},{"location":"examples/batch_classification/#defining-the-structures","title":"Defining the Structures","text":"<p>One of the easy things to do is to allow users to define a set of tags in some kind of schema and save that in a database. Here's an example of a schema that we might use:</p> tag_id name instructions 0 personal Personal information 1 phone Phone number 2 email Email address 3 address Address 4 Other Other information <ol> <li>tag_id \u2014 The unique identifier for the tag.</li> <li>name \u2014 The name of the tag.</li> <li>instructions \u2014 A description of the tag, which can be used as a prompt to describe the tag.</li> </ol>"},{"location":"examples/batch_classification/#implementing-the-classification","title":"Implementing the Classification","text":"<p>In order to do this we'll do a couple of things:</p> <ol> <li>We'll use the <code>instructor</code> library to patch the <code>openai</code> library to use the <code>AsyncOpenAI</code> client.</li> <li>Implement a <code>Tag</code> model that will be used to validate the tags from the context. (This will allow us to avoid hallucinating tags that are not in the context.)</li> <li>Helper models for the request and response.</li> <li>An async function to do the classification.</li> <li>A main function to run the classification using the <code>asyncio.gather</code> function to run the classification in parallel.</li> </ol> <p>If you want to learn more about how to do bad computations, check out our post on AsyncIO here.</p> <pre><code>import openai\nimport instructor\n\nclient = instructor.patch(\n    openai.AsyncOpenAI(),\n)\n</code></pre> <p>First, we'll need to import all of our Pydantic and instructor code and use the AsyncOpenAI client. Then, we'll define the tag model along with the tag instructions to provide input and output.</p> <p>This is very helpful because once we use something like FastAPI to create endpoints, the Pydantic functions will serve as multiple tools:</p> <ol> <li>A description for the developer</li> <li>Type hints for the IDE</li> <li>OpenAPI documentation for the FastAPI endpoint</li> <li>Schema and Response Model for the language model.</li> </ol> <pre><code>from typing import List\nfrom pydantic import BaseModel, ValidationInfo, model_validator\n\nclass Tag(BaseModel):\n    id: int\n    name: str\n\n    @model_validator(mode=\"after\")\n    def validate_ids(self, info: ValidationInfo):\n        context = info.context\n        if context:\n            tags: List[Tag] = context.get(\"tags\")\n            assert self.id in {\n                tag.id for tag in tags\n            }, f\"Tag ID {self.id} not found in context\"\n            assert self.name in {\n                tag.name for tag in tags\n            }, f\"Tag name {self.name} not found in context\"\n        return self\n\n\nclass TagWithInstructions(Tag):\n    instructions: str\n\n\nclass TagRequest(BaseModel):\n    texts: List[str]\n    tags: List[TagWithInstructions]\n\n\nclass TagResponse(BaseModel):\n    texts: List[str]\n    predictions: List[Tag]\n</code></pre> <p>Let's delve deeper into what the <code>validate_ids</code> function does. Notice that its purpose is to extract tags from the context and ensure that each ID and name exists in the set of tags. This approach helps minimize hallucinations. If we mistakenly identify either the ID or the tag, an error will be thrown, and the instructor will prompt the language model to retry until the correct item is successfully extracted.</p> <pre><code>@model_validator(mode=\"after\")\ndef validate_ids(self, info: ValidationInfo):\n    context = info.context\n    if context:\n        tags: List[Tag] = context.get(\"tags\")\n        assert self.id in {\n            tag.id for tag in tags\n        }, f\"Tag ID {self.id} not found in context\"\n        assert self.name in {\n            tag.name for tag in tags\n        }, f\"Tag name {self.name} not found in context\"\n    return self\n</code></pre> <p>Now, let's implement the function to do the classification. This function will take a single text and a list of tags and return the predicted tag.</p> <pre><code>async def tag_single_request(text: str, tags: List[Tag]) -&gt; Tag:\n    allowed_tags = [(tag.id, tag.name) for tag in tags]\n    allowed_tags_str = \", \".join([f\"`{tag}`\" for tag in allowed_tags])\n\n    return await client.chat.completions.create(\n        model=\"gpt-4-turbo-preview\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a world-class text tagging system.\",\n            },\n            {\"role\": \"user\", \"content\": f\"Describe the following text: `{text}`\"},\n            {\n                \"role\": \"user\",\n                \"content\": f\"Here are the allowed tags: {allowed_tags_str}\",\n            },\n        ],\n        response_model=Tag,  # Minimizes the hallucination of tags that are not in the allowed tags.\n        validation_context={\"tags\": tags},\n    )\n\n\nasync def tag_request(request: TagRequest) -&gt; TagResponse:\n    predictions = await asyncio.gather(\n        *[tag_single_request(text, request.tags) for text in request.texts]\n    )\n    return TagResponse(\n        texts=request.texts,\n        predictions=predictions,\n    )\n</code></pre> <p>Notice that we first define a single async function that makes a prediction of a tag, and we pass it into the validation context in order to minimize hallucinations.</p> <p>Finally, we'll implement the main function to run the classification using the <code>asyncio.gather</code> function to run the classification in parallel.</p> <pre><code>tags = [\n    TagWithInstructions(id=0, name=\"personal\", instructions=\"Personal information\"),\n    TagWithInstructions(id=1, name=\"phone\", instructions=\"Phone number\"),\n    TagWithInstructions(id=2, name=\"email\", instructions=\"Email address\"),\n    TagWithInstructions(id=3, name=\"address\", instructions=\"Address\"),\n    TagWithInstructions(id=4, name=\"Other\", instructions=\"Other information\"),\n]\n\n# Texts will be a range of different questions.\n# Such as \"How much does it cost?\", \"What is your privacy policy?\", etc.\ntexts = [\n    \"What is your phone number?\",\n    \"What is your email address?\",\n    \"What is your address?\",\n    \"What is your privacy policy?\",\n]\n\n# The request will contain the texts and the tags.\nrequest = TagRequest(texts=texts, tags=tags)\n\n# The response will contain the texts, the predicted tags, and the confidence.\nresponse = asyncio.run(tag_request(request))\nprint(response.model_dump_json(indent=2))\n</code></pre> <p>Which would result in:</p> <pre><code>{\n  \"texts\": [\n    \"What is your phone number?\",\n    \"What is your email address?\",\n    \"What is your address?\",\n    \"What is your privacy policy?\"\n  ],\n  \"predictions\": [\n    {\n      \"id\": 1,\n      \"name\": \"phone\"\n    },\n    {\n      \"id\": 2,\n      \"name\": \"email\"\n    },\n    {\n      \"id\": 3,\n      \"name\": \"address\"\n    },\n    {\n      \"id\": 4,\n      \"name\": \"Other\"\n    }\n  ]\n}\n</code></pre>"},{"location":"examples/batch_classification/#what-happens-in-production","title":"What happens in production?","text":"<p>If we were to use this in production, we might expect to have some kind of fast API endpoint.</p> <pre><code>from fastapi import FastAPI\n\napp = FastAPI()\n\n\n@app.post(\"/tag\", response_model=TagResponse)\nasync def tag(request: TagRequest) -&gt; TagResponse:\n    return await tag_request(request)\n</code></pre> <p>Since everything is already annotated with Pydantic, this code is very simple to write!</p> <p>Where do tags come from?</p> <p>I just want to call out that here you can also imagine the tag spec IDs and names and instructions for example could come from a database or somewhere else. I'll leave this as an exercise to the reader, but I hope this gives us a clear understanding of how we can do something like user-defined classification.</p>"},{"location":"examples/batch_classification/#improving-the-model","title":"Improving the Model","text":"<p>There's a couple things we could do to make this system a little bit more robust.</p> <ol> <li>Use confidence score:</li> </ol> <pre><code>class TagWithConfidence(Tag):\n    confidence: float = Field(\n        ...,\n        ge=0,\n        le=1,\n        description=\"The confidence of the prediction, 0 is low, 1 is high\",\n    )\n</code></pre> <ol> <li>Use multiclass classification:</li> </ol> <p>Notice in the example we use Iterable[Tag] vs Tag. This is because we might want to use a multiclass classification model that returns multiple tag!</p> <pre><code>await client.chat.completions.create(\n    model=\"gpt-4-turbo-preview\",\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a world-class text tagging system.\",\n        },\n        {\"role\": \"user\", \"content\": f\"Describe the following text: `{text}`\"},\n        {\n            \"role\": \"user\",\n            \"content\": f\"Here are the allowed tags: {allowed_tags_str}\",\n        },\n    ],\n    response_model=Iterable[Tag],\n    validation_context={\"tags\": tags},\n)\n</code></pre>"},{"location":"examples/classification/","title":"Example: Text Classification using OpenAI and Pydantic","text":"<p>This tutorial showcases how to implement text classification tasks\u2014specifically, single-label and multi-label classifications\u2014using the OpenAI API, Python's <code>enum</code> module, and Pydantic models.</p> <p>Motivation</p> <p>Text classification is a common problem in many NLP applications, such as spam detection or support ticket categorization. The goal is to provide a systematic way to handle these cases using OpenAI's GPT models in combination with Python data structures.</p>"},{"location":"examples/classification/#single-label-classification","title":"Single-Label Classification","text":""},{"location":"examples/classification/#defining-the-structures","title":"Defining the Structures","text":"<p>For single-label classification, we first define an <code>enum</code> for possible labels and a Pydantic model for the output.</p> <pre><code>import enum\nfrom pydantic import BaseModel\n\n\nclass Labels(str, enum.Enum):\n    \"\"\"Enumeration for single-label text classification.\"\"\"\n\n    SPAM = \"spam\"\n    NOT_SPAM = \"not_spam\"\n\n\nclass SinglePrediction(BaseModel):\n    \"\"\"\n    Class for a single class label prediction.\n    \"\"\"\n\n    class_label: Labels\n</code></pre>"},{"location":"examples/classification/#classifying-text","title":"Classifying Text","text":"<p>The function <code>classify</code> will perform the single-label classification.</p> <pre><code>from openai import OpenAI\nimport instructor\n\n# Apply the patch to the OpenAI client\n# enables response_model keyword\nclient = instructor.patch(OpenAI())\n\n\ndef classify(data: str) -&gt; SinglePrediction:\n    \"\"\"Perform single-label classification on the input text.\"\"\"\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo-0613\",\n        response_model=SinglePrediction,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"Classify the following text: {data}\",\n            },\n        ],\n    )  # type: ignore\n</code></pre>"},{"location":"examples/classification/#testing-and-evaluation","title":"Testing and Evaluation","text":"<p>Let's run an example to see if it correctly identifies a spam message.</p> <pre><code># Test single-label classification\nprediction = classify(\"Hello there I'm a Nigerian prince and I want to give you money\")\nassert prediction.class_label == Labels.SPAM\n</code></pre>"},{"location":"examples/classification/#multi-label-classification","title":"Multi-Label Classification","text":""},{"location":"examples/classification/#defining-the-structures_1","title":"Defining the Structures","text":"<p>For multi-label classification, we introduce a new enum class and a different Pydantic model to handle multiple labels.</p> <pre><code>from typing import List\nimport enum\n\n# Define Enum class for multiple labels\nclass MultiLabels(str, enum.Enum):\n    TECH_ISSUE = \"tech_issue\"\n    BILLING = \"billing\"\n    GENERAL_QUERY = \"general_query\"\n\n\n# Define the multi-class prediction model\nclass MultiClassPrediction(BaseModel):\n    \"\"\"\n    Class for a multi-class label prediction.\n    \"\"\"\n\n    class_labels: List[MultiLabels]\n</code></pre>"},{"location":"examples/classification/#classifying-text_1","title":"Classifying Text","text":"<p>The function <code>multi_classify</code> is responsible for multi-label classification.</p> <pre><code>def multi_classify(data: str) -&gt; MultiClassPrediction:\n    \"\"\"Perform multi-label classification on the input text.\"\"\"\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo-0613\",\n        response_model=MultiClassPrediction,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"Classify the following support ticket: {data}\",\n            },\n        ],\n    )  # type: ignore\n</code></pre>"},{"location":"examples/classification/#testing-and-evaluation_1","title":"Testing and Evaluation","text":"<p>Finally, we test the multi-label classification function using a sample support ticket.</p> <pre><code># Test multi-label classification\nticket = \"My account is locked and I can't access my billing info.\"\nprediction = multi_classify(ticket)\nassert MultiLabels.TECH_ISSUE in prediction.class_labels\nassert MultiLabels.BILLING in prediction.class_labels\n</code></pre>"},{"location":"examples/entity_resolution/","title":"Entity Resolution and Visualization for Legal Documents","text":"<p>In this guide, we demonstrate how to extract and resolve entities from a sample legal contract. Then, we visualize these entities and their dependencies as an entity graph. This approach can be invaluable for legal tech applications, aiding in the understanding of complex documents.</p> <p>Motivation</p> <p>Legal contracts are full of intricate details and interconnected clauses. Automatically extracting and visualizing these elements can make it easier to understand the document's overall structure and terms.</p>"},{"location":"examples/entity_resolution/#defining-the-data-structures","title":"Defining the Data Structures","text":"<p>The <code>Entity</code> and <code>Property</code> classes model extracted entities and their attributes. <code>DocumentExtraction</code> encapsulates a list of these entities.</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import List\n\n\nclass Property(BaseModel):\n    key: str\n    value: str\n    resolved_absolute_value: str\n\n\nclass Entity(BaseModel):\n    id: int = Field(\n        ...,\n        description=\"Unique identifier for the entity, used for deduplication, design a scheme allows multiple entities\",\n    )\n    subquote_string: List[str] = Field(\n        ...,\n        description=\"Correctly resolved value of the entity, if the entity is a reference to another entity, this should be the id of the referenced entity, include a few more words before and after the value to allow for some context to be used in the resolution\",\n    )\n    entity_title: str\n    properties: List[Property] = Field(\n        ..., description=\"List of properties of the entity\"\n    )\n    dependencies: List[int] = Field(\n        ...,\n        description=\"List of entity ids that this entity depends  or relies on to resolve it\",\n    )\n\n\nclass DocumentExtraction(BaseModel):\n    entities: List[Entity] = Field(\n        ...,\n        description=\"Body of the answer, each fact should be a separate object with a body and a list of sources\",\n    )\n</code></pre>"},{"location":"examples/entity_resolution/#entity-extraction-and-resolution","title":"Entity Extraction and Resolution","text":"<p>The <code>ask_ai</code> function utilizes OpenAI's API to extract and resolve entities from the input content.</p> <pre><code>import instructor\nfrom openai import OpenAI\n\n# Apply the patch to the OpenAI client\n# enables response_model keyword\nclient = instructor.patch(OpenAI())\n\n\ndef ask_ai(content) -&gt; DocumentExtraction:\n    return client.chat.completions.create(\n        model=\"gpt-4\",\n        response_model=DocumentExtraction,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"Extract and resolve a list of entities from the following document:\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": content,\n            },\n        ],\n    )  # type: ignore\n</code></pre>"},{"location":"examples/entity_resolution/#graph-visualization","title":"Graph Visualization","text":"<p><code>generate_graph</code> takes the extracted entities and visualizes them using Graphviz. It creates nodes for each entity and edges for their dependencies.</p> <pre><code>from graphviz import Digraph\n\n\ndef generate_html_label(entity: Entity) -&gt; str:\n    rows = [\n        f\"&lt;tr&gt;&lt;td&gt;{prop.key}&lt;/td&gt;&lt;td&gt;{prop.resolved_absolute_value}&lt;/td&gt;&lt;/tr&gt;\"\n        for prop in entity.properties\n    ]\n    table_rows = \"\".join(rows)\n    return f\"&lt;&lt;table border='0' cellborder='1' cellspacing='0'&gt;&lt;tr&gt;&lt;td colspan='2'&gt;&lt;b&gt;{entity.entity_title}&lt;/b&gt;&lt;/td&gt;&lt;/tr&gt;{table_rows}&lt;/table&gt;&gt;\"\n\n\ndef generate_graph(data: DocumentExtraction):\n    dot = Digraph(comment=\"Entity Graph\", node_attr={\"shape\": \"plaintext\"})\n\n    for entity in data.entities:\n        label = generate_html_label(entity)\n        dot.node(str(entity.id), label)\n\n    for entity in data.entities:\n        for dep_id in entity.dependencies:\n            dot.edge(str(entity.id), str(dep_id))\n\n    dot.render(\"entity.gv\", view=True)\n</code></pre>"},{"location":"examples/entity_resolution/#execution","title":"Execution","text":"<p>Finally, execute the code to visualize the entity graph for the sample legal contract.</p> <pre><code>content = \"\"\"\nSample Legal Contract\nAgreement Contract\n\nThis Agreement is made and entered into on 2020-01-01 by and between Company A (\"the Client\") and Company B (\"the Service Provider\").\n\nArticle 1: Scope of Work\n\nThe Service Provider will deliver the software product to the Client 30 days after the agreement date.\n\nArticle 2: Payment Terms\n\nThe total payment for the service is $50,000.\nAn initial payment of $10,000 will be made within 7 days of the the signed date.\nThe final payment will be due 45 days after [SignDate].\n\nArticle 3: Confidentiality\n\nThe parties agree not to disclose any confidential information received from the other party for 3 months after the final payment date.\n\nArticle 4: Termination\n\nThe contract can be terminated with a 30-day notice, unless there are outstanding obligations that must be fulfilled after the [DeliveryDate].\n\"\"\"  # Your legal contract here\nmodel = ask_ai(content)\ngenerate_graph(model)\n</code></pre> <p>This will produce a graphical representation of the entities and their dependencies, stored as \"entity.gv\".</p> <p></p>"},{"location":"examples/exact_citations/","title":"Example: Answering Questions with Validated Citations","text":"<p>For the full code example check out examples/citation_fuzzy_match.py</p>"},{"location":"examples/exact_citations/#overview","title":"Overview","text":"<p>This example shows how to use Instructor with validators to not only add citations to answers generated but also prevent hallucinations by ensuring that every statement made by the LLM is backed up by a direct quote from the context provided, and that those quotes exist!.Two Python classes, <code>Fact</code> and <code>QuestionAnswer</code>, are defined to encapsulate the information of individual facts and the entire answer, respectively.</p>"},{"location":"examples/exact_citations/#data-structures","title":"Data Structures","text":""},{"location":"examples/exact_citations/#the-fact-class","title":"The <code>Fact</code> Class","text":"<p>The <code>Fact</code> class encapsulates a single statement or fact. It contains two fields:</p> <ul> <li><code>fact</code>: A string representing the body of the fact or statement.</li> <li><code>substring_quote</code>: A list of strings. Each string is a direct quote from the context that supports the <code>fact</code>.</li> </ul>"},{"location":"examples/exact_citations/#validation-method-validate_sources","title":"Validation Method: <code>validate_sources</code>","text":"<p>This method validates the sources (<code>substring_quote</code>) in the context. It utilizes regex to find the span of each substring quote in the given context. If the span is not found, the quote is removed from the list.</p> <pre><code>from pydantic import Field, BaseModel, model_validator, FieldValidationInfo\nfrom typing import List\n\n\nclass Fact(BaseModel):\n    fact: str = Field(...)\n    substring_quote: List[str] = Field(...)\n\n    @model_validator(mode=\"after\")\n    def validate_sources(self, info: FieldValidationInfo) -&gt; \"Fact\":\n        text_chunks = info.context.get(\"text_chunk\", None)\n        spans = list(self.get_spans(text_chunks))\n        self.substring_quote = [text_chunks[span[0] : span[1]] for span in spans]\n        return self\n\n    def get_spans(self, context):\n        for quote in self.substring_quote:\n            yield from self._get_span(quote, context)\n\n    def _get_span(self, quote, context):\n        for match in re.finditer(re.escape(quote), context):\n            yield match.span()\n</code></pre>"},{"location":"examples/exact_citations/#the-questionanswer-class","title":"The <code>QuestionAnswer</code> Class","text":"<p>This class encapsulates the question and its corresponding answer. It contains two fields:</p> <ul> <li><code>question</code>: The question asked.</li> <li><code>answer</code>: A list of <code>Fact</code> objects that make up the answer.</li> </ul>"},{"location":"examples/exact_citations/#validation-method-validate_sources_1","title":"Validation Method: <code>validate_sources</code>","text":"<p>This method checks that each <code>Fact</code> object in the <code>answer</code> list has at least one valid source. If a <code>Fact</code> object has no valid sources, it is removed from the <code>answer</code> list.</p> <pre><code>class QuestionAnswer(BaseModel):\n    question: str = Field(...)\n    answer: List[Fact] = Field(...)\n\n    @model_validator(mode=\"after\")\n    def validate_sources(self) -&gt; \"QuestionAnswer\":\n        self.answer = [fact for fact in self.answer if len(fact.substring_quote) &gt; 0]\n        return self\n</code></pre>"},{"location":"examples/exact_citations/#function-to-ask-ai-a-question","title":"Function to Ask AI a Question","text":""},{"location":"examples/exact_citations/#the-ask_ai-function","title":"The <code>ask_ai</code> Function","text":"<p>This function takes a string <code>question</code> and a string <code>context</code> and returns a <code>QuestionAnswer</code> object. It uses the OpenAI API to fetch the answer and then validates the sources using the defined classes.</p> <p>To understand the validation context work from pydantic check out pydantic's docs</p> <pre><code>from openai import OpenAI\nimport instructor\n\n# Apply the patch to the OpenAI client\n# enables response_model, validation_context keyword\nclient = instructor.patch(OpenAI())\n\n\ndef ask_ai(question: str, context: str) -&gt; QuestionAnswer:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo-0613\",\n        temperature=0,\n        response_model=QuestionAnswer,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a world class algorithm to answer questions with correct and exact citations.\",\n            },\n            {\"role\": \"user\", \"content\": f\"{context}\"},\n            {\"role\": \"user\", \"content\": f\"Question: {question}\"},\n        ],\n        validation_context={\"text_chunk\": context},\n    )\n</code></pre>"},{"location":"examples/exact_citations/#example","title":"Example","text":"<p>dd Here's an example of using these classes and functions to ask a question and validate the answer.</p> <pre><code>question = \"What did the author do during college?\"\ncontext = \"\"\"\nMy name is Jason Liu, and I grew up in Toronto Canada but I was born in China.\nI went to an arts high school but in university I studied Computational Mathematics and physics.\nAs part of coop I worked at many companies including Stitchfix, Facebook.\nI also started the Data Science club at the University of Waterloo and I was the president of the club for 2 years.\n\"\"\"\n</code></pre> <p>The output would be a <code>QuestionAnswer</code> object containing validated facts and their sources.</p> <pre><code>{\n    \"question\": \"where did he go to school?\",\n    \"answer\": [\n        {\n            \"statement\": \"Jason Liu went to an arts highschool.\",\n            \"substring_phrase\": [\"arts highschool\"],\n        },\n        {\n            \"statement\": \"Jason Liu studied Computational Mathematics and physics in university.\",\n            \"substring_phrase\": [\"university\"],\n        },\n    ],\n}\n</code></pre> <p>This ensures that every piece of information in the answer has been validated against the context.</p>"},{"location":"examples/extracting_tables/","title":"Extracting Tables using GPT-Vision","text":"<p>This post demonstrates how to use Python's type annotations and OpenAI's new vision model to extract tables from images and convert them into markdown format. This method is particularly useful for data analysis and automation tasks.</p> <p>The full code is available on GitHub</p>"},{"location":"examples/extracting_tables/#building-the-custom-type-for-markdown-tables","title":"Building the Custom Type for Markdown Tables","text":"<p>First, we define a custom type, <code>MarkdownDataFrame</code>, to handle pandas DataFrames formatted in markdown. This type uses Python's <code>Annotated</code> and <code>InstanceOf</code> types, along with decorators <code>BeforeValidator</code> and <code>PlainSerializer</code>, to process and serialize the data.</p> <pre><code>from io import StringIO\nfrom typing import Annotated, Any\nfrom pydantic import BaseModel, Field, BeforeValidator, PlainSerializer, InstanceOf, WithJsonSchema\nfrom typing import Iterable\nimport pandas as pd\n\n\ndef md_to_df(data: Any) -&gt; Any:\n    # Convert markdown to DataFrame\n    if isinstance(data, str):\n        return (\n            pd.read_csv(\n                StringIO(data),  # Process data\n                sep=\"|\",\n                index_col=1,\n            )\n            .dropna(axis=1, how=\"all\")\n            .iloc[1:]\n            .applymap(lambda x: x.strip())\n        )\n    return data\n\n\nMarkdownDataFrame = Annotated[\n    InstanceOf[pd.DataFrame],\n    BeforeValidator(md_to_df),\n    PlainSerializer(lambda df: df.to_markdown()),\n    WithJsonSchema(\n        {\n            \"type\": \"string\",\n            \"description\": \"The markdown representation of the table, each one should be tidy, do not try to join tables that should be seperate\",\n        }\n    ),\n]\n</code></pre>"},{"location":"examples/extracting_tables/#defining-the-table-class","title":"Defining the Table Class","text":"<p>The <code>Table</code> class is essential for organizing the extracted data. It includes a caption and a dataframe, processed as a markdown table. Since most of the complexity is handled by the <code>MarkdownDataFrame</code> type, the <code>Table</code> class is straightforward!</p> <pre><code>class Table(BaseModel):\n    caption: str\n    dataframe: MarkdownDataFrame\n</code></pre>"},{"location":"examples/extracting_tables/#extracting-tables-from-images","title":"Extracting Tables from Images","text":"<p>The <code>extract_table</code> function uses OpenAI's vision model to process an image URL and extract tables in markdown format. We utilize the <code>instructor</code> library to patch the OpenAI client for this purpose.</p> <pre><code>import instructor\nfrom openai import OpenAI\n\n# Apply the patch to the OpenAI client to support response_model\n# Also use MD_JSON mode since the visino model does not support any special structured output mode\nclient = instructor.patch(OpenAI(), mode=instructor.function_calls.Mode.MD_JSON)\n\n\ndef extract_table(url: str) -&gt; Iterable[Table]:\n    return client.chat.completions.create(\n        model=\"gpt-4-vision-preview\",\n        response_model=Iterable[Table],\n        max_tokens=1800,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"text\", \"text\": \"Extract table from image.\"},\n                    {\"type\": \"image_url\", \"image_url\": {\"url\": url}},\n                ],\n            }\n        ],\n    )\n</code></pre>"},{"location":"examples/extracting_tables/#practical-example","title":"Practical Example","text":"<p>In this example, we apply the method to extract data from an image showing the top grossing apps in Ireland for October 2023.</p> <pre><code>url = \"https://a.storyblok.com/f/47007/2400x2000/bf383abc3c/231031_uk-ireland-in-three-charts_table_v01_b.png\"\ntables = extract_table(url)\nfor table in tables:\n    print(table.caption, end=\"\\n\")\n    print(table.dataframe)\n</code></pre> Expand to see the output <p></p>"},{"location":"examples/extracting_tables/#top-10-grossing-apps-in-october-2023-ireland-for-android-platforms","title":"Top 10 Grossing Apps in October 2023 (Ireland) for Android Platforms","text":"Rank App Name Category 1 Google One Productivity 2 Disney+ Entertainment 3 TikTok - Videos, Music &amp; LIVE Entertainment 4 Candy Crush Saga Games 5 Tinder: Dating, Chat &amp; Friends Social networking 6 Coin Master Games 7 Roblox Games 8 Bumble - Dating &amp; Make Friends Dating 9 Royal Match Games 10 Spotify: Music and Podcasts Music &amp; Audio"},{"location":"examples/extracting_tables/#top-10-grossing-apps-in-october-2023-ireland-for-ios-platforms","title":"Top 10 Grossing Apps in October 2023 (Ireland) for iOS Platforms","text":"Rank App Name Category 1 Tinder: Dating, Chat &amp; Friends Social networking 2 Disney+ Entertainment 3 YouTube: Watch, Listen, Stream Entertainment 4 Audible: Audio Entertainment Entertainment 5 Candy Crush Saga Games 6 TikTok - Videos, Music &amp; LIVE Entertainment 7 Bumble - Dating &amp; Make Friends Dating 8 Roblox Games 9 LinkedIn: Job Search &amp; News Business 10 Duolingo - Language Lessons Education"},{"location":"examples/image_to_ad_copy/","title":"Use Vision API to detect products and generate advertising copy","text":"<p>This post demonstrates how to use GPT-4 Vision API and the Chat API to automatically generate advertising copy from product images. This method can be useful for marketing and advertising teams, as well as for e-commerce platforms.</p> <p>The full code is available on GitHub.</p>"},{"location":"examples/image_to_ad_copy/#building-the-models","title":"Building the models","text":""},{"location":"examples/image_to_ad_copy/#product","title":"Product","text":"<p>For the <code>Product</code> model, we define a class that represents a product extracted from an image and store the name, key features, and description. The product attributes are dynamically determined based on the content of the image.</p> <p>Note that it is easy to add Validators and other Pydantic features to the model to ensure that the data is valid and consistent.</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import List, Optional\n\nclass Product(BaseModel):\n    \"\"\"\n    Represents a product extracted from an image using AI.\n\n    The product attributes are dynamically determined based on the content\n    of the image and the AI's interpretation. This class serves as a structured\n    representation of the identified product characteristics.\n    \"\"\"\n\n    name: str = Field(\n        description=\"A generic name for the product.\", example=\"Headphones\"\n    )\n    key_features: Optional[List[str]] = Field(\n        description=\"A list of key features of the product that stand out.\",\n        default=None,\n    )\n\n    description: Optional[str] = Field(\n        description=\"A description of the product.\",\n        default=None,\n    )\n\n    # Can be customized and automatically generated\n    def generate_prompt(self):\n        prompt = f\"Product: {self.name}\\n\"\n        if self.description:\n            prompt += f\"Description: {self.description}\\n\"\n        if self.key_features:\n            prompt += f\"Key Features: {', '.join(self.key_features)}\\n\"\n        return prompt\n</code></pre>"},{"location":"examples/image_to_ad_copy/#identified-product","title":"Identified Product","text":"<p>We also define a class that represents a list of products identified in the images. We also add an error flag and message to indicate if there was an error in the processing of the image.</p> <pre><code>class IdentifiedProduct(BaseModel):\n    \"\"\"\n    Represents a list of products identified in the images.\n    \"\"\"\n\n    products: Optional[List[Product]] = Field(\n        description=\"A list of products identified by the AI.\",\n        example=[\n            Product(\n                name=\"Headphones\",\n                description=\"Wireless headphones with noise cancellation.\",\n                key_features=[\"Wireless\", \"Noise Cancellation\"],\n            )\n        ],\n        default=None,\n    )\n\n    error: bool = Field(default=False)\n    message: Optional[str] = Field(default=None)\n</code></pre>"},{"location":"examples/image_to_ad_copy/#advertising-copy","title":"Advertising Copy","text":"<p>Finally, the <code>AdCopy</code> models stores the output in a structured format with a headline and the text.</p> <pre><code>class AdCopy(BaseModel):\n    \"\"\"\n    Represents a generated ad copy.\n    \"\"\"\n\n    headline: str = Field(\n        description=\"A short, catchy, and memorable headline for the given product. The headline should invoke curiosity and interest in the product.\",\n    )\n    ad_copy: str = Field(\n        description=\"A long-form advertisement copy for the given product. This will be used in campaigns to promote the product with a persuasive message and a call-to-action with the objective of driving sales.\",\n    )\n    name: str = Field(description=\"The name of the product being advertised.\")\n</code></pre>"},{"location":"examples/image_to_ad_copy/#calling-the-api","title":"Calling the API","text":""},{"location":"examples/image_to_ad_copy/#product-detection","title":"Product Detection","text":"<p>The <code>read_images</code> function uses OpenAI's vision model to process a list of image URLs and identify products in each of them. We utilize the <code>instructor</code> library to patch the OpenAI client for this purpose.</p> <pre><code>def read_images(image_urls: List[str]) -&gt; IdentifiedProduct:\n    \"\"\"\n    Given a list of image URLs, identify the products in the images.\n    \"\"\"\n\n    logger.info(f\"Identifying products in images... {len(image_urls)} images\")\n\n    return client_image.chat.completions.create(\n        model=\"gpt-4-vision-preview\",\n        response_model=IdentifiedProduct,\n        max_tokens=1024,  # can be changed\n        temperature=0,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": \"Identify products using the given images and generate key features for each product.\",\n                    },\n                    *[\n                        {\"type\": \"image_url\", \"image_url\": {\"url\": url}}\n                        for url in image_urls\n                    ],\n                ],\n            }\n        ],\n    )\n</code></pre> <p>This gives us a list of products identified in all the images.</p>"},{"location":"examples/image_to_ad_copy/#generate-advertising-copy","title":"Generate advertising copy","text":"<p>Then, we can use the <code>generate_ad_copy</code> function to generate advertising copy for each of the products identified in the images.</p> <p>Two clients are defined for the two different models. This is because the <code>gpt-4-vision-preview</code> model is not compatible with the <code>gpt-4-1106-preview</code> model in terms of their response format.</p> <pre><code>def generate_ad_copy(product: Product) -&gt; AdCopy:\n    \"\"\"\n    Given a product, generate an ad copy for the product.\n    \"\"\"\n\n    logger.info(f\"Generating ad copy for product: {product.name}\")\n\n    return client_copy.chat.completions.create(\n        model=\"gpt-4-1106-preview\",\n        response_model=AdCopy,\n        temperature=0.3,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are an expert marketing assistant for all products. Your task is to generate an advertisement copy for a product using the name, description, and key features.\",\n            },\n            {\"role\": \"user\", \"content\": product.generate_prompt()},\n        ],\n    )\n</code></pre>"},{"location":"examples/image_to_ad_copy/#putting-it-all-together","title":"Putting it all together","text":"<p>Finally, we can put it all together in a single function that takes a list of image URLs and generates advertising copy for the products identified in the images. Please refer to the full code for the complete implementation.</p>"},{"location":"examples/image_to_ad_copy/#input-file","title":"Input file","text":"<p>The input file is currently a list of image URLs, but this trivial to change to any required format.</p> <pre><code>https://contents.mediadecathlon.com/p1279823/9a1c59ad97a4084a346c014740ae4d3ff860ea70b485ee65f34017ff5e9ae5f7/recreational-ice-skates-fit-50-black.jpg?format=auto\nhttps://contents.mediadecathlon.com/p1279822/a730505231dbd6747c14ee93e8f89e824d3fa2a5b885ec26de8d7feb5626638a/recreational-ice-skates-fit-50-black.jpg?format=auto\nhttps://contents.mediadecathlon.com/p2329893/1ed75517602a5e00245b89ab6a1c6be6d8968a5a227c932b10599f857f3ed4cd/mens-hiking-leather-boots-sh-100-x-warm.jpg?format=auto\nhttps://contents.mediadecathlon.com/p2047870/8712c55568dd9928c83b19c6a4067bf161811a469433dc89244f0ff96a50e3e9/men-s-winter-hiking-boots-sh-100-x-warm-grey.jpg?format=auto\n</code></pre> Expand to see the output <p> </p> <pre><code>{\n    \"products\":\n    [\n        {\n            \"name\": \"Ice Skates\",\n            \"key_features\": [\n                \"Lace-up closure\",\n                \"Durable blade\",\n                \"Ankle support\"\n            ],\n            \"description\": \"A pair of ice skates with lace-up closure for secure fit, durable blade for ice skating, and reinforced ankle support.\"\n        },\n        {\n            \"name\": \"Hiking Boots\",\n            \"key_features\": [\n                \"High-top design\",\n                \"Rugged outsole\",\n                \"Water-resistant\"\n            ],\n            \"description\": \"Sturdy hiking boots featuring a high-top design for ankle support, rugged outsole for grip on uneven terrain, and water-resistant construction.\"\n        },\n        {\n            \"name\": \"Winter Boots\",\n            \"key_features\": [\n                \"Insulated lining\",\n                \"Waterproof lower\",\n                \"Slip-resistant sole\"\n            ],\n            \"description\": \"Warm winter boots with insulated lining for cold weather, waterproof lower section to keep feet dry, and a slip-resistant sole for stability.\"\n        }\n    ],\n    \"ad_copies\": [\n        {\n            \"headline\": \"Glide with Confidence - Discover the Perfect Ice Skates!\",\n            \"ad_copy\": \"Step onto the ice with poise and precision with our premium Ice Skates. Designed for both beginners and seasoned skaters, these skates offer a perfect blend of comfort and performance. The lace-up closure ensures a snug fit that keeps you stable as you carve through the ice. With a durable blade that withstands the test of time, you can focus on perfecting your moves rather than worrying about your equipment. The reinforced ankle support provides the necessary protection and aids in preventing injuries, allowing you to skate with peace of mind. Whether you're practicing your spins, jumps, or simply enjoying a leisurely glide across the rink, our Ice Skates are the ideal companion for your ice adventures. Lace up and get ready to experience the thrill of ice skating like never before!\",\n            \"name\": \"Ice Skates\"\n        },\n        {\n            \"headline\": \"Conquer Every Trail with Confidence!\",\n            \"ad_copy\": \"Embark on your next adventure with our top-of-the-line Hiking Boots! Designed for the trail-blazing spirits, these boots boast a high-top design that provides unparalleled ankle support to keep you steady on any path. The rugged outsole ensures a firm grip on the most uneven terrains, while the water-resistant construction keeps your feet dry as you traverse through streams and muddy trails. Whether you're a seasoned hiker or just starting out, our Hiking Boots are the perfect companion for your outdoor escapades. Lace up and step into the wild with confidence - your journey awaits!\",\n            \"name\": \"Hiking Boots\"\n        },\n        {\n            \"headline\": \"Conquer the Cold with Comfort!\",\n            \"ad_copy\": \"Step into the season with confidence in our Winter Boots, the ultimate ally against the chill. Designed for those who don't let the cold dictate their moves, these boots feature an insulated lining that wraps your feet in a warm embrace, ensuring that the biting cold is a worry of the past. But warmth isn't their only virtue. With a waterproof lower section, your feet will remain dry and cozy, come rain, snow, or slush. And let's not forget the slip-resistant sole that stands between you and the treacherous ice, offering stability and peace of mind with every step you take. Whether you're braving a blizzard or just nipping out for a coffee, our Winter Boots are your trusty companions, keeping you warm, dry, and upright. Don't let winter slow you down. Lace up and embrace the elements!\",\n            \"name\": \"Winter Boots\"\n        }\n    ]\n}\n</code></pre>"},{"location":"examples/knowledge_graph/","title":"Visualizing Knowledge Graphs for Complex Topics","text":"<p>In this guide, you'll discover how to visualise a detailed knowledge graph when dealing with complex topics. We'll then move on to iteratively updating our knowledge graph with new information through a series of sequential api calls using only the Instructor library, Pydantic and Graphviz to visualise our graph.</p> <p>Motivation</p> <p>Knowledge graphs offer a visually appealing and coherent way to understand complicated topics like quantum mechanics. By generating these graphs automatically, you can accelerate the learning process and make it easier to digest complex information.</p>"},{"location":"examples/knowledge_graph/#defining-the-structures","title":"Defining the Structures","text":"<p>Let's model a knowledge graph with <code>Node</code> and <code>Edge</code> objects. <code>Node</code> objects represent key concepts or entities, while <code>Edge</code> objects indicate the relationships between them.</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import List\n\n\nclass Node(BaseModel):\n    id: int\n    label: str\n    color: str\n\n\nclass Edge(BaseModel):\n    source: int\n    target: int\n    label: str\n    color: str = \"black\"\n\n\nclass KnowledgeGraph(BaseModel):\n    nodes: List[Node] = Field(..., default_factory=list)\n    edges: List[Edge] = Field(..., default_factory=list)\n</code></pre>"},{"location":"examples/knowledge_graph/#generating-knowledge-graphs","title":"Generating Knowledge Graphs","text":"<p>The <code>generate_graph</code> function leverages OpenAI's API to generate a knowledge graph based on the input query.</p> <pre><code>from openai import OpenAI\nimport instructor\n\n# Adds response_model to ChatCompletion\n# Allows the return of Pydantic model rather than raw JSON\nclient = instructor.patch(OpenAI())\n\n\ndef generate_graph(input) -&gt; KnowledgeGraph:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"Help me understand the following by describing it as a detailed knowledge graph: {input}\",\n            }\n        ],\n        response_model=KnowledgeGraph,\n    )  # type: ignore\n</code></pre>"},{"location":"examples/knowledge_graph/#visualizing-the-graph","title":"Visualizing the Graph","text":"<p>The <code>visualize_knowledge_graph</code> function uses the Graphviz library to render the generated knowledge graph.</p> <pre><code>from graphviz import Digraph\n\n\ndef visualize_knowledge_graph(kg: KnowledgeGraph):\n    dot = Digraph(comment=\"Knowledge Graph\")\n\n    # Add nodes\n    for node in kg.nodes:\n        dot.node(str(node.id), node.label, color=node.color)\n\n    # Add edges\n    for edge in kg.edges:\n        dot.edge(str(edge.source), str(edge.target), label=edge.label, color=edge.color)\n\n    # Render the graph\n    dot.render(\"knowledge_graph.gv\", view=True)\n\ngraph = generate_graph(\"Teach me about quantum mechanics\")\nvisualize_knowledge_graph(graph)\n</code></pre> <p></p> <p>This will produce a visual representation of the knowledge graph, stored as \"knowledge_graph.gv\". You can open this file to explore the key concepts and their relationships in quantum mechanics.</p>"},{"location":"examples/knowledge_graph/#iterative-updates","title":"Iterative Updates","text":"<p>Now that we've seen how to generate a knowledge graph from a single input, let's see how we can iteratively update our knowledge graph with new information, or when informatino does not fit into a single prompt.</p> <p>Let's take an easy example where we want to visualise the combined knowledge graph that the following sentences represent.</p> <pre><code>text_chunks = [\n    \"Jason knows a lot about quantum mechanics. He is a physicist. He is a professor\",\n    \"Professors are smart.\",\n    \"Sarah knows Jason and is a student of his.\",\n    \"Sarah is a student at the University of Toronto. and UofT is in Canada\",\n]\n</code></pre>"},{"location":"examples/knowledge_graph/#updating-our-data-model","title":"Updating Our Data Model","text":"<p>To support our new iterative approach, we need to update our data model. We can do this by adding helper methods <code>update</code> and <code>draw</code> to our Pydantic models. These methods will simplify our code and allow us to easily visualize the knowledge graph.</p> <p>In the <code>KnowledgeGraph</code> class, we have migrated the code from the <code>visualize_knowledge_graph</code> method and added new lists for nodes and edges.</p> <pre><code>class KnowledgeGraph(BaseModel):\n    nodes: Optional[List[Node]] = Field(..., default_factory=list)\n    edges: Optional[List[Edge]] = Field(..., default_factory=list)\n\n    def update(self, other: \"KnowledgeGraph\") -&gt; \"KnowledgeGraph\":\n        \"\"\"Updates the current graph with the other graph, deduplicating nodes and edges.\"\"\"\n        return KnowledgeGraph(\n            nodes=list(set(self.nodes + other.nodes)),\n            edges=list(set(self.edges + other.edges)),\n        )\n\n    def draw(self, prefix: str = None):\n        dot = Digraph(comment=\"Knowledge Graph\")\n\n        for node in self.nodes:  # (1)!\n            dot.node(str(node.id), node.label, color=node.color)\n\n        for edge in self.edges:  # (2)!\n            dot.edge(\n                str(edge.source), str(edge.target), label=edge.label, color=edge.color\n            )\n        dot.render(prefix, format=\"png\", view=True)\n</code></pre> <ol> <li>We iterate through all the nodes in our graph and add them to the graph</li> <li>We iterate through all the edges in our graph and add them to the graph</li> </ol> <p>We can modify our <code>generate_graph</code> function to now take in a list of strings. At each step, it'll extract out the key insights from the sentences in the form of edges and nodes like we've seen before. We can then combine these new edges and nodes with our existing knowledge graph through iterative updates to our graph before arriving at our final result.</p> <pre><code>def generate_graph(input: List[str]) -&gt; KnowledgeGraph:\n    cur_state = KnowledgeGraph()  # (1)!\n    num_iterations = len(input)\n    for i, inp in enumerate(input):\n        new_updates = client.chat.completions.create(\n            model=\"gpt-3.5-turbo-16k\",\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": \"\"\"You are an iterative knowledge graph builder.\n                    You are given the current state of the graph, and you must append the nodes and edges\n                    to it Do not procide any duplcates and try to reuse nodes as much as possible.\"\"\",\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"\"\"Extract any new nodes and edges from the following:\n                    # Part {i}/{num_iterations} of the input:\n\n                    {inp}\"\"\",\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"\"\"Here is the current state of the graph:\n                    {cur_state.model_dump_json(indent=2)}\"\"\",\n                },  # (2)!\n            ],\n            response_model=KnowledgeGraph,\n        )  # type: ignore\n\n        # Update the current state\n        cur_state = cur_state.update(new_updates)  # (3)!\n        cur_state.draw(prefix=f\"iteration_{i}\")\n    return cur_state\n</code></pre> <ol> <li> <p>We first initialise an empty <code>KnowledgeGraph</code>. In this state, it has zero nodes and edges</p> </li> <li> <p>We then add in the current state of the graph into the prompt so that the model knows what new information needs to be added</p> </li> <li> <p>We then update the nodes and edges of our graph with the information that our model has returned before visualizing the new changes</p> </li> </ol> <p>Once we've done this, we can now run this new <code>generate_graph</code> function with the following two lines.</p> <pre><code>graph: KnowledgeGraph = generate_graph(text_chunks)\ngraph.draw(prefix=\"final\")\n</code></pre>"},{"location":"examples/knowledge_graph/#conclusion","title":"Conclusion","text":"<p>We've seen how we can use <code>Instructor</code> to obtain structured outputs from the OpenAI LLM API but you could use that for any of the other open-source models that the library is compatible with. If you enjoy the content or want to try out <code>Instructor</code> check out the github and don't forget to give us a star!</p>"},{"location":"examples/moderation/","title":"OpenAI Moderation","text":"<p>This example uses OpenAI's moderation endpoint to check content compliance with OpenAI's usage policies. It can identify and filter harmful content that violates the policies.</p> <p>The model flags content and classifies it into categories including hate, harassment, self-harm, sexual content, and violence. Each category has subcategories for detailed classification.</p> <p>This validator is to be used for monitoring OpenAI API inputs and outputs, other use cases are currently not allowed.</p>"},{"location":"examples/moderation/#incorporating-openai-moderation-validator","title":"Incorporating OpenAI moderation validator","text":"<p>The following code defines a function to validate content using OpenAI's Moderation endpoint. The <code>AfterValidator</code> is used to apply OpenAI's moderation after the compute. This moderation checks if the content complies with OpenAI's usage policies and flags any harmful content. Here's how it works:</p> <ol> <li> <p>Generate the OpenAI client and patch it with the <code>instructor</code>. Patching is not strictly necessary for this example but its a good idea to always patch the client to leverage the full <code>instructor</code> functionality.</p> </li> <li> <p>Annotate our <code>message</code> field with <code>AfterValidator(openai_moderation(client=client))</code>. This means that after the <code>message</code> is computed, it will be passed to the <code>openai_moderation</code> function for validation.</p> </li> </ol> <pre><code>import instructor\n\nfrom instructor import openai_moderation\n\nfrom typing_extensions import Annotated\nfrom pydantic import BaseModel, AfterValidator\nfrom openai import OpenAI\n\nclient = instructor.patch(OpenAI())\n\n\nclass Response(BaseModel):\n    message: Annotated[str, AfterValidator(openai_moderation(client=client))]\n\ntry:\n    Response(message=\"I want to make them suffer the consequences\")\nexcept Exception as e:\n    print(e)\n    \"\"\"\n    1 validation error for Response\n    message\n      Value error, `I want to make them suffer the consequences` was flagged for violence, violence/threat [type=value_error, input_value='I want to make them suffer the consequences', input_type=str]\n    \"\"\"\n\ntry:\n    Response(message=\"I want to hurt myself.\")\nexcept Exception as e:\n    print(e)\n    \"\"\"\n    1 validation error for Response\n    message\n      Value error, `I want to hurt myself` was flagged for self_harm, self_harm_intent, violence, self-harm, self-harm/intent [type=value_error, input_value='I want to hurt myself', input_type=str]\n    \"\"\"\n</code></pre>"},{"location":"examples/ollama/","title":"Structured Outputs with Ollama","text":"<p>Open-source LLMS are gaining popularity, and the release of Ollama's OpenAI compatibility later it has made it possible to obtain structured outputs using JSON schema.</p> <p>By the end of this blog post, you will learn how to effectively utilize instructor with ollama. But before we proceed, let's first explore the concept of patching.</p>"},{"location":"examples/ollama/#patching","title":"Patching","text":"<p>Instructor's patch enhances a openai api it with the following features:</p> <ul> <li><code>response_model</code> in <code>create</code> calls that returns a pydantic model</li> <li><code>max_retries</code> in <code>create</code> calls that retries the call if it fails by using a backoff strategy</li> </ul> <p>Learn More</p> <p>To learn more, please refer to the docs. To understand the benefits of using Pydantic with Instructor, visit the tips and tricks section of the why use Pydantic page.</p>"},{"location":"examples/ollama/#ollama","title":"Ollama","text":"<p>Start by downloading Ollama, and then pull a model such as Llama 2 or Mistral.</p> <p>Make sure you update your <code>ollama</code> to the latest version!</p> <pre><code>ollama pull llama2\n</code></pre> <pre><code>from openai import OpenAI\nfrom pydantic import BaseModel, Field\nfrom typing import List\n\nimport instructor\n\n\nclass Character(BaseModel):\n    name: str\n    age: int\n    fact: List[str] = Field(..., description=\"A list of facts about the character\")\n\n\n# enables `response_model` in create call\nclient = instructor.patch(\n    OpenAI(\n        base_url=\"http://localhost:11434/v1\",\n        api_key=\"ollama\",  # required, but unused\n    ),\n    mode=instructor.Mode.JSON,\n)\n\nresp = client.chat.completions.create(\n    model=\"llama2\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Tell me about the Harry Potter\",\n        }\n    ],\n    response_model=Character,\n)\nprint(resp.model_dump_json(indent=2))\n\"\"\"\n{\n  \"name\": \"Harry James Potter\",\n  \"age\": 37,\n  \"fact\": [\n    \"He is the chosen one.\",\n    \"He has a lightning-shaped scar on his forehead.\",\n    \"He is the son of James and Lily Potter.\",\n    \"He attended Hogwarts School of Witchcraft and Wizardry.\",\n    \"He is a skilled wizard and sorcerer.\",\n    \"He fought against Lord Voldemort and his followers.\",\n    \"He has a pet owl named Snowy.\"\n  ]\n}\n\"\"\"\n</code></pre>"},{"location":"examples/open_source/","title":"Instructor with open source models","text":"<p>Instructor works with Open source model providers that support the OpenAI API chat endpoint</p> <p>See examples README here</p>"},{"location":"examples/open_source/#currently-tested-open-source-model-providers","title":"Currently tested open source model providers","text":"<ul> <li>OpenRouter</li> <li>Perplexity</li> <li>RunPod TheBloke LLMs **</li> </ul> <p>** This utilizes text-generation-webui w/ Openai plugin under the hood. </p>"},{"location":"examples/pii/","title":"PII Data Extraction and Scrubbing","text":""},{"location":"examples/pii/#overview","title":"Overview","text":"<p>This example demonstrates the usage of OpenAI's ChatCompletion model for the extraction and scrubbing of Personally Identifiable Information (PII) from a document. The code defines Pydantic models to manage the PII data and offers methods for both extraction and sanitation.</p>"},{"location":"examples/pii/#defining-the-structures","title":"Defining the Structures","text":"<p>First, Pydantic models are defined to represent the PII data and the overall structure for PII data extraction.</p> <pre><code>from typing import List\nfrom pydantic import BaseModel\n\n\n# Define Schemas for PII data\nclass Data(BaseModel):\n    index: int\n    data_type: str\n    pii_value: str\n\n\nclass PIIDataExtraction(BaseModel):\n    \"\"\"\n    Extracted PII data from a document, all data_types should try to have consistent property names\n    \"\"\"\n\n    private_data: List[Data]\n\n    def scrub_data(self, content: str) -&gt; str:\n        \"\"\"\n        Iterates over the private data and replaces the value with a placeholder in the form of\n        &lt;{data_type}_{i}&gt;\n        \"\"\"\n        for i, data in enumerate(self.private_data):\n            content = content.replace(data.pii_value, f\"&lt;{data.data_type}_{i}&gt;\")\n        return content\n</code></pre>"},{"location":"examples/pii/#extracting-pii-data","title":"Extracting PII Data","text":"<p>The OpenAI API is utilized to extract PII information from a given document.</p> <pre><code>from openai import OpenAI\nimport instructor\n\nclient = instructor.patch(OpenAI())\n\nEXAMPLE_DOCUMENT = \"\"\"\n# Fake Document with PII for Testing PII Scrubbing Model\n# (The content here)\n\"\"\"\n\npii_data = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=PIIDataExtraction,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a world class PII scrubbing model, Extract the PII data from the following document\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": EXAMPLE_DOCUMENT,\n        },\n    ],\n)  # type: ignore\n\nprint(\"Extracted PII Data:\")\nprint(pii_data.model_dump_json())\n</code></pre>"},{"location":"examples/pii/#output-of-extracted-pii-data","title":"Output of Extracted PII Data","text":"<pre><code>{\n  \"private_data\": [\n    {\n      \"index\": 0,\n      \"data_type\": \"date\",\n      \"pii_value\": \"01/02/1980\"\n    },\n    {\n      \"index\": 1,\n      \"data_type\": \"ssn\",\n      \"pii_value\": \"123-45-6789\"\n    },\n    {\n      \"index\": 2,\n      \"data_type\": \"email\",\n      \"pii_value\": \"john.doe@email.com\"\n    },\n    {\n      \"index\": 3,\n      \"data_type\": \"phone\",\n      \"pii_value\": \"555-123-4567\"\n    },\n    {\n      \"index\": 4,\n      \"data_type\": \"address\",\n      \"pii_value\": \"123 Main St, Springfield, IL, 62704\"\n    }\n  ]\n}\n</code></pre>"},{"location":"examples/pii/#scrubbing-pii-data","title":"Scrubbing PII Data","text":"<p>After extracting the PII data, the <code>scrub_data</code> method is used to sanitize the document.</p> <pre><code>print(\"Scrubbed Document:\")\nprint(pii_data.scrub_data(EXAMPLE_DOCUMENT))\n</code></pre>"},{"location":"examples/pii/#output-of-scrubbed-document","title":"Output of Scrubbed Document","text":"<pre><code># Fake Document with PII for Testing PII Scrubbing Model\n\n## Personal Story\n\nJohn Doe was born on &lt;date_0&gt;. His social security number is &lt;ssn_1&gt;. He has been using the email address &lt;email_2&gt; for years, and he can always be reached at &lt;phone_3&gt;.\n\n## Residence\n\nJohn currently resides at &lt;address_4&gt;. He's been living there for about 5 years now.\n</code></pre>"},{"location":"examples/planning-tasks/","title":"Example: Planning and Executing a Query Plan","text":"<p>This example demonstrates how to use the OpenAI Function Call ChatCompletion model to plan and execute a query plan in a question-answering system. By breaking down a complex question into smaller sub-questions with defined dependencies, the system can systematically gather the necessary information to answer the main question.</p> <p>Motivation</p> <p>The goal of this example is to showcase how query planning can be used to handle complex questions, facilitate iterative information gathering, automate workflows, and optimize processes. By leveraging the OpenAI Function Call model, you can design and execute a structured plan to find answers effectively.</p> <p>Use Cases:</p> <ul> <li>Complex question answering</li> <li>Iterative information gathering</li> <li>Workflow automation</li> <li>Process optimization</li> </ul> <p>With the OpenAI Function Call model, you can customize the planning process and integrate it into your specific application to meet your unique requirements.</p>"},{"location":"examples/planning-tasks/#defining-the-structures","title":"Defining the Structures","text":"<p>Let's define the necessary Pydantic models to represent the query plan and the queries.</p> <pre><code>import enum\nfrom typing import List\nfrom pydantic import Field, BaseModel\n\n\nclass QueryType(str, enum.Enum):\n    \"\"\"Enumeration representing the types of queries that can be asked to a question answer system.\"\"\"\n\n    SINGLE_QUESTION = \"SINGLE\"\n    MERGE_MULTIPLE_RESPONSES = \"MERGE_MULTIPLE_RESPONSES\"\n\n\nclass Query(BaseModel):\n    \"\"\"Class representing a single question in a query plan.\"\"\"\n\n    id: int = Field(..., description=\"Unique id of the query\")\n    question: str = Field(\n        ...,\n        description=\"Question asked using a question answering system\",\n    )\n    dependencies: List[int] = Field(\n        default_factory=list,\n        description=\"List of sub questions that need to be answered before asking this question\",\n    )\n    node_type: QueryType = Field(\n        default=QueryType.SINGLE_QUESTION,\n        description=\"Type of question, either a single question or a multi-question merge\",\n    )\n\n\nclass QueryPlan(BaseModel):\n    \"\"\"Container class representing a tree of questions to ask a question answering system.\"\"\"\n\n    query_graph: List[Query] = Field(\n        ..., description=\"The query graph representing the plan\"\n    )\n\n    def _dependencies(self, ids: List[int]) -&gt; List[Query]:\n        \"\"\"Returns the dependencies of a query given their ids.\"\"\"\n        return [q for q in self.query_graph if q.id in ids]\n</code></pre> <p>Graph Generation</p> <p>Notice that this example produces a flat list of items with dependencies that resemble a graph, while pydantic allows for recursive definitions, it's much easier and less confusing for the model to generate flat schemas rather than recursive schemas. If you want to see a recursive example, see recursive schemas</p>"},{"location":"examples/planning-tasks/#planning-a-query-plan","title":"Planning a Query Plan","text":"<p>Now, let's demonstrate how to plan and execute a query plan using the defined models and the OpenAI API.</p> <pre><code>import instructor\nfrom openai import OpenAI\n\n# Apply the patch to the OpenAI client\n# enables response_model keyword\nclient = instructor.patch(OpenAI())\n\n\ndef query_planner(question: str) -&gt; QueryPlan:\n    PLANNING_MODEL = \"gpt-4-0613\"\n\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a world class query planning algorithm capable ofbreaking apart questions into its dependency queries such that the answers can be used to inform the parent question. Do not answer the questions, simply provide a correct compute graph with good specific questions to ask and relevant dependencies. Before you call the function, think step-by-step to get a better understanding of the problem.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"Consider: {question}\\nGenerate the correct query plan.\",\n        },\n    ]\n\n    root = client.chat.completions.create(\n        model=PLANNING_MODEL,\n        temperature=0,\n        response_model=QueryPlan,\n        messages=messages,\n        max_tokens=1000,\n    )\n    return root\n</code></pre> <pre><code>plan = query_planner(\n    \"What is the difference in populations of Canada and the Jason's home country?\"\n)\nplan.model_dump()\n</code></pre> <p>No RAG</p> <p>While we build the query plan in this example, we do not propose a method to actually answer the question. You can implement your own answer function that perhaps makes a retrieval and calls openai for retrieval augmented generation. That step would also make use of function calls but goes beyond the scope of this example.</p> <pre><code>{\n    \"query_graph\": [\n        {\n            \"dependencies\": [],\n            \"id\": 1,\n            \"node_type\": \"SINGLE\",\n            \"question\": \"Identify Jason's home country\",\n        },\n        {\n            \"dependencies\": [],\n            \"id\": 2,\n            \"node_type\": \"SINGLE\",\n            \"question\": \"Find the population of Canada\",\n        },\n        {\n            \"dependencies\": [1],\n            \"id\": 3,\n            \"node_type\": \"SINGLE\",\n            \"question\": \"Find the population of Jason's home country\",\n        },\n        {\n            \"dependencies\": [2, 3],\n            \"id\": 4,\n            \"node_type\": \"SINGLE\",\n            \"question\": \"Calculate the difference in populations between Canada and Jasons home country\",\n        },\n    ]\n}\n</code></pre> <p>In the above code, we define a <code>query_planner</code> function that takes a question as input and generates a query plan using the OpenAI API.</p>"},{"location":"examples/planning-tasks/#conclusion","title":"Conclusion","text":"<p>In this example, we demonstrated how to use the OpenAI Function Call <code>ChatCompletion</code> model to plan and execute a query plan using a question-answering system. We defined the necessary structures using Pydantic, created a query planner function.</p> <p>If you want to see multiple versions of this style of code, please visit:</p> <ol> <li>query planning example</li> <li>task planning with topo sort</li> </ol> <p>Feel free to modify the code to fit your specific use case and explore other possibilities of using the OpenAI Function Call model to plan and execute complex workflows.</p>"},{"location":"examples/search/","title":"Example: Segmenting Search Queries","text":"<p>In this example, we will demonstrate how to leverage the <code>MultiTask</code> and <code>enum.Enum</code> features of OpenAI Function Call to segment search queries. We will define the necessary structures using Pydantic and demonstrate how segment queries into multiple sub queries and execute them in parallel with <code>asyncio</code>.</p> <p>Motivation</p> <p>Extracting a list of tasks from text is a common use case for leveraging language models. This pattern can be applied to various applications, such as virtual assistants like Siri or Alexa, where understanding user intent and breaking down requests into actionable tasks is crucial. In this example, we will demonstrate how to use OpenAI Function Call to segment search queries and execute them in parallel.</p>"},{"location":"examples/search/#structure-of-the-data","title":"Structure of the Data","text":"<p>The <code>Search</code> class is a Pydantic model that defines the structure of the search query. It has three fields: <code>title</code>, <code>query</code>, and <code>type</code>. The <code>title</code> field is the title of the request, the <code>query</code> field is the query to search for relevant content, and the <code>type</code> field is the type of search. The <code>execute</code> method is used to execute the search query.</p> <p>```python hl_line import instructor from openai import OpenAI from typing import Iterable from pydantic import BaseModel, Field</p>"},{"location":"examples/search/#apply-the-patch-to-the-openai-client","title":"Apply the patch to the OpenAI client","text":""},{"location":"examples/search/#enables-response_model-keyword","title":"enables response_model keyword","text":"<p>client = instructor.patch(OpenAI())</p> <p>class Search(BaseModel):     query: str = Field(..., description=\"Query to search for relevant content\")     type: Literal[\"web\", \"image\", \"video\"] = Field(..., description=\"Type of search\")</p> <pre><code>async def execute(self):\n    print(\n        f\"Searching for `{self.title}` with query `{self.query}` using `{self.type}`\"\n    )\n</code></pre> <p>def segment(data: str) -&gt; MultiSearch:     return client.chat.completions.create(         model=\"gpt-3.5-turbo-0613\",         response_model=Iterable[Search],         messages=[             {                 \"role\": \"user\",                 \"content\": f\"Consider the data below: '\\n{data}' and segment it into multiple search queries\",             },         ],         max_tokens=1000,     )</p> <p>for search in segment(\"Search for a picture of a cat and a video of a dog\"):     print(search.model_dump_json())     \"\"\"     {         \"query\": \"a picture of a cat\",         \"type\": \"image\"     }     {         \"query\": \"a video of a dog\",         \"type\": \"video\"     }     \"\"\"     } ```</p>"},{"location":"examples/self_critique/","title":"Self-Correction with <code>llm_validator</code>","text":""},{"location":"examples/self_critique/#introduction","title":"Introduction","text":"<p>This guide demonstrates how to use <code>llm_validator</code> for implementing self-healing. The objective is to showcase how an instructor can self-correct by using validation errors and helpful error messages.</p> <pre><code>from openai import OpenAI\nfrom pydantic import BaseModel\nimport instructor\n\n# Apply the patch to the OpenAI client\n# enables response_model keyword\nclient = instructor.patch(OpenAI())\n\nclass QuestionAnswer(BaseModel):\n    question: str\n    answer: str\n\nquestion = \"What is the meaning of life?\"\ncontext = \"The according to the devil the meaning of live is to live a life of sin and debauchery.\"\n\nqa: QuestionAnswer = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=QuestionAnswer,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"using the context: {context}\\n\\nAnswer the following question: {question}\",\n        },\n    ],\n)\n</code></pre>"},{"location":"examples/self_critique/#output-before-validation","title":"Output Before Validation","text":"<p>While it calls out the objectionable content, it doesn't provide any details on how to correct it.</p> <pre><code>{\n  \"question\": \"What is the meaning of life?\",\n  \"answer\": \"The meaning of life, according to the context, is to live a life of sin and debauchery.\"\n}\n</code></pre>"},{"location":"examples/self_critique/#adding-custom-validation","title":"Adding Custom Validation","text":"<p>By adding a validator to the <code>answer</code> field, we can try to catch the issue and correct it. Lets integrate <code>llm_validator</code> into the model and see the error message. Its important to note that you can use all of pydantic's validators as you would normally as long as you raise a <code>ValidationError</code> with a helpful error message as it will be used as part of the self correction prompt.</p> <pre><code>from pydantic import BaseModel, BeforeValidator\nfrom typing_extensions import Annotated\nfrom instructor import llm_validator\n\nclass QuestionAnswerNoEvil(BaseModel):\n    question: str\n    answer: Annotated[\n        str,\n        BeforeValidator(\n            llm_validator(\"don't say objectionable things\", allow_override=True)\n        ),\n    ]\n\n\ntry:\n    qa: QuestionAnswerNoEvil = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=QuestionAnswerNoEvil,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"using the context: {context}\\n\\nAnswer the following question: {question}\",\n            },\n        ],\n    )\nexcept Exception as e:\n    print(e)\n</code></pre>"},{"location":"examples/self_critique/#output-after-validation","title":"Output After Validation","text":"<p>Now, we throw validation error that its objectionable and provide a helpful error message.</p> <pre><code>1 validation error for QuestionAnswerNoEvil\nanswer\n  Assertion failed, The statement promotes sin and debauchery, which is objectionable.\n</code></pre>"},{"location":"examples/self_critique/#retrying-with-corrections","title":"Retrying with Corrections","text":"<p>By adding the <code>max_retries</code> parameter, we can retry the request with corrections. and use the error message to correct the output.</p> <pre><code>qa: QuestionAnswerNoEvil = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=QuestionAnswerNoEvil,\n    max_retries=1,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"using the context: {context}\\n\\nAnswer the following question: {question}\",\n        },\n    ],\n)\n</code></pre>"},{"location":"examples/self_critique/#final-output","title":"Final Output","text":"<p>Now, we get a valid response that is not objectionable!</p> <pre><code>{\n  \"question\": \"What is the meaning of life?\",\n  \"answer\": \"The meaning of life is subjective and can vary depending on individual beliefs and philosophies.\"\n}\n</code></pre>"},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/archive/2023/","title":"2023","text":""},{"location":"blog/page/2/","title":"Welcome to the Instructor Blog","text":""},{"location":"blog/archive/2023/page/2/","title":"2023","text":""}]}